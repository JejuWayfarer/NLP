{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT_Batch15_Maxlen512_epoch8_lr5e-6.ipynb","provenance":[{"file_id":"1zoegtBGk43qk_t72PWoCl82ncTehvOkh","timestamp":1594886561480},{"file_id":"1YwsxWyODqRtdMwpEwxrY95ciqsnHXCPG","timestamp":1594869499691},{"file_id":"1XxmuMe4tpkl36LW9YGLTdbagAfgmBVFy","timestamp":1594801789924},{"file_id":"1uA6AJddxdDaqY7rZyBBlmKNrFOw7FXHo","timestamp":1594788662006},{"file_id":"1NHrD84KklkpU6rN41_oznpg3J1zcuZJc","timestamp":1594717130705}],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1m98PpjAP6z7O1ysZCa45d_xjHaNe_Tvs","authorship_tag":"ABX9TyMEdwPDGKrywLL4peXugPMq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"1d4ea679b2dd43c689c40a937c4294bb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_794e25cd67624f0390f3fe484b98449d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_96ba038dc6f742fcb0233178d1357d3a","IPY_MODEL_aae0a11663454c2ea64f27600f3ec1c0"]}},"794e25cd67624f0390f3fe484b98449d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"96ba038dc6f742fcb0233178d1357d3a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5d1dde7e3af44656bcb4a34ba33c6a56","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":625,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":625,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d790b8638d704510852195dec2a0d631"}},"aae0a11663454c2ea64f27600f3ec1c0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_89abb2391e694c1abc610fe87b63c4cc","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 625/625 [00:44&lt;00:00, 14.2B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_285e2a4fbc6a4b33a8fcdb5de0769bf6"}},"5d1dde7e3af44656bcb4a34ba33c6a56":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d790b8638d704510852195dec2a0d631":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"89abb2391e694c1abc610fe87b63c4cc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"285e2a4fbc6a4b33a8fcdb5de0769bf6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b9523c4eac9347a98e26a1eda129150e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a217b79c5fba41ca9d240c093c7197f6","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_09cc43f243ac45949558f4e69357a699","IPY_MODEL_1abb1ea0db9140dc9883554ad9aa15e6"]}},"a217b79c5fba41ca9d240c093c7197f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"09cc43f243ac45949558f4e69357a699":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_103e044ca39e43cf9d52a306a7076406","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":714314041,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":714314041,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_30a66cef681f45d593a4c199681a8206"}},"1abb1ea0db9140dc9883554ad9aa15e6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d99def37e1d044b6b586087a3f6c0470","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 714M/714M [00:39&lt;00:00, 18.0MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_375aaead46da40469f579343ff6e83ae"}},"103e044ca39e43cf9d52a306a7076406":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"30a66cef681f45d593a4c199681a8206":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d99def37e1d044b6b586087a3f6c0470":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"375aaead46da40469f579343ff6e83ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9368808fad1d43228e149fbd698f0c5f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4e66268afc0649cb8f043f1b9b1e8aa2","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9074740621bc42d0b9fc2b9f01972ade","IPY_MODEL_98902a52edf5448d95b06e8d20bcccad"]}},"4e66268afc0649cb8f043f1b9b1e8aa2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9074740621bc42d0b9fc2b9f01972ade":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_918bcf7a3c7949b8bd1a62b0954cc306","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":995526,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":995526,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7ca7847658bc4841a3d04662fdbd6206"}},"98902a52edf5448d95b06e8d20bcccad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3587304aed614f799cc014512b4b37f1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 996k/996k [00:00&lt;00:00, 1.91MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_176f3af0558c41e8bc0a49acf92eaf99"}},"918bcf7a3c7949b8bd1a62b0954cc306":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7ca7847658bc4841a3d04662fdbd6206":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3587304aed614f799cc014512b4b37f1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"176f3af0558c41e8bc0a49acf92eaf99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7189a6674a104567852020ed2a4150b1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bbae5a87741343ccb47eea1d8447a1b1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f35b6c56f206427d815aefdda417b0d8","IPY_MODEL_00507e378dff46e88f75e2540f0899d2"]}},"bbae5a87741343ccb47eea1d8447a1b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f35b6c56f206427d815aefdda417b0d8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_50f6030574be4406af564cf6f0db722c","_dom_classes":[],"description":"  0%","_model_name":"FloatProgressModel","bar_style":"","max":8,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5647c28ab40e4f4ba16864d676df83cc"}},"00507e378dff46e88f75e2540f0899d2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a249cabec5d54d4ba0ae06882071218b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/8 [00:00&lt;?, ?it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dfbb5cc1ba124e3b85ccc7e3d008e6f2"}},"50f6030574be4406af564cf6f0db722c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5647c28ab40e4f4ba16864d676df83cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a249cabec5d54d4ba0ae06882071218b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"dfbb5cc1ba124e3b85ccc7e3d008e6f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"52eb959f7d4d40f497322781ca2db919":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e45631ab13354a1081cb79d312bacda0","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9bb8c857c66241c9a0294f67ba182be1","IPY_MODEL_ddf307ff3c774832986350558d81503a"]}},"e45631ab13354a1081cb79d312bacda0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9bb8c857c66241c9a0294f67ba182be1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ac97fb9c012e4d5da3ff4221721d05ca","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d6373b6cfb7d4b2abf1fc2b527c8ab7d"}},"ddf307ff3c774832986350558d81503a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b27683108e474359a49afcae1b1afc73","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1278/? [16:51&lt;00:00,  1.26it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c742054c5533482d93c775e972ad0ec8"}},"ac97fb9c012e4d5da3ff4221721d05ca":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d6373b6cfb7d4b2abf1fc2b527c8ab7d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b27683108e474359a49afcae1b1afc73":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c742054c5533482d93c775e972ad0ec8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"YtJe70ygeQmp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":615},"executionInfo":{"status":"ok","timestamp":1594887151509,"user_tz":-540,"elapsed":13280,"user":{"displayName":"민지웅","photoUrl":"","userId":"03970464211046260688"}},"outputId":"eec123c9-88c4-4b38-9257-671603239727"},"source":["!pip install transformers\n","\n","# basic\n","import pandas as pd\n","import urllib.request\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import re\n","import random\n","import time\n","import datetime\n","import numpy as np\n","import os\n","import tqdm\n","%config InlineBackend.figure_format = 'retina'\n","import shutil\n","import logging\n","logging.basicConfig(level=logging.ERROR)\n","\n","    # # tf\n","    # from tensorflow.keras.datasets import reuters\n","    # from tensorflow.keras.models import Sequential\n","    # from tensorflow.keras.layers import Dense, LSTM, Embedding\n","    # from tensorflow.keras.preprocessing.sequence import pad_sequences\n","    # from tensorflow.keras.utils import to_categorical\n","    # from tensorflow.keras.models import load_model\n","    # from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","#torch\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, random_split\n","from torchvision import datasets, transforms\n","import matplotlib.pyplot as plt\n","\n","# transformers(BERT)\n","from transformers import BertModel, BertTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n","\r\u001b[K     |▍                               | 10kB 28.6MB/s eta 0:00:01\r\u001b[K     |▉                               | 20kB 2.8MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30kB 3.8MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40kB 4.1MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51kB 3.3MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61kB 3.8MB/s eta 0:00:01\r\u001b[K     |███                             | 71kB 4.0MB/s eta 0:00:01\r\u001b[K     |███▍                            | 81kB 4.4MB/s eta 0:00:01\r\u001b[K     |███▉                            | 92kB 4.7MB/s eta 0:00:01\r\u001b[K     |████▎                           | 102kB 4.5MB/s eta 0:00:01\r\u001b[K     |████▊                           | 112kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 122kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 133kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 143kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 153kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 163kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 174kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 184kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████                        | 194kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 204kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████                       | 215kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 225kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 235kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 245kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 256kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 266kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 276kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 286kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 296kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 307kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 317kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 327kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 337kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 348kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 358kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 368kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 378kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 389kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 399kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 409kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 419kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 430kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 440kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 450kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 460kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 471kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 481kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 491kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 501kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 512kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 522kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 532kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 542kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 552kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 563kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 573kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 583kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 593kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 604kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 614kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 624kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 634kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 645kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 655kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 665kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 675kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 686kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 696kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 706kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 716kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 727kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 737kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 747kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 757kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 768kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 778kB 4.5MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Collecting tokenizers==0.8.1.rc1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 19.3MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Collecting sentencepiece!=0.1.92\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 47.2MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 67.4MB/s \n","\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=bd32667bcf8fc314cbe1415f1e191fa2a0d3848138c783e08c96c01ad60f26dc\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0mRjk8eH7kDG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":166,"referenced_widgets":["1d4ea679b2dd43c689c40a937c4294bb","794e25cd67624f0390f3fe484b98449d","96ba038dc6f742fcb0233178d1357d3a","aae0a11663454c2ea64f27600f3ec1c0","5d1dde7e3af44656bcb4a34ba33c6a56","d790b8638d704510852195dec2a0d631","89abb2391e694c1abc610fe87b63c4cc","285e2a4fbc6a4b33a8fcdb5de0769bf6","b9523c4eac9347a98e26a1eda129150e","a217b79c5fba41ca9d240c093c7197f6","09cc43f243ac45949558f4e69357a699","1abb1ea0db9140dc9883554ad9aa15e6","103e044ca39e43cf9d52a306a7076406","30a66cef681f45d593a4c199681a8206","d99def37e1d044b6b586087a3f6c0470","375aaead46da40469f579343ff6e83ae","9368808fad1d43228e149fbd698f0c5f","4e66268afc0649cb8f043f1b9b1e8aa2","9074740621bc42d0b9fc2b9f01972ade","98902a52edf5448d95b06e8d20bcccad","918bcf7a3c7949b8bd1a62b0954cc306","7ca7847658bc4841a3d04662fdbd6206","3587304aed614f799cc014512b4b37f1","176f3af0558c41e8bc0a49acf92eaf99"]},"executionInfo":{"status":"ok","timestamp":1594887196074,"user_tz":-540,"elapsed":57831,"user":{"displayName":"민지웅","photoUrl":"","userId":"03970464211046260688"}},"outputId":"46dbdaa1-39a2-48ee-eaf3-b8aaec9d8b74"},"source":["bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"],"execution_count":2,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1d4ea679b2dd43c689c40a937c4294bb","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=625.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b9523c4eac9347a98e26a1eda129150e","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=714314041.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9368808fad1d43228e149fbd698f0c5f","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=995526.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uvSu9nlcjFLh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1594887196453,"user_tz":-540,"elapsed":58199,"user":{"displayName":"민지웅","photoUrl":"","userId":"03970464211046260688"}},"outputId":"e5215aaa-60f8-443c-9f7f-85f7c6091eba"},"source":["if torch.cuda.is_available():    \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","else:\n","    device = torch.device(\"cpu\")\n","    print('No GPU available, using the CPU instead.')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla P100-PCIE-16GB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hb1buSippm0O","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594887200265,"user_tz":-540,"elapsed":62000,"user":{"displayName":"민지웅","photoUrl":"","userId":"03970464211046260688"}}},"source":["train = pd.read_csv(\"/content/drive/My Drive/bluehouse/train.csv\", usecols=['category','data'])[['data','category']].dropna()\n","test = pd.read_csv('/content/drive/My Drive/bluehouse/test.csv', encoding = 'utf-8')"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"R_V5uQBxAJzv","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594887289397,"user_tz":-540,"elapsed":151125,"user":{"displayName":"민지웅","photoUrl":"","userId":"03970464211046260688"}}},"source":["train_sentences = train.data.values\n","train_labels = train.category.values\n","\n","train_input_ids = []\n","train_attention_masks = []\n","\n","for sent in train_sentences:\n","    encoded_dict = tokenizer.encode_plus(\n","                sent,\n","                add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                max_length = 512,           # Pad & truncate all sentences.\n","                pad_to_max_length = True,\n","                return_attention_mask = True,   # Construct attn. masks.\n","                return_tensors = 'pt')  \n","    train_input_ids.append(encoded_dict['input_ids'])\n","    train_attention_masks.append(encoded_dict['attention_mask'])\n","\n","train_input_ids = torch.cat(train_input_ids, dim=0)\n","train_attention_masks = torch.cat(train_attention_masks, dim=0)\n","train_labels = torch.tensor(train_labels)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"BWKyM62_ADs4","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594887301378,"user_tz":-540,"elapsed":163100,"user":{"displayName":"민지웅","photoUrl":"","userId":"03970464211046260688"}}},"source":["test_sentences = test.data.values\n","\n","test_input_ids = []\n","test_attention_masks = []\n","\n","for sent in test_sentences:\n","    encoded_dict = tokenizer.encode_plus(\n","                sent,\n","                add_special_tokens = True, \n","                max_length = 512,           \n","                pad_to_max_length = True,\n","                return_attention_mask = True,  \n","                return_tensors = 'pt')   \n","    test_input_ids.append(encoded_dict['input_ids'])\n","    test_attention_masks.append(encoded_dict['attention_mask'])\n","\n","test_input_ids = torch.cat(test_input_ids, dim=0)\n","test_attention_masks = torch.cat(test_attention_masks, dim=0)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"kxm9xVKBw42j","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1594887301379,"user_tz":-540,"elapsed":163093,"user":{"displayName":"민지웅","photoUrl":"","userId":"03970464211046260688"}},"outputId":"0e5a397e-39a6-4263-8a71-2a9ab14d1be6"},"source":["dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n","prediction_data = TensorDataset(test_input_ids, test_attention_masks)\n","\n","train_size = int(0.9 * len(dataset))\n","val_size = len(dataset) - train_size\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","print('{:>5,} training samples'.format(train_size))\n","print('{:>5,} validation samples'.format(val_size))\n","print('{:>5,} test samples'.format(len(prediction_data)))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["35,992 training samples\n","4,000 validation samples\n","5,000 test samples\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SE1S2Nn02gqJ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594887301380,"user_tz":-540,"elapsed":163086,"user":{"displayName":"민지웅","photoUrl":"","userId":"03970464211046260688"}}},"source":["train_batch_size = 15\n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = train_batch_size # Trains with this batch size.\n","        )\n","validation_dataloader = DataLoader(\n","            val_dataset, # The validation samples.\n","            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n","            batch_size = train_batch_size # Evaluate with this batch size.\n","        )\n","\n","test_batch_size = 15\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=test_batch_size)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"cnBXhnRm3MOD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1594887323085,"user_tz":-540,"elapsed":184780,"user":{"displayName":"민지웅","photoUrl":"","userId":"03970464211046260688"}},"outputId":"d2c66f2d-c1c8-4b63-cf5b-7fa4d8242e1f"},"source":["model = BertForSequenceClassification.from_pretrained(\n","    'bert-base-multilingual-cased', # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = 3, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","model.cuda()"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"46p3T5123V5O","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":633},"executionInfo":{"status":"ok","timestamp":1594887323086,"user_tz":-540,"elapsed":184774,"user":{"displayName":"민지웅","photoUrl":"","userId":"03970464211046260688"}},"outputId":"21e9892f-3cea-422e-ceeb-c182e4fee4b8"},"source":["params = list(model.named_parameters())\n","print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n","print('==== Embedding Layer ====\\n')\n","for p in params[0:5]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","print('\\n==== First Transformer ====\\n')\n","for p in params[5:21]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","print('\\n==== Output Layer ====\\n')\n","for p in params[-4:]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["The BERT model has 201 different named parameters.\n","\n","==== Embedding Layer ====\n","\n","bert.embeddings.word_embeddings.weight                  (119547, 768)\n","bert.embeddings.position_embeddings.weight                (512, 768)\n","bert.embeddings.token_type_embeddings.weight                (2, 768)\n","bert.embeddings.LayerNorm.weight                              (768,)\n","bert.embeddings.LayerNorm.bias                                (768,)\n","\n","==== First Transformer ====\n","\n","bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n","bert.encoder.layer.0.attention.self.query.bias                (768,)\n","bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n","bert.encoder.layer.0.attention.self.key.bias                  (768,)\n","bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n","bert.encoder.layer.0.attention.self.value.bias                (768,)\n","bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n","bert.encoder.layer.0.attention.output.dense.bias              (768,)\n","bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n","bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n","bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n","bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n","bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n","bert.encoder.layer.0.output.dense.bias                        (768,)\n","bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n","bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n","\n","==== Output Layer ====\n","\n","bert.pooler.dense.weight                                  (768, 768)\n","bert.pooler.dense.bias                                        (768,)\n","classifier.weight                                           (3, 768)\n","classifier.bias                                                 (3,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"01QFyq0M3cxt","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594887323086,"user_tz":-540,"elapsed":184767,"user":{"displayName":"민지웅","photoUrl":"","userId":"03970464211046260688"}}},"source":["optimizer = AdamW(model.parameters(),\n","                  lr = 5e-6, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )\n","# Number of training epochs. The BERT authors recommend between 2 and 4. \n","epochs = 8\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","# (Note that this is not the same as the number of training samples).\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"TvSoio9r3dF8","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594887323087,"user_tz":-540,"elapsed":184762,"user":{"displayName":"민지웅","photoUrl":"","userId":"03970464211046260688"}}},"source":["# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","def format_time(elapsed):\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"GkEd5uqoPiNc","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594887323087,"user_tz":-540,"elapsed":184757,"user":{"displayName":"민지웅","photoUrl":"","userId":"03970464211046260688"}}},"source":["check_path = '/content/drive/My Drive/bluehouse/chp/current_checkpoint.pt'\n","best_path = '/content/drive/My Drive/bluehouse/chp/best/best_checkpoint.pt'\n","\n","def save_ckp(state, is_best, checkpoint_path, best_model_path):\n","    f_path = checkpoint_path\n","    torch.save(state, f_path)\n","    if is_best:\n","        best_fpath = best_model_path\n","        shutil.copyfile(f_path, best_fpath)\n","\n","def load_ckp(checkpoint_fpath, model, optimizer):\n","    checkpoint = torch.load(checkpoint_fpath)\n","    model.load_state_dict(checkpoint['state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","    valid_loss_min = checkpoint['valid_loss_min']\n","    return model, optimizer, checkpoint['epoch'], valid_loss_min.item()"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"YpCxPy_K3ljP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":574,"referenced_widgets":["7189a6674a104567852020ed2a4150b1","bbae5a87741343ccb47eea1d8447a1b1","f35b6c56f206427d815aefdda417b0d8","00507e378dff46e88f75e2540f0899d2","50f6030574be4406af564cf6f0db722c","5647c28ab40e4f4ba16864d676df83cc","a249cabec5d54d4ba0ae06882071218b","dfbb5cc1ba124e3b85ccc7e3d008e6f2","52eb959f7d4d40f497322781ca2db919","e45631ab13354a1081cb79d312bacda0","9bb8c857c66241c9a0294f67ba182be1","ddf307ff3c774832986350558d81503a","ac97fb9c012e4d5da3ff4221721d05ca","d6373b6cfb7d4b2abf1fc2b527c8ab7d","b27683108e474359a49afcae1b1afc73","c742054c5533482d93c775e972ad0ec8"]},"outputId":"694b718f-49ed-4cb1-d5e8-ebcf63c207e7"},"source":["seed_val = 0\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","training_stats = []\n","valid_loss_min = np.inf\n","total_t0 = time.time()\n","\n","\n","for epoch_i in tqdm.notebook.tqdm(range(0, epochs)):\n","    #               Training\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    t0 = time.time()\n","    total_train_loss = 0\n","\n","    model.train()\n","\n","    for step, batch in tqdm.notebook.tqdm(enumerate(train_dataloader)):\n","\n","        # Progress update every 50 batches.\n","        if step % 50 == 0 and not step == 0:\n","            elapsed = format_time(time.time() - t0)\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","\n","        model.zero_grad()        \n","\n","        loss, logits = model(b_input_ids, \n","                             token_type_ids=None, \n","                             attention_mask=b_input_mask, \n","                             labels=b_labels)\n","\n","        total_train_loss += loss.item()\n","\n","        loss.backward()\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        optimizer.step()\n","        scheduler.step() # Update the learning rate.\n","\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    training_time = format_time(time.time() - t0)\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(training_time))\n","        \n","\n","    # Validation\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","    t0 = time.time()\n","    model.eval()\n","\n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    for batch in tqdm.notebook.tqdm(validation_dataloader):\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        \n","        with torch.no_grad():        \n","\n","            (loss, logits) = model(b_input_ids, \n","                                   token_type_ids=None, \n","                                   attention_mask=b_input_mask,\n","                                   labels=b_labels)\n","\n","        total_eval_loss += loss.item()\n","\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","        \n","    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","    avg_val_loss = total_eval_loss / len(validation_dataloader)\n","    validation_time = format_time(time.time() - t0)\n","\n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            'Valid_Loss': avg_val_loss,\n","            'Valid_Accur.': avg_val_accuracy,\n","            'Training Time': training_time,\n","            'Validation Time': validation_time\n","        }\n","    )\n","\n","    save_ckp(training_stats, False, check_path, best_path)\n","\n","    if avg_val_loss <= valid_loss_min: # if bset model\n","        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,avg_val_loss))\n","        save_ckp(training_stats, True, check_path, best_path)\n","        valid_loss_min = avg_val_loss\n","\n","    # test\n","    print('Predicting labels for {:,} test sentences...'.format(len(test_input_ids)))\n","\n","    model.eval()\n","    predictions = []\n","\n","    for batch in tqdm.notebook.tqdm(prediction_dataloader):\n","\n","        batch = tuple(t.to(device) for t in batch)\n","\n","        b_input_ids, b_input_mask = batch\n","        \n","        with torch.no_grad():\n","            outputs = model(b_input_ids, token_type_ids=None,  # Telling the model not to compute or store gradients, saving memory and \n","                            attention_mask=b_input_mask)\n","        logits = outputs[0]\n","        logits = logits.detach().cpu().numpy()\n","        predictions.append(logits)\n","    print('DONE.')\n","\n","    flat_predictions = np.concatenate(predictions, axis=0)\n","    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","\n","    submission = pd.DataFrame(flat_predictions)\n","    submission.columns = ['category']\n","    submission.index.name = 'index'\n","    submission.to_csv('/content/drive/My Drive/bluehouse/BH_Batch8_Maxlen512_epoch4_lr4e-5_{}.csv'.format(epoch_i+1), encoding='utf-8')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7189a6674a104567852020ed2a4150b1","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","======== Epoch 1 / 8 ========\n","Training...\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"52eb959f7d4d40f497322781ca2db919","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["  Batch    50  of  2,400.    Elapsed: 0:00:40.\n","  Batch   100  of  2,400.    Elapsed: 0:01:20.\n","  Batch   150  of  2,400.    Elapsed: 0:01:59.\n","  Batch   200  of  2,400.    Elapsed: 0:02:39.\n","  Batch   250  of  2,400.    Elapsed: 0:03:18.\n","  Batch   300  of  2,400.    Elapsed: 0:03:58.\n","  Batch   350  of  2,400.    Elapsed: 0:04:37.\n","  Batch   400  of  2,400.    Elapsed: 0:05:17.\n","  Batch   450  of  2,400.    Elapsed: 0:05:57.\n","  Batch   500  of  2,400.    Elapsed: 0:06:36.\n","  Batch   550  of  2,400.    Elapsed: 0:07:16.\n","  Batch   600  of  2,400.    Elapsed: 0:07:55.\n","  Batch   650  of  2,400.    Elapsed: 0:08:35.\n","  Batch   700  of  2,400.    Elapsed: 0:09:14.\n","  Batch   750  of  2,400.    Elapsed: 0:09:54.\n","  Batch   800  of  2,400.    Elapsed: 0:10:33.\n","  Batch   850  of  2,400.    Elapsed: 0:11:13.\n","  Batch   900  of  2,400.    Elapsed: 0:11:53.\n","  Batch   950  of  2,400.    Elapsed: 0:12:32.\n","  Batch 1,000  of  2,400.    Elapsed: 0:13:12.\n","  Batch 1,050  of  2,400.    Elapsed: 0:13:51.\n","  Batch 1,100  of  2,400.    Elapsed: 0:14:31.\n","  Batch 1,150  of  2,400.    Elapsed: 0:15:11.\n","  Batch 1,200  of  2,400.    Elapsed: 0:15:50.\n","  Batch 1,250  of  2,400.    Elapsed: 0:16:30.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pfQY8QZKnYkc","colab_type":"code","colab":{}},"source":["    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XbrfAb415Fud","colab_type":"code","colab":{}},"source":["pd.set_option('precision', 2)\n","\n","# Create a DataFrame from our training statistics.\n","df_stats = pd.DataFrame(data=training_stats)\n","\n","# Use the 'epoch' as the row index.\n","df_stats = df_stats.set_index('epoch')\n","\n","# A hack to force the column headers to wrap.\n","#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n","\n","# Display the table.\n","df_stats"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nfRfsQgtGyUn","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","% matplotlib inline\n","\n","import seaborn as sns\n","\n","# Use plot styling from seaborn.\n","sns.set(style='darkgrid')\n","\n","# Increase the plot size and font size.\n","sns.set(font_scale=1.5)\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","# Plot the learning curve.\n","plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n","plt.plot(df_stats['Valid_Loss'], 'g-o', label=\"Validation\")\n","\n","# Label the plot.\n","plt.title(\"Training & Validation Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.xticks([1, 2, 3, 4])\n","\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6wnXvQrifXYr","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}