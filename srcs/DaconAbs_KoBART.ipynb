{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.library, model load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, random_split\n",
    "\n",
    "from transformers import BartConfig, BartTokenizer, BartForConditionalGeneration, Adafactor, get_linear_schedule_with_warmup\n",
    "from kobart import get_kobart_tokenizer, get_pytorch_kobart_model\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_kobart_tokenizer()\n",
    "model = BartForConditionalGeneration.from_pretrained(get_pytorch_kobart_model(), return_dict=True)\n",
    "config = BartConfig(get_pytorch_kobart_model())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('../dataset/train.jsonl', 'r') as json_file:\n",
    "    train_list = list(json_file)\n",
    "\n",
    "trains = []\n",
    "for json_str in train_list:\n",
    "    line = json.loads(json_str)\n",
    "    trains.append(line)\n",
    "\n",
    "train_data = pd.DataFrame(trains)\n",
    "\n",
    "\n",
    "with open('../dataset/abstractive_test_v2.jsonl', 'r') as json_file:\n",
    "    test_list = list(json_file)\n",
    "\n",
    "tests = []\n",
    "for json_str in test_list:\n",
    "    line = json.loads(json_str)\n",
    "    tests.append(line)\n",
    "\n",
    "test_data = pd.DataFrame(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = train_data['article_original']\n",
    "labels = train_data['abstractive']\n",
    "test_ariticles = test_data['article_original']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round(elapsed))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max_length check "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trian input length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids_list=[]\n",
    "# attention_mask_list = []\n",
    "# for article in input_list:\n",
    "#     output = tokenizer.encode_plus(article, return_tensors=\"pt\")\n",
    "#     input_ids_list.append(output.input_ids)\n",
    "#     attention_mask_list.append(output.attention_mask)\n",
    "    \n",
    "# length_list =[]\n",
    "\n",
    "# for i in range(len(train_data)):\n",
    "#     length_list.append(input_ids_list[i].size()[1])\n",
    "# train_data['length'] = length_list\n",
    "# train_data = train_data.sort_values(by='length' ,ascending=False)\n",
    "# train_data.describe()\n",
    "\n",
    "# test_input_ids_list=[]\n",
    "# for article in test_input_list:\n",
    "#     test_input_ids_list.append(tokenizer.encode_plus(article, return_tensors=\"pt\").input_ids)\n",
    "\n",
    "# test_length_list=[]\n",
    "# for i in range(len(test_data)):\n",
    "#     test_length_list.append(test_input_ids_list[i].size()[1])\n",
    "# test_data['length'] = test_length_list\n",
    "# test_data = test_data.sort_values(by='length' ,ascending=False)\n",
    "# test_data.describe()\n",
    "\n",
    "# labels_list = []\n",
    "# for article in tqdm(labels):\n",
    "#     labels_list.append(tokenizer.encode_plus(article, return_tensors=\"pt\").input_ids)\n",
    "\n",
    "# label_length_list=[]\n",
    "# for i in range(len(labels_list)):\n",
    "#     label_length_list.append(labels_list[i].size()[1])\n",
    "# train_data['label_length'] = label_length_list\n",
    "# train_data = train_data.sort_values(by='label_length' ,ascending=False)\n",
    "# train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b7734e7741d4a098131eb3af6244559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=42803.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a92f1c5fdee470484f223767ab4c0a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9987.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "to list time:  0:00:00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e07e57d27a44075ae8db8556c34605c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=42803.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be0fe1097cd4961b249daa8d2026f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=42803.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f3586442a04db49945ee415dfede9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9987.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoding:  0:01:34\n",
      "append sep pad:  0:00:08\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "articles = articles.apply(' '.join)\n",
    "input_list = []\n",
    "for article in tqdm(articles):\n",
    "    str_list = [article, '</s>']\n",
    "    input_list.append(''.join(str_list))\n",
    "\n",
    "labels = labels + '</s>'\n",
    "\n",
    "test_ariticles = test_ariticles.apply(' '.join)\n",
    "test_input_list = []\n",
    "for article in tqdm(test_ariticles):\n",
    "    str_list = [article, '</s>']\n",
    "    test_input_list.append(''.join(str_list)) \n",
    "\n",
    "t1 = time.time()\n",
    "print('to list time: ', format_time(t1-t0))\n",
    "\n",
    "input_ids_list=[]\n",
    "attention_mask_list = []\n",
    "for article in tqdm(input_list):\n",
    "    output = tokenizer.encode_plus(article, max_length=512, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    input_ids_list.append(output.input_ids)\n",
    "    attention_mask_list.append(output.attention_mask)\n",
    "    \n",
    "labels_list = []\n",
    "for article in tqdm(labels):\n",
    "    labels_list.append(tokenizer.encode_plus(article, max_length=250, padding='max_length', truncation=True, return_tensors=\"pt\").input_ids)\n",
    "    \n",
    "test_input_ids_list=[]\n",
    "test_attention_mask_list = []\n",
    "for article in tqdm(test_input_list):\n",
    "    output = tokenizer.encode_plus(article, max_length=968, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    test_input_ids_list.append(output.input_ids)\n",
    "    test_attention_mask_list.append(output.attention_mask)\n",
    "\n",
    "t2 = time.time()\n",
    "print('Encoding: ', format_time(t2-t1))\n",
    "        \n",
    "new_input_ids = torch.zeros(len(input_ids_list), 513, dtype = torch.int64)\n",
    "new_attention_mask = torch.zeros(len(input_ids_list), 513, dtype = torch.int64)\n",
    "\n",
    "for i, input in enumerate(input_ids_list):\n",
    "    if input.eq(1).sum() == 1:\n",
    "        new_input_ids[i] = torch.cat((input, torch.tensor([3]).view(1,1)), dim=1)\n",
    "    else: new_input_ids[i] = torch.cat((input, torch.tensor([1]).view(1,1)), dim=1)\n",
    "        \n",
    "for i, input in enumerate(input_ids_list):\n",
    "    if input.eq(1).sum() == 1:\n",
    "        new_attention_mask[i] = torch.cat((attention_mask_list[i], torch.tensor([0]).view(1,1)), dim=1)\n",
    "    else: new_attention_mask[i] = torch.cat((attention_mask_list[i], torch.tensor([1]).view(1,1)), dim=1)\n",
    "        \n",
    "new_label_ids = torch.zeros(len(input_ids_list), 251, dtype = torch.int64)\n",
    "for i, input in enumerate(labels_list):\n",
    "    if input.eq(1).sum() == 1:\n",
    "        new_label_ids[i] = torch.cat((input, torch.tensor([3]).view(1,1)), dim=1)\n",
    "    else: new_label_ids[i] = torch.cat((input, torch.tensor([1]).view(1,1)), dim=1)\n",
    "        \n",
    "labels = torch.cat(labels_list, axis=0)\n",
    "labels[labels[:,:] == tokenizer.pad_token_id] = -100\n",
    "\n",
    "test_input_ids = torch.cat(test_input_ids_list, dim=0)\n",
    "test_attention_mask = torch.cat(test_attention_mask_list, dim=0)\n",
    "\n",
    "t3 = time.time()\n",
    "print('append sep pad: ', format_time(t3-t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38,523 training samples\n",
      "4,280 validation samples\n",
      "9,987 test samples\n"
     ]
    }
   ],
   "source": [
    "dataset = TensorDataset(new_input_ids, new_attention_mask, labels)\n",
    "train_size, val_size = round(len(train_data) * 0.9), round(len(train_data) * 0.1)\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_mask)\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))\n",
    "print('{:>5,} test samples'.format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 4\n",
    "val_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, sampler=SequentialSampler(train_dataset), batch_size=train_size)\n",
    "val_dataloader = DataLoader(val_dataset, sampler = SequentialSampler(val_dataset), batch_size = val_size)\n",
    "test_dataloader = DataLoader(test_dataset, sampler = SequentialSampler(test_dataset), batch_size = val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_NUM = 4 # 원하는 GPU 번호 입력\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device) # change allocation of current GPU\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '4, 5'\n",
    "model = nn.DataParallel(model, device_ids=[4,5], output_device=5)\n",
    "model.to(f'cuda:{model.device_ids[0]}')\n",
    "\n",
    "params = list(model.named_parameters())\n",
    "optimizer = Adafactor(model.parameters(),\n",
    "                     lr = 1e-4,\n",
    "                     relative_step=False,\n",
    "                     )\n",
    "epochs = 30\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                           num_warmup_steps= len(train_dataloader) // 15,\n",
    "                                           num_training_steps=total_steps)\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round(elapsed))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9aacddb115542a8a7f4e023ca8afb1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=30.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 30 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce069e2ca0c46f5bb62e3b9222626f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/twindoc_mt5_classification/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/opt/conda/envs/twindoc_mt5_classification/lib/python3.7/site-packages/transformers/optimization.py:506: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  exp_avg_sq_row.mul_(beta2t).add_(1.0 - beta2t, update.mean(dim=-1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch    50 of 9,631 Elased 0:00:22. Training loss: 17.803697452545165. Learning Rate: 7.78816199376947e-06\n",
      "Batch   100 of 9,631 Elased 0:00:40. Training loss: 17.962763719558716. Learning Rate: 1.557632398753894e-05\n",
      "Batch   150 of 9,631 Elased 0:01:00. Training loss: 18.132352879842124. Learning Rate: 2.3364485981308414e-05\n",
      "Batch   200 of 9,631 Elased 0:01:18. Training loss: 18.128156189918517. Learning Rate: 3.115264797507788e-05\n",
      "Batch   250 of 9,631 Elased 0:01:37. Training loss: 18.07415645980835. Learning Rate: 3.8940809968847354e-05\n",
      "Batch   300 of 9,631 Elased 0:01:56. Training loss: 18.012225008010866. Learning Rate: 4.672897196261683e-05\n",
      "Batch   350 of 9,631 Elased 0:02:15. Training loss: 17.96528716768537. Learning Rate: 5.4517133956386294e-05\n",
      "Batch   400 of 9,631 Elased 0:02:34. Training loss: 17.955538744926454. Learning Rate: 6.230529595015576e-05\n",
      "Batch   450 of 9,631 Elased 0:02:53. Training loss: 17.98933436287774. Learning Rate: 7.009345794392523e-05\n",
      "Batch   500 of 9,631 Elased 0:03:12. Training loss: 17.94647931098938. Learning Rate: 7.788161993769471e-05\n",
      "Batch   550 of 9,631 Elased 0:03:31. Training loss: 17.8669377933849. Learning Rate: 8.566978193146418e-05\n",
      "Batch   600 of 9,631 Elased 0:03:49. Training loss: 17.8899232673645. Learning Rate: 9.345794392523365e-05\n",
      "Batch   650 of 9,631 Elased 0:04:09. Training loss: 17.863237672952504. Learning Rate: 9.9997224997225e-05\n",
      "Batch   700 of 9,631 Elased 0:04:28. Training loss: 17.853884859085085. Learning Rate: 9.997988122988123e-05\n",
      "Batch   750 of 9,631 Elased 0:04:46. Training loss: 17.81160600789388. Learning Rate: 9.996253746253746e-05\n",
      "Batch   800 of 9,631 Elased 0:05:05. Training loss: 17.822815232276916. Learning Rate: 9.994519369519369e-05\n",
      "Batch   850 of 9,631 Elased 0:05:24. Training loss: 17.772851445815142. Learning Rate: 9.992784992784994e-05\n",
      "Batch   900 of 9,631 Elased 0:05:43. Training loss: 17.77695920202467. Learning Rate: 9.991050616050617e-05\n",
      "Batch   950 of 9,631 Elased 0:06:02. Training loss: 17.756199124988758. Learning Rate: 9.98931623931624e-05\n",
      "Batch 1,000 of 9,631 Elased 0:06:21. Training loss: 17.754066373825072. Learning Rate: 9.987581862581862e-05\n",
      "Batch 1,050 of 9,631 Elased 0:06:40. Training loss: 17.738583199637276. Learning Rate: 9.985847485847485e-05\n",
      "Batch 1,100 of 9,631 Elased 0:06:59. Training loss: 17.7368024409901. Learning Rate: 9.98411310911311e-05\n",
      "Batch 1,150 of 9,631 Elased 0:07:18. Training loss: 17.711749239382538. Learning Rate: 9.982378732378734e-05\n",
      "Batch 1,200 of 9,631 Elased 0:07:37. Training loss: 17.686159110069276. Learning Rate: 9.980644355644357e-05\n",
      "Batch 1,250 of 9,631 Elased 0:07:56. Training loss: 17.664598805236817. Learning Rate: 9.97890997890998e-05\n",
      "Batch 1,300 of 9,631 Elased 0:08:15. Training loss: 17.66392580472506. Learning Rate: 9.977175602175603e-05\n",
      "Batch 1,350 of 9,631 Elased 0:08:35. Training loss: 17.633046515429463. Learning Rate: 9.975441225441226e-05\n",
      "Batch 1,400 of 9,631 Elased 0:08:53. Training loss: 17.612669288771492. Learning Rate: 9.97370684870685e-05\n",
      "Batch 1,450 of 9,631 Elased 0:09:12. Training loss: 17.5964232221143. Learning Rate: 9.971972471972473e-05\n",
      "Batch 1,500 of 9,631 Elased 0:09:31. Training loss: 17.58271763420105. Learning Rate: 9.970238095238096e-05\n",
      "Batch 1,550 of 9,631 Elased 0:09:51. Training loss: 17.561486872396163. Learning Rate: 9.968503718503719e-05\n",
      "Batch 1,600 of 9,631 Elased 0:10:10. Training loss: 17.554451882839203. Learning Rate: 9.966769341769342e-05\n",
      "Batch 1,650 of 9,631 Elased 0:10:29. Training loss: 17.53758459784768. Learning Rate: 9.965034965034964e-05\n",
      "Batch 1,700 of 9,631 Elased 0:10:48. Training loss: 17.52137579020332. Learning Rate: 9.963300588300589e-05\n",
      "Batch 1,750 of 9,631 Elased 0:11:07. Training loss: 17.494425301688057. Learning Rate: 9.961566211566212e-05\n",
      "Batch 1,800 of 9,631 Elased 0:11:27. Training loss: 17.468669141663444. Learning Rate: 9.959831834831836e-05\n",
      "Batch 1,850 of 9,631 Elased 0:11:46. Training loss: 17.450484076319515. Learning Rate: 9.958097458097459e-05\n",
      "Batch 1,900 of 9,631 Elased 0:12:05. Training loss: 17.4340001889279. Learning Rate: 9.956363081363082e-05\n",
      "Batch 1,950 of 9,631 Elased 0:12:24. Training loss: 17.41193296970465. Learning Rate: 9.954628704628705e-05\n",
      "Batch 2,000 of 9,631 Elased 0:12:43. Training loss: 17.38825427055359. Learning Rate: 9.952894327894329e-05\n",
      "Batch 2,050 of 9,631 Elased 0:13:03. Training loss: 17.369192042001863. Learning Rate: 9.951159951159952e-05\n",
      "Batch 2,100 of 9,631 Elased 0:13:22. Training loss: 17.357045366196406. Learning Rate: 9.949425574425575e-05\n",
      "Batch 2,150 of 9,631 Elased 0:13:41. Training loss: 17.338921844571136. Learning Rate: 9.947691197691198e-05\n",
      "Batch 2,200 of 9,631 Elased 0:14:00. Training loss: 17.321088021885267. Learning Rate: 9.945956820956821e-05\n",
      "Batch 2,250 of 9,631 Elased 0:14:19. Training loss: 17.285986735873752. Learning Rate: 9.944222444222445e-05\n",
      "Batch 2,300 of 9,631 Elased 0:14:38. Training loss: 17.267765934156333. Learning Rate: 9.942488067488068e-05\n",
      "Batch 2,350 of 9,631 Elased 0:14:57. Training loss: 17.249828654350118. Learning Rate: 9.940753690753691e-05\n",
      "Batch 2,400 of 9,631 Elased 0:15:16. Training loss: 17.236995263497036. Learning Rate: 9.939019314019314e-05\n",
      "Batch 2,450 of 9,631 Elased 0:15:35. Training loss: 17.21070233014165. Learning Rate: 9.937284937284938e-05\n",
      "Batch 2,500 of 9,631 Elased 0:15:54. Training loss: 17.195546514892577. Learning Rate: 9.935550560550561e-05\n",
      "Batch 2,550 of 9,631 Elased 0:16:12. Training loss: 17.177369585224227. Learning Rate: 9.933816183816185e-05\n",
      "Batch 2,600 of 9,631 Elased 0:16:32. Training loss: 17.164600460345927. Learning Rate: 9.932081807081808e-05\n",
      "Batch 2,650 of 9,631 Elased 0:16:53. Training loss: 17.15788974222147. Learning Rate: 9.930347430347431e-05\n",
      "Batch 2,700 of 9,631 Elased 0:17:14. Training loss: 17.143690249125164. Learning Rate: 9.928613053613054e-05\n",
      "Batch 2,750 of 9,631 Elased 0:17:35. Training loss: 17.122399794838646. Learning Rate: 9.926878676878677e-05\n",
      "Batch 2,800 of 9,631 Elased 0:17:56. Training loss: 17.105082900864737. Learning Rate: 9.9251443001443e-05\n",
      "Batch 2,850 of 9,631 Elased 0:18:17. Training loss: 17.0869561958313. Learning Rate: 9.923409923409924e-05\n",
      "Batch 2,900 of 9,631 Elased 0:18:38. Training loss: 17.072083669530933. Learning Rate: 9.921675546675547e-05\n",
      "Batch 2,950 of 9,631 Elased 0:18:59. Training loss: 17.04783699811515. Learning Rate: 9.91994116994117e-05\n",
      "Batch 3,000 of 9,631 Elased 0:19:20. Training loss: 17.037561327616373. Learning Rate: 9.918206793206793e-05\n",
      "Batch 3,050 of 9,631 Elased 0:19:41. Training loss: 17.014828882999108. Learning Rate: 9.916472416472416e-05\n",
      "Batch 3,100 of 9,631 Elased 0:20:02. Training loss: 16.998652708299698. Learning Rate: 9.91473803973804e-05\n",
      "Batch 3,150 of 9,631 Elased 0:20:23. Training loss: 16.97214789102948. Learning Rate: 9.913003663003663e-05\n",
      "Batch 3,200 of 9,631 Elased 0:20:44. Training loss: 16.946876115202905. Learning Rate: 9.911269286269287e-05\n",
      "Batch 3,250 of 9,631 Elased 0:21:05. Training loss: 16.937036400428184. Learning Rate: 9.90953490953491e-05\n",
      "Batch 3,300 of 9,631 Elased 0:21:26. Training loss: 16.92489088202968. Learning Rate: 9.907800532800533e-05\n",
      "Batch 3,350 of 9,631 Elased 0:21:47. Training loss: 16.912812096894676. Learning Rate: 9.906066156066156e-05\n",
      "Batch 3,400 of 9,631 Elased 0:22:08. Training loss: 16.8927996747634. Learning Rate: 9.90433177933178e-05\n",
      "Batch 3,450 of 9,631 Elased 0:22:29. Training loss: 16.875245114478513. Learning Rate: 9.902597402597403e-05\n",
      "Batch 3,500 of 9,631 Elased 0:22:51. Training loss: 16.856371146065847. Learning Rate: 9.900863025863026e-05\n",
      "Batch 3,550 of 9,631 Elased 0:23:12. Training loss: 16.843627887242278. Learning Rate: 9.899128649128649e-05\n",
      "Batch 3,600 of 9,631 Elased 0:23:33. Training loss: 16.83009566174613. Learning Rate: 9.897394272394272e-05\n",
      "Batch 3,650 of 9,631 Elased 0:23:54. Training loss: 16.81326966037489. Learning Rate: 9.895659895659895e-05\n",
      "Batch 3,700 of 9,631 Elased 0:24:15. Training loss: 16.784713358492464. Learning Rate: 9.89392551892552e-05\n",
      "Batch 3,750 of 9,631 Elased 0:24:36. Training loss: 16.763495259857176. Learning Rate: 9.892191142191142e-05\n",
      "Batch 3,800 of 9,631 Elased 0:24:57. Training loss: 16.74283905430844. Learning Rate: 9.890456765456765e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3,850 of 9,631 Elased 0:25:18. Training loss: 16.71668427405419. Learning Rate: 9.88872238872239e-05\n",
      "Batch 3,900 of 9,631 Elased 0:25:39. Training loss: 16.692634248244456. Learning Rate: 9.886988011988012e-05\n",
      "Batch 3,950 of 9,631 Elased 0:26:00. Training loss: 16.67965532689155. Learning Rate: 9.885253635253637e-05\n",
      "Batch 4,000 of 9,631 Elased 0:26:21. Training loss: 16.657658908367157. Learning Rate: 9.88351925851926e-05\n",
      "Batch 4,050 of 9,631 Elased 0:26:42. Training loss: 16.632928882410496. Learning Rate: 9.881784881784883e-05\n",
      "Batch 4,100 of 9,631 Elased 0:27:03. Training loss: 16.61508051848993. Learning Rate: 9.880050505050506e-05\n",
      "Batch 4,150 of 9,631 Elased 0:27:24. Training loss: 16.596438727091595. Learning Rate: 9.878316128316128e-05\n",
      "Batch 4,200 of 9,631 Elased 0:27:45. Training loss: 16.57544740290869. Learning Rate: 9.876581751581751e-05\n",
      "Batch 4,250 of 9,631 Elased 0:28:06. Training loss: 16.558234784070184. Learning Rate: 9.874847374847376e-05\n",
      "Batch 4,300 of 9,631 Elased 0:28:27. Training loss: 16.534248969055884. Learning Rate: 9.873112998112999e-05\n",
      "Batch 4,350 of 9,631 Elased 0:28:48. Training loss: 16.509369853008753. Learning Rate: 9.871378621378622e-05\n",
      "Batch 4,400 of 9,631 Elased 0:29:09. Training loss: 16.488364966782658. Learning Rate: 9.869644244644244e-05\n",
      "Batch 4,450 of 9,631 Elased 0:29:30. Training loss: 16.47459620400761. Learning Rate: 9.867909867909869e-05\n",
      "Batch 4,500 of 9,631 Elased 0:29:51. Training loss: 16.451367633183796. Learning Rate: 9.866175491175492e-05\n",
      "Batch 4,550 of 9,631 Elased 0:30:13. Training loss: 16.426496552477825. Learning Rate: 9.864441114441116e-05\n",
      "Batch 4,600 of 9,631 Elased 0:30:34. Training loss: 16.400483950532. Learning Rate: 9.862706737706739e-05\n",
      "Batch 4,650 of 9,631 Elased 0:30:55. Training loss: 16.365854207520844. Learning Rate: 9.860972360972362e-05\n",
      "Batch 4,700 of 9,631 Elased 0:31:16. Training loss: 16.335174645768834. Learning Rate: 9.859237984237985e-05\n",
      "Batch 4,750 of 9,631 Elased 0:31:37. Training loss: 16.299735138541774. Learning Rate: 9.857503607503608e-05\n",
      "Batch 4,800 of 9,631 Elased 0:31:58. Training loss: 16.26202456553777. Learning Rate: 9.855769230769232e-05\n",
      "Batch 4,850 of 9,631 Elased 0:32:19. Training loss: 16.22433811581012. Learning Rate: 9.854034854034855e-05\n",
      "Batch 4,900 of 9,631 Elased 0:32:41. Training loss: 16.183688682925943. Learning Rate: 9.852300477300478e-05\n",
      "Batch 4,950 of 9,631 Elased 0:33:02. Training loss: 16.14352229060549. Learning Rate: 9.850566100566101e-05\n",
      "Batch 5,000 of 9,631 Elased 0:33:23. Training loss: 16.10216908502579. Learning Rate: 9.848831723831724e-05\n",
      "Batch 5,050 of 9,631 Elased 0:33:44. Training loss: 16.060110127194093. Learning Rate: 9.847097347097347e-05\n",
      "Batch 5,100 of 9,631 Elased 0:34:05. Training loss: 16.012328935043485. Learning Rate: 9.845362970362971e-05\n",
      "Batch 5,150 of 9,631 Elased 0:34:27. Training loss: 15.971215188572708. Learning Rate: 9.843628593628594e-05\n",
      "Batch 5,200 of 9,631 Elased 0:34:47. Training loss: 15.928976514981343. Learning Rate: 9.841894216894218e-05\n",
      "Batch 5,250 of 9,631 Elased 0:35:09. Training loss: 15.886169547671363. Learning Rate: 9.840159840159841e-05\n",
      "Batch 5,300 of 9,631 Elased 0:35:29. Training loss: 15.845488203876423. Learning Rate: 9.838425463425464e-05\n",
      "Batch 5,350 of 9,631 Elased 0:35:50. Training loss: 15.807745766595145. Learning Rate: 9.836691086691087e-05\n",
      "Batch 5,400 of 9,631 Elased 0:36:10. Training loss: 15.7692096096498. Learning Rate: 9.834956709956711e-05\n",
      "Batch 5,450 of 9,631 Elased 0:36:31. Training loss: 15.726336610164118. Learning Rate: 9.833222333222334e-05\n",
      "Batch 5,500 of 9,631 Elased 0:36:51. Training loss: 15.683864364103837. Learning Rate: 9.831487956487957e-05\n",
      "Batch 5,550 of 9,631 Elased 0:37:12. Training loss: 15.644055969994348. Learning Rate: 9.82975357975358e-05\n",
      "Batch 5,600 of 9,631 Elased 0:37:33. Training loss: 15.600854162658964. Learning Rate: 9.828019203019203e-05\n",
      "Batch 5,650 of 9,631 Elased 0:37:53. Training loss: 15.562586507966033. Learning Rate: 9.826284826284827e-05\n",
      "Batch 5,700 of 9,631 Elased 0:38:14. Training loss: 15.51804600933142. Learning Rate: 9.82455044955045e-05\n",
      "Batch 5,750 of 9,631 Elased 0:38:34. Training loss: 15.472571394713029. Learning Rate: 9.822816072816073e-05\n",
      "Batch 5,800 of 9,631 Elased 0:38:55. Training loss: 15.432035595795204. Learning Rate: 9.821081696081696e-05\n",
      "Batch 5,850 of 9,631 Elased 0:39:16. Training loss: 15.392539768382015. Learning Rate: 9.81934731934732e-05\n",
      "Batch 5,900 of 9,631 Elased 0:39:36. Training loss: 15.350163521766662. Learning Rate: 9.817612942612943e-05\n",
      "Batch 5,950 of 9,631 Elased 0:39:57. Training loss: 15.312251679075866. Learning Rate: 9.815878565878567e-05\n",
      "Batch 6,000 of 9,631 Elased 0:40:18. Training loss: 15.275844921588897. Learning Rate: 9.81414418914419e-05\n",
      "Batch 6,050 of 9,631 Elased 0:40:38. Training loss: 15.236879327789811. Learning Rate: 9.812409812409813e-05\n",
      "Batch 6,100 of 9,631 Elased 0:40:58. Training loss: 15.20191812906109. Learning Rate: 9.810675435675436e-05\n",
      "Batch 6,150 of 9,631 Elased 0:41:19. Training loss: 15.160176861848289. Learning Rate: 9.808941058941059e-05\n",
      "Batch 6,200 of 9,631 Elased 0:41:40. Training loss: 15.11737131857103. Learning Rate: 9.807206682206682e-05\n",
      "Batch 6,250 of 9,631 Elased 0:42:00. Training loss: 15.080747875213623. Learning Rate: 9.805472305472306e-05\n",
      "Batch 6,300 of 9,631 Elased 0:42:21. Training loss: 15.042769137715537. Learning Rate: 9.803737928737929e-05\n",
      "Batch 6,350 of 9,631 Elased 0:42:42. Training loss: 15.007649692475326. Learning Rate: 9.802003552003552e-05\n",
      "Batch 6,400 of 9,631 Elased 0:43:03. Training loss: 14.97283870369196. Learning Rate: 9.800269175269175e-05\n",
      "Batch 6,450 of 9,631 Elased 0:43:23. Training loss: 14.936336783549581. Learning Rate: 9.798534798534798e-05\n",
      "Batch 6,500 of 9,631 Elased 0:43:44. Training loss: 14.895850661717928. Learning Rate: 9.796800421800422e-05\n",
      "Batch 6,550 of 9,631 Elased 0:44:04. Training loss: 14.861395471514637. Learning Rate: 9.795066045066045e-05\n",
      "Batch 6,600 of 9,631 Elased 0:44:25. Training loss: 14.826603556329554. Learning Rate: 9.79333166833167e-05\n",
      "Batch 6,650 of 9,631 Elased 0:44:45. Training loss: 14.792513515716209. Learning Rate: 9.791597291597292e-05\n",
      "Batch 6,700 of 9,631 Elased 0:45:06. Training loss: 14.75577920963515. Learning Rate: 9.789862914862915e-05\n",
      "Batch 6,750 of 9,631 Elased 0:45:27. Training loss: 14.719924911216454. Learning Rate: 9.788128538128538e-05\n",
      "Batch 6,800 of 9,631 Elased 0:45:47. Training loss: 14.683586559506024. Learning Rate: 9.786394161394163e-05\n",
      "Batch 6,850 of 9,631 Elased 0:46:08. Training loss: 14.64828719459311. Learning Rate: 9.784659784659785e-05\n",
      "Batch 6,900 of 9,631 Elased 0:46:29. Training loss: 14.615248826483022. Learning Rate: 9.782925407925408e-05\n",
      "Batch 6,950 of 9,631 Elased 0:46:49. Training loss: 14.58231690283302. Learning Rate: 9.781191031191031e-05\n",
      "Batch 7,000 of 9,631 Elased 0:47:10. Training loss: 14.548293628624508. Learning Rate: 9.779456654456654e-05\n",
      "Batch 7,050 of 9,631 Elased 0:47:31. Training loss: 14.515609818992885. Learning Rate: 9.777722277722277e-05\n",
      "Batch 7,100 of 9,631 Elased 0:47:51. Training loss: 14.483472036912408. Learning Rate: 9.775987900987901e-05\n",
      "Batch 7,150 of 9,631 Elased 0:48:12. Training loss: 14.452080699413806. Learning Rate: 9.774253524253524e-05\n",
      "Batch 7,200 of 9,631 Elased 0:48:33. Training loss: 14.420402699708939. Learning Rate: 9.772519147519149e-05\n",
      "Batch 7,250 of 9,631 Elased 0:48:53. Training loss: 14.386723292778278. Learning Rate: 9.770784770784772e-05\n",
      "Batch 7,300 of 9,631 Elased 0:49:14. Training loss: 14.356896694718975. Learning Rate: 9.769050394050395e-05\n",
      "Batch 7,350 of 9,631 Elased 0:49:35. Training loss: 14.321485895948346. Learning Rate: 9.767316017316019e-05\n",
      "Batch 7,400 of 9,631 Elased 0:49:55. Training loss: 14.287953131327757. Learning Rate: 9.765581640581642e-05\n",
      "Batch 7,450 of 9,631 Elased 0:50:16. Training loss: 14.256612689895118. Learning Rate: 9.763847263847265e-05\n",
      "Batch 7,500 of 9,631 Elased 0:50:36. Training loss: 14.224448654683432. Learning Rate: 9.762112887112888e-05\n",
      "Batch 7,550 of 9,631 Elased 0:50:57. Training loss: 14.194346356612957. Learning Rate: 9.76037851037851e-05\n",
      "Batch 7,600 of 9,631 Elased 0:51:18. Training loss: 14.164089161659541. Learning Rate: 9.758644133644133e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7,650 of 9,631 Elased 0:51:38. Training loss: 14.133833759033601. Learning Rate: 9.756909756909758e-05\n",
      "Batch 7,700 of 9,631 Elased 0:51:59. Training loss: 14.103852274201133. Learning Rate: 9.75517538017538e-05\n",
      "Batch 7,750 of 9,631 Elased 0:52:19. Training loss: 14.071223438385994. Learning Rate: 9.753441003441004e-05\n",
      "Batch 7,800 of 9,631 Elased 0:52:40. Training loss: 14.044158413410187. Learning Rate: 9.751706626706626e-05\n",
      "Batch 7,850 of 9,631 Elased 0:53:00. Training loss: 14.014947387610272. Learning Rate: 9.749972249972251e-05\n",
      "Batch 7,900 of 9,631 Elased 0:53:21. Training loss: 13.985232609555691. Learning Rate: 9.748237873237874e-05\n",
      "Batch 7,950 of 9,631 Elased 0:53:42. Training loss: 13.95834605079027. Learning Rate: 9.746503496503498e-05\n",
      "Batch 8,000 of 9,631 Elased 0:54:02. Training loss: 13.93078393727541. Learning Rate: 9.744769119769121e-05\n",
      "Batch 8,050 of 9,631 Elased 0:54:23. Training loss: 13.903370044009286. Learning Rate: 9.743034743034744e-05\n",
      "Batch 8,100 of 9,631 Elased 0:54:44. Training loss: 13.87489651574029. Learning Rate: 9.741300366300367e-05\n",
      "Batch 8,150 of 9,631 Elased 0:55:04. Training loss: 13.848790479671736. Learning Rate: 9.73956598956599e-05\n",
      "Batch 8,200 of 9,631 Elased 0:55:24. Training loss: 13.820805438786017. Learning Rate: 9.737831612831614e-05\n",
      "Batch 8,250 of 9,631 Elased 0:55:45. Training loss: 13.795792658372358. Learning Rate: 9.736097236097237e-05\n",
      "Batch 8,300 of 9,631 Elased 0:56:06. Training loss: 13.767324378634074. Learning Rate: 9.73436285936286e-05\n",
      "Batch 8,350 of 9,631 Elased 0:56:26. Training loss: 13.740166383617652. Learning Rate: 9.732628482628483e-05\n",
      "Batch 8,400 of 9,631 Elased 0:56:47. Training loss: 13.712798203684034. Learning Rate: 9.730894105894106e-05\n",
      "Batch 8,450 of 9,631 Elased 0:57:08. Training loss: 13.685248498973056. Learning Rate: 9.729159729159729e-05\n",
      "Batch 8,500 of 9,631 Elased 0:57:28. Training loss: 13.660158917651456. Learning Rate: 9.727425352425353e-05\n",
      "Batch 8,550 of 9,631 Elased 0:57:49. Training loss: 13.632381018699958. Learning Rate: 9.725690975690976e-05\n",
      "Batch 8,600 of 9,631 Elased 0:58:10. Training loss: 13.60819238601729. Learning Rate: 9.7239565989566e-05\n",
      "Batch 8,650 of 9,631 Elased 0:58:30. Training loss: 13.581272247568032. Learning Rate: 9.722222222222223e-05\n",
      "Batch 8,700 of 9,631 Elased 0:58:51. Training loss: 13.556797861833681. Learning Rate: 9.720487845487846e-05\n",
      "Batch 8,750 of 9,631 Elased 0:59:11. Training loss: 13.532719304929461. Learning Rate: 9.718753468753469e-05\n",
      "Batch 8,800 of 9,631 Elased 0:59:32. Training loss: 13.508212200078097. Learning Rate: 9.717019092019093e-05\n",
      "Batch 8,850 of 9,631 Elased 0:59:53. Training loss: 13.483271211203883. Learning Rate: 9.715284715284716e-05\n",
      "Batch 8,900 of 9,631 Elased 1:00:13. Training loss: 13.459256542237958. Learning Rate: 9.713550338550339e-05\n",
      "Batch 8,950 of 9,631 Elased 1:00:34. Training loss: 13.434561480090604. Learning Rate: 9.711815961815962e-05\n",
      "Batch 9,000 of 9,631 Elased 1:00:55. Training loss: 13.409609670003254. Learning Rate: 9.710081585081585e-05\n",
      "Batch 9,050 of 9,631 Elased 1:01:15. Training loss: 13.387336368824236. Learning Rate: 9.708347208347209e-05\n",
      "Batch 9,100 of 9,631 Elased 1:01:36. Training loss: 13.36306436428657. Learning Rate: 9.706612831612832e-05\n",
      "Batch 9,150 of 9,631 Elased 1:01:57. Training loss: 13.338554361426764. Learning Rate: 9.704878454878455e-05\n",
      "Batch 9,200 of 9,631 Elased 1:02:17. Training loss: 13.317983113008996. Learning Rate: 9.703144078144078e-05\n",
      "Batch 9,250 of 9,631 Elased 1:02:38. Training loss: 13.294154544675672. Learning Rate: 9.701409701409702e-05\n",
      "Batch 9,300 of 9,631 Elased 1:02:59. Training loss: 13.272127344121214. Learning Rate: 9.699675324675325e-05\n",
      "Batch 9,350 of 9,631 Elased 1:03:20. Training loss: 13.24869604105618. Learning Rate: 9.69794094794095e-05\n",
      "Batch 9,400 of 9,631 Elased 1:03:40. Training loss: 13.225391268298981. Learning Rate: 9.696206571206572e-05\n",
      "Batch 9,450 of 9,631 Elased 1:04:01. Training loss: 13.202914503768639. Learning Rate: 9.694472194472195e-05\n",
      "Batch 9,500 of 9,631 Elased 1:04:22. Training loss: 13.18031656247691. Learning Rate: 9.692737817737818e-05\n",
      "Batch 9,550 of 9,631 Elased 1:04:42. Training loss: 13.158301290417217. Learning Rate: 9.691003441003441e-05\n",
      "Batch 9,600 of 9,631 Elased 1:05:03. Training loss: 13.137496821905176. Learning Rate: 9.689269064269064e-05\n",
      "\n",
      "\n",
      "  Average training loss: 13.12\n",
      "  Training epcoh took: 1:05:16\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c267182dd84a1ba9df631c40ed6ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=67.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Validation Loss: 9.24\n",
      "  Validation took: 0:00:42\n",
      "\n",
      "======== Epoch 2 / 30 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded348ab685e449da861addebc0d20e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch    50 of 9,631 Elased 0:00:21. Training loss: 8.992788877487182. Learning Rate: 9.686459373959374e-05\n",
      "Batch   100 of 9,631 Elased 0:00:42. Training loss: 8.897632279396056. Learning Rate: 9.684724997224997e-05\n",
      "Batch   150 of 9,631 Elased 0:01:02. Training loss: 8.856861934661865. Learning Rate: 9.68299062049062e-05\n",
      "Batch   200 of 9,631 Elased 0:01:23. Training loss: 8.920357007980346. Learning Rate: 9.681256243756243e-05\n",
      "Batch   250 of 9,631 Elased 0:01:43. Training loss: 8.819697729110718. Learning Rate: 9.679521867021868e-05\n",
      "Batch   300 of 9,631 Elased 0:02:04. Training loss: 8.703252104123434. Learning Rate: 9.67778749028749e-05\n",
      "Batch   350 of 9,631 Elased 0:02:24. Training loss: 8.704681302479335. Learning Rate: 9.676053113553113e-05\n",
      "Batch   400 of 9,631 Elased 0:02:45. Training loss: 8.710780429840089. Learning Rate: 9.674318736818738e-05\n",
      "Batch   450 of 9,631 Elased 0:03:06. Training loss: 8.740195255279541. Learning Rate: 9.67258436008436e-05\n",
      "Batch   500 of 9,631 Elased 0:03:26. Training loss: 8.730611130714417. Learning Rate: 9.670849983349984e-05\n",
      "Batch   550 of 9,631 Elased 0:03:47. Training loss: 8.696887793107466. Learning Rate: 9.669115606615608e-05\n",
      "Batch   600 of 9,631 Elased 0:04:07. Training loss: 8.699213127295176. Learning Rate: 9.667381229881231e-05\n",
      "Batch   650 of 9,631 Elased 0:04:28. Training loss: 8.667330234967745. Learning Rate: 9.665646853146854e-05\n",
      "Batch   700 of 9,631 Elased 0:04:48. Training loss: 8.667719777652195. Learning Rate: 9.663912476412477e-05\n",
      "Batch   750 of 9,631 Elased 0:05:09. Training loss: 8.63640517616272. Learning Rate: 9.6621780996781e-05\n",
      "Batch   800 of 9,631 Elased 0:05:30. Training loss: 8.622095587849618. Learning Rate: 9.660443722943724e-05\n",
      "Batch   850 of 9,631 Elased 0:05:50. Training loss: 8.612888067469877. Learning Rate: 9.658709346209347e-05\n",
      "Batch   900 of 9,631 Elased 0:06:11. Training loss: 8.62831002023485. Learning Rate: 9.65697496947497e-05\n",
      "Batch   950 of 9,631 Elased 0:06:32. Training loss: 8.618347413414403. Learning Rate: 9.655240592740593e-05\n",
      "Batch 1,000 of 9,631 Elased 0:06:52. Training loss: 8.62261160516739. Learning Rate: 9.653506216006216e-05\n",
      "Batch 1,050 of 9,631 Elased 0:07:13. Training loss: 8.627145715895153. Learning Rate: 9.65177183927184e-05\n",
      "Batch 1,100 of 9,631 Elased 0:07:34. Training loss: 8.623784022331238. Learning Rate: 9.650037462537464e-05\n",
      "Batch 1,150 of 9,631 Elased 0:07:55. Training loss: 8.624909139301465. Learning Rate: 9.648303085803087e-05\n",
      "Batch 1,200 of 9,631 Elased 0:08:15. Training loss: 8.618223257859547. Learning Rate: 9.64656870906871e-05\n",
      "Batch 1,250 of 9,631 Elased 0:08:36. Training loss: 8.615781087875366. Learning Rate: 9.644834332334333e-05\n",
      "Batch 1,300 of 9,631 Elased 0:08:56. Training loss: 8.608304439691397. Learning Rate: 9.643099955599956e-05\n",
      "Batch 1,350 of 9,631 Elased 0:09:17. Training loss: 8.599511918668394. Learning Rate: 9.641365578865579e-05\n",
      "Batch 1,400 of 9,631 Elased 0:09:38. Training loss: 8.595776135240282. Learning Rate: 9.639631202131203e-05\n",
      "Batch 1,450 of 9,631 Elased 0:09:58. Training loss: 8.5890336944317. Learning Rate: 9.637896825396826e-05\n",
      "Batch 1,500 of 9,631 Elased 0:10:19. Training loss: 8.571468403816223. Learning Rate: 9.636162448662449e-05\n",
      "Batch 1,550 of 9,631 Elased 0:10:40. Training loss: 8.56060704354317. Learning Rate: 9.634428071928072e-05\n",
      "Batch 1,600 of 9,631 Elased 0:11:00. Training loss: 8.553860964179039. Learning Rate: 9.632693695193695e-05\n",
      "Batch 1,650 of 9,631 Elased 0:11:21. Training loss: 8.555915146741. Learning Rate: 9.630959318459319e-05\n",
      "Batch 1,700 of 9,631 Elased 0:11:42. Training loss: 8.54372962699217. Learning Rate: 9.629224941724942e-05\n",
      "Batch 1,750 of 9,631 Elased 0:12:02. Training loss: 8.540043330601284. Learning Rate: 9.627490564990566e-05\n",
      "Batch 1,800 of 9,631 Elased 0:12:23. Training loss: 8.52299503935708. Learning Rate: 9.625756188256189e-05\n",
      "Batch 1,850 of 9,631 Elased 0:12:44. Training loss: 8.521509138829. Learning Rate: 9.624021811521812e-05\n",
      "Batch 1,900 of 9,631 Elased 0:13:04. Training loss: 8.516776314032706. Learning Rate: 9.622287434787435e-05\n",
      "Batch 1,950 of 9,631 Elased 0:13:25. Training loss: 8.500593517743624. Learning Rate: 9.620553058053059e-05\n",
      "Batch 2,000 of 9,631 Elased 0:13:45. Training loss: 8.484650918960572. Learning Rate: 9.618818681318682e-05\n",
      "Batch 2,050 of 9,631 Elased 0:14:06. Training loss: 8.47437511467352. Learning Rate: 9.617084304584305e-05\n",
      "Batch 2,100 of 9,631 Elased 0:14:26. Training loss: 8.475357408523559. Learning Rate: 9.615349927849928e-05\n",
      "Batch 2,150 of 9,631 Elased 0:14:47. Training loss: 8.469536858935689. Learning Rate: 9.613615551115551e-05\n",
      "Batch 2,200 of 9,631 Elased 0:15:07. Training loss: 8.46126350923018. Learning Rate: 9.611881174381174e-05\n",
      "Batch 2,250 of 9,631 Elased 0:15:27. Training loss: 8.446672145843506. Learning Rate: 9.610146797646798e-05\n",
      "Batch 2,300 of 9,631 Elased 0:15:48. Training loss: 8.43161558565886. Learning Rate: 9.608412420912421e-05\n",
      "Batch 2,350 of 9,631 Elased 0:16:08. Training loss: 8.420264569749223. Learning Rate: 9.606678044178044e-05\n",
      "Batch 2,400 of 9,631 Elased 0:16:29. Training loss: 8.408504172762235. Learning Rate: 9.604943667443668e-05\n",
      "Batch 2,450 of 9,631 Elased 0:16:50. Training loss: 8.389431244597143. Learning Rate: 9.603209290709291e-05\n",
      "Batch 2,500 of 9,631 Elased 0:17:10. Training loss: 8.37535432434082. Learning Rate: 9.601474913974916e-05\n",
      "Batch 2,550 of 9,631 Elased 0:17:31. Training loss: 8.365389070136874. Learning Rate: 9.599740537240538e-05\n",
      "Batch 2,600 of 9,631 Elased 0:17:52. Training loss: 8.347946496009827. Learning Rate: 9.598006160506161e-05\n",
      "Batch 2,650 of 9,631 Elased 0:18:12. Training loss: 8.337707133742999. Learning Rate: 9.596271783771784e-05\n",
      "Batch 2,700 of 9,631 Elased 0:18:33. Training loss: 8.326821555561489. Learning Rate: 9.594537407037407e-05\n",
      "Batch 2,750 of 9,631 Elased 0:18:54. Training loss: 8.305833426909013. Learning Rate: 9.59280303030303e-05\n",
      "Batch 2,800 of 9,631 Elased 0:19:14. Training loss: 8.29035649214472. Learning Rate: 9.591068653568654e-05\n",
      "Batch 2,850 of 9,631 Elased 0:19:35. Training loss: 8.270943971265826. Learning Rate: 9.589334276834277e-05\n",
      "Batch 2,900 of 9,631 Elased 0:19:55. Training loss: 8.255568692437533. Learning Rate: 9.5875999000999e-05\n",
      "Batch 2,950 of 9,631 Elased 0:20:16. Training loss: 8.235946881892318. Learning Rate: 9.585865523365523e-05\n",
      "Batch 3,000 of 9,631 Elased 0:20:37. Training loss: 8.22122419500351. Learning Rate: 9.584131146631146e-05\n",
      "Batch 3,050 of 9,631 Elased 0:20:57. Training loss: 8.201278643451753. Learning Rate: 9.58239676989677e-05\n",
      "Batch 3,100 of 9,631 Elased 0:21:18. Training loss: 8.180668657441293. Learning Rate: 9.580662393162393e-05\n",
      "Batch 3,150 of 9,631 Elased 0:21:39. Training loss: 8.152630641044132. Learning Rate: 9.578928016428018e-05\n",
      "Batch 3,200 of 9,631 Elased 0:21:59. Training loss: 8.13197343222797. Learning Rate: 9.57719363969364e-05\n",
      "Batch 3,250 of 9,631 Elased 0:22:20. Training loss: 8.112969346780043. Learning Rate: 9.575459262959263e-05\n",
      "Batch 3,300 of 9,631 Elased 0:22:40. Training loss: 8.099191938963804. Learning Rate: 9.573724886224886e-05\n",
      "Batch 3,350 of 9,631 Elased 0:23:01. Training loss: 8.081230933986493. Learning Rate: 9.571990509490511e-05\n",
      "Batch 3,400 of 9,631 Elased 0:23:21. Training loss: 8.061297594869838. Learning Rate: 9.570256132756134e-05\n",
      "Batch 3,450 of 9,631 Elased 0:23:42. Training loss: 8.043384106331978. Learning Rate: 9.568521756021757e-05\n",
      "Batch 3,500 of 9,631 Elased 0:24:03. Training loss: 8.022514835970743. Learning Rate: 9.56678737928738e-05\n",
      "Batch 3,550 of 9,631 Elased 0:24:23. Training loss: 8.004989892999891. Learning Rate: 9.565053002553002e-05\n",
      "Batch 3,600 of 9,631 Elased 0:24:44. Training loss: 7.987436908417278. Learning Rate: 9.563318625818625e-05\n",
      "Batch 3,650 of 9,631 Elased 0:25:04. Training loss: 7.973123002052307. Learning Rate: 9.56158424908425e-05\n",
      "Batch 3,700 of 9,631 Elased 0:25:25. Training loss: 7.953255609628316. Learning Rate: 9.559849872349873e-05\n",
      "Batch 3,750 of 9,631 Elased 0:25:46. Training loss: 7.9331448354085286. Learning Rate: 9.558115495615495e-05\n",
      "Batch 3,800 of 9,631 Elased 0:26:07. Training loss: 7.912463674545288. Learning Rate: 9.55638111888112e-05\n",
      "Batch 3,850 of 9,631 Elased 0:26:27. Training loss: 7.88796250157542. Learning Rate: 9.554646742146743e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3,900 of 9,631 Elased 0:26:48. Training loss: 7.864864469797183. Learning Rate: 9.552912365412366e-05\n",
      "Batch 3,950 of 9,631 Elased 0:27:08. Training loss: 7.846159944172147. Learning Rate: 9.55117798867799e-05\n",
      "Batch 4,000 of 9,631 Elased 0:27:29. Training loss: 7.830097855329513. Learning Rate: 9.549443611943613e-05\n",
      "Batch 4,050 of 9,631 Elased 0:27:50. Training loss: 7.810883913864324. Learning Rate: 9.547709235209236e-05\n",
      "Batch 4,100 of 9,631 Elased 0:28:10. Training loss: 7.794305253261473. Learning Rate: 9.545974858474859e-05\n",
      "Batch 4,150 of 9,631 Elased 0:28:31. Training loss: 7.778855286219033. Learning Rate: 9.544240481740482e-05\n",
      "Batch 4,200 of 9,631 Elased 0:28:52. Training loss: 7.7611145804041906. Learning Rate: 9.542506105006106e-05\n",
      "Batch 4,250 of 9,631 Elased 0:29:13. Training loss: 7.74303648443783. Learning Rate: 9.540771728271729e-05\n",
      "Batch 4,300 of 9,631 Elased 0:29:34. Training loss: 7.725146540375643. Learning Rate: 9.539037351537352e-05\n",
      "Batch 4,350 of 9,631 Elased 0:29:55. Training loss: 7.7060071196501285. Learning Rate: 9.537302974802975e-05\n",
      "Batch 4,400 of 9,631 Elased 0:30:16. Training loss: 7.69052615349943. Learning Rate: 9.535568598068598e-05\n",
      "Batch 4,450 of 9,631 Elased 0:30:38. Training loss: 7.67352500422617. Learning Rate: 9.533834221334222e-05\n",
      "Batch 4,500 of 9,631 Elased 0:30:59. Training loss: 7.657856104003058. Learning Rate: 9.532099844599846e-05\n",
      "Batch 4,550 of 9,631 Elased 0:31:20. Training loss: 7.63776303081722. Learning Rate: 9.530365467865469e-05\n",
      "Batch 4,600 of 9,631 Elased 0:31:42. Training loss: 7.6245615793311075. Learning Rate: 9.528631091131092e-05\n",
      "Batch 4,650 of 9,631 Elased 0:32:02. Training loss: 7.608134526078419. Learning Rate: 9.526896714396715e-05\n",
      "Batch 4,700 of 9,631 Elased 0:32:23. Training loss: 7.592741082171177. Learning Rate: 9.525162337662338e-05\n",
      "Batch 4,750 of 9,631 Elased 0:32:44. Training loss: 7.576270770625064. Learning Rate: 9.523427960927961e-05\n",
      "Batch 4,800 of 9,631 Elased 0:33:04. Training loss: 7.559466500878334. Learning Rate: 9.521693584193585e-05\n",
      "Batch 4,850 of 9,631 Elased 0:33:25. Training loss: 7.542460362591695. Learning Rate: 9.519959207459208e-05\n",
      "Batch 4,900 of 9,631 Elased 0:33:46. Training loss: 7.525720751188239. Learning Rate: 9.518224830724831e-05\n",
      "Batch 4,950 of 9,631 Elased 0:34:06. Training loss: 7.513725582903081. Learning Rate: 9.516490453990454e-05\n",
      "Batch 5,000 of 9,631 Elased 0:34:27. Training loss: 7.499392720079422. Learning Rate: 9.514756077256077e-05\n",
      "Batch 5,050 of 9,631 Elased 0:34:47. Training loss: 7.484803180175253. Learning Rate: 9.513021700521701e-05\n",
      "Batch 5,100 of 9,631 Elased 0:35:08. Training loss: 7.468776777725593. Learning Rate: 9.511287323787324e-05\n",
      "Batch 5,150 of 9,631 Elased 0:35:28. Training loss: 7.455390145709214. Learning Rate: 9.509552947052948e-05\n",
      "Batch 5,200 of 9,631 Elased 0:35:49. Training loss: 7.441304700695551. Learning Rate: 9.507818570318571e-05\n",
      "Batch 5,250 of 9,631 Elased 0:36:10. Training loss: 7.426826052347819. Learning Rate: 9.506084193584194e-05\n",
      "Batch 5,300 of 9,631 Elased 0:36:30. Training loss: 7.410604276252243. Learning Rate: 9.504349816849817e-05\n",
      "Batch 5,350 of 9,631 Elased 0:36:51. Training loss: 7.398035972764559. Learning Rate: 9.502615440115441e-05\n",
      "Batch 5,400 of 9,631 Elased 0:37:12. Training loss: 7.384532331537318. Learning Rate: 9.500881063381064e-05\n",
      "Batch 5,450 of 9,631 Elased 0:37:33. Training loss: 7.370772680930042. Learning Rate: 9.499146686646687e-05\n",
      "Batch 5,500 of 9,631 Elased 0:37:53. Training loss: 7.35764759609916. Learning Rate: 9.49741230991231e-05\n",
      "Batch 5,550 of 9,631 Elased 0:38:13. Training loss: 7.345951597969811. Learning Rate: 9.495677933177933e-05\n",
      "Batch 5,600 of 9,631 Elased 0:38:35. Training loss: 7.330779596737453. Learning Rate: 9.493943556443556e-05\n",
      "Batch 5,650 of 9,631 Elased 0:38:55. Training loss: 7.32072089748045. Learning Rate: 9.49220917970918e-05\n",
      "Batch 5,700 of 9,631 Elased 0:39:15. Training loss: 7.306191931356463. Learning Rate: 9.490474802974803e-05\n",
      "Batch 5,750 of 9,631 Elased 0:39:37. Training loss: 7.291310115689817. Learning Rate: 9.488740426240426e-05\n",
      "Batch 5,800 of 9,631 Elased 0:39:57. Training loss: 7.278447770496895. Learning Rate: 9.48700604950605e-05\n",
      "Batch 5,850 of 9,631 Elased 0:40:18. Training loss: 7.2647765986124675. Learning Rate: 9.485271672771673e-05\n",
      "Batch 5,900 of 9,631 Elased 0:40:38. Training loss: 7.251317870859372. Learning Rate: 9.483537296037298e-05\n",
      "Batch 5,950 of 9,631 Elased 0:40:59. Training loss: 7.2380719509044615. Learning Rate: 9.48180291930292e-05\n",
      "Batch 6,000 of 9,631 Elased 0:41:19. Training loss: 7.225866517583529. Learning Rate: 9.480068542568543e-05\n",
      "Batch 6,050 of 9,631 Elased 0:41:40. Training loss: 7.2144056073102085. Learning Rate: 9.478334165834166e-05\n",
      "Batch 6,100 of 9,631 Elased 0:42:01. Training loss: 7.203176246744688. Learning Rate: 9.476599789099789e-05\n",
      "Batch 6,150 of 9,631 Elased 0:42:22. Training loss: 7.1903176641464235. Learning Rate: 9.474865412365412e-05\n",
      "Batch 6,200 of 9,631 Elased 0:42:42. Training loss: 7.175516188990685. Learning Rate: 9.473131035631037e-05\n",
      "Batch 6,250 of 9,631 Elased 0:43:03. Training loss: 7.16269626361847. Learning Rate: 9.47139665889666e-05\n",
      "Batch 6,300 of 9,631 Elased 0:43:24. Training loss: 7.150080962786599. Learning Rate: 9.469662282162282e-05\n",
      "Batch 6,350 of 9,631 Elased 0:43:44. Training loss: 7.137365597251832. Learning Rate: 9.467927905427905e-05\n",
      "Batch 6,400 of 9,631 Elased 0:44:05. Training loss: 7.12435845464468. Learning Rate: 9.466193528693528e-05\n",
      "Batch 6,450 of 9,631 Elased 0:44:26. Training loss: 7.11554316853368. Learning Rate: 9.464459151959152e-05\n",
      "Batch 6,500 of 9,631 Elased 0:44:46. Training loss: 7.101255676709688. Learning Rate: 9.462724775224775e-05\n",
      "Batch 6,550 of 9,631 Elased 0:45:07. Training loss: 7.0900321249197455. Learning Rate: 9.4609903984904e-05\n",
      "Batch 6,600 of 9,631 Elased 0:45:28. Training loss: 7.079136114590096. Learning Rate: 9.459256021756023e-05\n",
      "Batch 6,650 of 9,631 Elased 0:45:49. Training loss: 7.067868752838077. Learning Rate: 9.457521645021646e-05\n",
      "Batch 6,700 of 9,631 Elased 0:46:09. Training loss: 7.058417202180891. Learning Rate: 9.455787268287268e-05\n",
      "Batch 6,750 of 9,631 Elased 0:46:30. Training loss: 7.046567507425944. Learning Rate: 9.454052891552893e-05\n",
      "Batch 6,800 of 9,631 Elased 0:46:51. Training loss: 7.034629350585096. Learning Rate: 9.452318514818516e-05\n",
      "Batch 6,850 of 9,631 Elased 0:47:11. Training loss: 7.022789050505979. Learning Rate: 9.450584138084139e-05\n",
      "Batch 6,900 of 9,631 Elased 0:47:32. Training loss: 7.012712001524109. Learning Rate: 9.448849761349762e-05\n",
      "Batch 6,950 of 9,631 Elased 0:47:53. Training loss: 7.001084214800553. Learning Rate: 9.447115384615384e-05\n",
      "Batch 7,000 of 9,631 Elased 0:48:13. Training loss: 6.989734499863216. Learning Rate: 9.445381007881007e-05\n",
      "Batch 7,050 of 9,631 Elased 0:48:34. Training loss: 6.976796286765565. Learning Rate: 9.443646631146632e-05\n",
      "Batch 7,100 of 9,631 Elased 0:48:54. Training loss: 6.966452429395327. Learning Rate: 9.441912254412255e-05\n",
      "Batch 7,150 of 9,631 Elased 0:49:15. Training loss: 6.9555075455045365. Learning Rate: 9.440177877677878e-05\n",
      "Batch 7,200 of 9,631 Elased 0:49:36. Training loss: 6.9457635339432295. Learning Rate: 9.438443500943502e-05\n",
      "Batch 7,250 of 9,631 Elased 0:49:56. Training loss: 6.9334178286585315. Learning Rate: 9.436709124209125e-05\n",
      "Batch 7,300 of 9,631 Elased 0:50:17. Training loss: 6.9246912292258385. Learning Rate: 9.434974747474748e-05\n",
      "Batch 7,350 of 9,631 Elased 0:50:38. Training loss: 6.913530014907422. Learning Rate: 9.433240370740372e-05\n",
      "Batch 7,400 of 9,631 Elased 0:50:58. Training loss: 6.901672526759071. Learning Rate: 9.431505994005995e-05\n",
      "Batch 7,450 of 9,631 Elased 0:51:19. Training loss: 6.891130637098478. Learning Rate: 9.429771617271618e-05\n",
      "Batch 7,500 of 9,631 Elased 0:51:39. Training loss: 6.880499536069234. Learning Rate: 9.428037240537241e-05\n",
      "Batch 7,550 of 9,631 Elased 0:52:00. Training loss: 6.870632415765169. Learning Rate: 9.426302863802864e-05\n",
      "Batch 7,600 of 9,631 Elased 0:52:20. Training loss: 6.863250525154566. Learning Rate: 9.424568487068488e-05\n",
      "Batch 7,650 of 9,631 Elased 0:52:41. Training loss: 6.854264973690308. Learning Rate: 9.422834110334111e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7,700 of 9,631 Elased 0:53:01. Training loss: 6.844023193043548. Learning Rate: 9.421099733599734e-05\n",
      "Batch 7,750 of 9,631 Elased 0:53:22. Training loss: 6.831853980433556. Learning Rate: 9.419365356865357e-05\n",
      "Batch 7,800 of 9,631 Elased 0:53:42. Training loss: 6.824068160851796. Learning Rate: 9.41763098013098e-05\n",
      "Batch 7,850 of 9,631 Elased 0:54:03. Training loss: 6.816200119188637. Learning Rate: 9.415896603396604e-05\n",
      "Batch 7,900 of 9,631 Elased 0:54:24. Training loss: 6.805942706518535. Learning Rate: 9.414162226662228e-05\n",
      "Batch 7,950 of 9,631 Elased 0:54:44. Training loss: 6.799497995676485. Learning Rate: 9.412427849927851e-05\n",
      "Batch 8,000 of 9,631 Elased 0:55:05. Training loss: 6.791099740892649. Learning Rate: 9.410693473193474e-05\n",
      "Batch 8,050 of 9,631 Elased 0:55:26. Training loss: 6.784250980874766. Learning Rate: 9.408959096459097e-05\n",
      "Batch 8,100 of 9,631 Elased 0:55:47. Training loss: 6.776137930022346. Learning Rate: 9.40722471972472e-05\n",
      "Batch 8,150 of 9,631 Elased 0:56:07. Training loss: 6.7679378421174965. Learning Rate: 9.405490342990343e-05\n",
      "Batch 8,200 of 9,631 Elased 0:56:27. Training loss: 6.758911025669517. Learning Rate: 9.403755966255967e-05\n",
      "Batch 8,250 of 9,631 Elased 0:56:48. Training loss: 6.751280184398998. Learning Rate: 9.40202158952159e-05\n",
      "Batch 8,300 of 9,631 Elased 0:57:09. Training loss: 6.743603321500571. Learning Rate: 9.400287212787213e-05\n",
      "Batch 8,350 of 9,631 Elased 0:57:29. Training loss: 6.735615096577627. Learning Rate: 9.398552836052836e-05\n",
      "Batch 8,400 of 9,631 Elased 0:57:50. Training loss: 6.726584169978187. Learning Rate: 9.396818459318459e-05\n",
      "Batch 8,450 of 9,631 Elased 0:58:11. Training loss: 6.718540817266385. Learning Rate: 9.395084082584083e-05\n",
      "Batch 8,500 of 9,631 Elased 0:58:31. Training loss: 6.709676929025089. Learning Rate: 9.393349705849706e-05\n",
      "Batch 8,550 of 9,631 Elased 0:58:52. Training loss: 6.701392294036017. Learning Rate: 9.39161532911533e-05\n",
      "Batch 8,600 of 9,631 Elased 0:59:13. Training loss: 6.695204110173292. Learning Rate: 9.389880952380953e-05\n",
      "Batch 8,650 of 9,631 Elased 0:59:33. Training loss: 6.685697397028091. Learning Rate: 9.388146575646576e-05\n",
      "Batch 8,700 of 9,631 Elased 0:59:54. Training loss: 6.678041025413864. Learning Rate: 9.386412198912199e-05\n",
      "Batch 8,750 of 9,631 Elased 1:00:15. Training loss: 6.670244162859236. Learning Rate: 9.384677822177823e-05\n",
      "Batch 8,800 of 9,631 Elased 1:00:35. Training loss: 6.663204365453937. Learning Rate: 9.382943445443446e-05\n",
      "Batch 8,850 of 9,631 Elased 1:00:56. Training loss: 6.654686717717661. Learning Rate: 9.381209068709069e-05\n",
      "Batch 8,900 of 9,631 Elased 1:01:17. Training loss: 6.6482191104835335. Learning Rate: 9.379474691974692e-05\n",
      "Batch 8,950 of 9,631 Elased 1:01:38. Training loss: 6.640632919892252. Learning Rate: 9.377740315240315e-05\n",
      "Batch 9,000 of 9,631 Elased 1:01:58. Training loss: 6.631978898207347. Learning Rate: 9.376005938505938e-05\n",
      "Batch 9,050 of 9,631 Elased 1:02:18. Training loss: 6.626484507671377. Learning Rate: 9.374271561771562e-05\n",
      "Batch 9,100 of 9,631 Elased 1:02:39. Training loss: 6.6181620932411365. Learning Rate: 9.372537185037185e-05\n",
      "Batch 9,150 of 9,631 Elased 1:03:00. Training loss: 6.6110933518018875. Learning Rate: 9.370802808302808e-05\n",
      "Batch 9,200 of 9,631 Elased 1:03:20. Training loss: 6.60584094301514. Learning Rate: 9.369068431568432e-05\n",
      "Batch 9,250 of 9,631 Elased 1:03:41. Training loss: 6.598274567500965. Learning Rate: 9.367334054834055e-05\n",
      "Batch 9,300 of 9,631 Elased 1:04:02. Training loss: 6.591543219038235. Learning Rate: 9.36559967809968e-05\n",
      "Batch 9,350 of 9,631 Elased 1:04:23. Training loss: 6.584306440404392. Learning Rate: 9.363865301365303e-05\n",
      "Batch 9,400 of 9,631 Elased 1:04:43. Training loss: 6.5775691390291176. Learning Rate: 9.362130924630926e-05\n",
      "Batch 9,450 of 9,631 Elased 1:05:04. Training loss: 6.56930984994091. Learning Rate: 9.360396547896548e-05\n",
      "Batch 9,500 of 9,631 Elased 1:05:25. Training loss: 6.562072091805308. Learning Rate: 9.358662171162171e-05\n",
      "Batch 9,550 of 9,631 Elased 1:05:45. Training loss: 6.555404506353808. Learning Rate: 9.356927794427794e-05\n",
      "Batch 9,600 of 9,631 Elased 1:06:06. Training loss: 6.549291816006104. Learning Rate: 9.355193417693419e-05\n",
      "\n",
      "\n",
      "  Average training loss: 6.55\n",
      "  Training epcoh took: 1:06:19\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09cb4f0db07420da3039bd8bd63f0cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=67.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Validation Loss: 5.14\n",
      "  Validation took: 0:00:42\n",
      "\n",
      "======== Epoch 3 / 30 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4032883a149b4c38b703c65c7db0efe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch    50 of 9,631 Elased 0:00:21. Training loss: 5.317487621307373. Learning Rate: 9.352383727383728e-05\n",
      "Batch   100 of 9,631 Elased 0:00:41. Training loss: 5.219072017669678. Learning Rate: 9.35064935064935e-05\n",
      "Batch   150 of 9,631 Elased 0:01:02. Training loss: 5.210784303347269. Learning Rate: 9.348914973914974e-05\n",
      "Batch   200 of 9,631 Elased 0:01:22. Training loss: 5.253367375135422. Learning Rate: 9.347180597180598e-05\n",
      "Batch   250 of 9,631 Elased 0:01:43. Training loss: 5.197244270324707. Learning Rate: 9.345446220446221e-05\n",
      "Batch   300 of 9,631 Elased 0:02:03. Training loss: 5.153470177650451. Learning Rate: 9.343711843711844e-05\n",
      "Batch   350 of 9,631 Elased 0:02:24. Training loss: 5.140523723874773. Learning Rate: 9.341977466977468e-05\n",
      "Batch   400 of 9,631 Elased 0:02:45. Training loss: 5.159726204872132. Learning Rate: 9.340243090243091e-05\n",
      "Batch   450 of 9,631 Elased 0:03:05. Training loss: 5.1576156346003215. Learning Rate: 9.338508713508714e-05\n",
      "Batch   500 of 9,631 Elased 0:03:26. Training loss: 5.16443948841095. Learning Rate: 9.336774336774338e-05\n",
      "Batch   550 of 9,631 Elased 0:03:46. Training loss: 5.160623348409479. Learning Rate: 9.335039960039961e-05\n",
      "Batch   600 of 9,631 Elased 0:04:07. Training loss: 5.160014163255692. Learning Rate: 9.333305583305584e-05\n",
      "Batch   650 of 9,631 Elased 0:04:27. Training loss: 5.154455250226534. Learning Rate: 9.331571206571207e-05\n",
      "Batch   700 of 9,631 Elased 0:04:48. Training loss: 5.146726736000606. Learning Rate: 9.32983682983683e-05\n",
      "Batch   750 of 9,631 Elased 0:05:08. Training loss: 5.130671218554179. Learning Rate: 9.328102453102453e-05\n",
      "Batch   800 of 9,631 Elased 0:05:29. Training loss: 5.1265805286169055. Learning Rate: 9.326368076368077e-05\n",
      "Batch   850 of 9,631 Elased 0:05:49. Training loss: 5.118820889416863. Learning Rate: 9.3246336996337e-05\n",
      "Batch   900 of 9,631 Elased 0:06:10. Training loss: 5.123568511009216. Learning Rate: 9.322899322899323e-05\n",
      "Batch   950 of 9,631 Elased 0:06:31. Training loss: 5.118581197136327. Learning Rate: 9.321164946164946e-05\n",
      "Batch 1,000 of 9,631 Elased 0:06:51. Training loss: 5.116864834785462. Learning Rate: 9.31943056943057e-05\n",
      "Batch 1,050 of 9,631 Elased 0:07:12. Training loss: 5.108093588692801. Learning Rate: 9.317696192696193e-05\n",
      "Batch 1,100 of 9,631 Elased 0:07:33. Training loss: 5.116626020561565. Learning Rate: 9.315961815961817e-05\n",
      "Batch 1,150 of 9,631 Elased 0:07:53. Training loss: 5.1139529412725695. Learning Rate: 9.31422743922744e-05\n",
      "Batch 1,200 of 9,631 Elased 0:08:14. Training loss: 5.115185939272245. Learning Rate: 9.312493062493063e-05\n",
      "Batch 1,250 of 9,631 Elased 0:08:34. Training loss: 5.117775671768189. Learning Rate: 9.310758685758686e-05\n",
      "Batch 1,300 of 9,631 Elased 0:08:55. Training loss: 5.115709891319275. Learning Rate: 9.309024309024309e-05\n",
      "Batch 1,350 of 9,631 Elased 0:09:16. Training loss: 5.117390864336932. Learning Rate: 9.307289932289933e-05\n",
      "Batch 1,400 of 9,631 Elased 0:09:36. Training loss: 5.119016770294734. Learning Rate: 9.305555555555556e-05\n",
      "Batch 1,450 of 9,631 Elased 0:09:57. Training loss: 5.115415616528741. Learning Rate: 9.303821178821179e-05\n",
      "Batch 1,500 of 9,631 Elased 0:10:18. Training loss: 5.108707629680634. Learning Rate: 9.302086802086802e-05\n",
      "Batch 1,550 of 9,631 Elased 0:10:38. Training loss: 5.106287071627955. Learning Rate: 9.300352425352425e-05\n",
      "Batch 1,600 of 9,631 Elased 0:10:59. Training loss: 5.111126185804605. Learning Rate: 9.298618048618048e-05\n",
      "Batch 1,650 of 9,631 Elased 0:11:20. Training loss: 5.119132892579743. Learning Rate: 9.296883671883672e-05\n",
      "Batch 1,700 of 9,631 Elased 0:11:41. Training loss: 5.115891847470228. Learning Rate: 9.295149295149296e-05\n",
      "Batch 1,750 of 9,631 Elased 0:12:01. Training loss: 5.111841065406799. Learning Rate: 9.29341491841492e-05\n",
      "Batch 1,800 of 9,631 Elased 0:12:22. Training loss: 5.10541941165924. Learning Rate: 9.291680541680542e-05\n",
      "Batch 1,850 of 9,631 Elased 0:12:43. Training loss: 5.105706750122277. Learning Rate: 9.289946164946165e-05\n",
      "Batch 1,900 of 9,631 Elased 0:13:03. Training loss: 5.102187979346827. Learning Rate: 9.28821178821179e-05\n",
      "Batch 1,950 of 9,631 Elased 0:13:24. Training loss: 5.097044720405187. Learning Rate: 9.286477411477412e-05\n",
      "Batch 2,000 of 9,631 Elased 0:13:45. Training loss: 5.0897101596593854. Learning Rate: 9.284743034743035e-05\n",
      "Batch 2,050 of 9,631 Elased 0:14:05. Training loss: 5.086728267785979. Learning Rate: 9.283008658008658e-05\n",
      "Batch 2,100 of 9,631 Elased 0:14:26. Training loss: 5.091597131888071. Learning Rate: 9.281274281274281e-05\n",
      "Batch 2,150 of 9,631 Elased 0:14:47. Training loss: 5.090264986836633. Learning Rate: 9.279539904539904e-05\n",
      "Batch 2,200 of 9,631 Elased 0:15:07. Training loss: 5.088820103190162. Learning Rate: 9.277805527805528e-05\n",
      "Batch 2,250 of 9,631 Elased 0:15:28. Training loss: 5.085593235121833. Learning Rate: 9.276071151071151e-05\n",
      "Batch 2,300 of 9,631 Elased 0:15:48. Training loss: 5.087525099360424. Learning Rate: 9.274336774336774e-05\n",
      "Batch 2,350 of 9,631 Elased 0:16:09. Training loss: 5.084552993165686. Learning Rate: 9.272602397602399e-05\n",
      "Batch 2,400 of 9,631 Elased 0:16:30. Training loss: 5.083720003863176. Learning Rate: 9.270868020868021e-05\n",
      "Batch 2,450 of 9,631 Elased 0:16:50. Training loss: 5.079100792845901. Learning Rate: 9.269133644133644e-05\n",
      "Batch 2,500 of 9,631 Elased 0:17:11. Training loss: 5.074088136577606. Learning Rate: 9.267399267399269e-05\n",
      "Batch 2,550 of 9,631 Elased 0:17:32. Training loss: 5.077615656852722. Learning Rate: 9.265664890664892e-05\n",
      "Batch 2,600 of 9,631 Elased 0:17:52. Training loss: 5.077029463236149. Learning Rate: 9.263930513930515e-05\n",
      "Batch 2,650 of 9,631 Elased 0:18:13. Training loss: 5.077879400703142. Learning Rate: 9.262196137196137e-05\n",
      "Batch 2,700 of 9,631 Elased 0:18:34. Training loss: 5.080677600436741. Learning Rate: 9.26046176046176e-05\n",
      "Batch 2,750 of 9,631 Elased 0:18:54. Training loss: 5.079544750993902. Learning Rate: 9.258727383727385e-05\n",
      "Batch 2,800 of 9,631 Elased 0:19:15. Training loss: 5.0825513786929. Learning Rate: 9.256993006993008e-05\n",
      "Batch 2,850 of 9,631 Elased 0:19:35. Training loss: 5.083534672720391. Learning Rate: 9.25525863025863e-05\n",
      "Batch 2,900 of 9,631 Elased 0:19:56. Training loss: 5.083368440989791. Learning Rate: 9.253524253524253e-05\n",
      "Batch 2,950 of 9,631 Elased 0:20:17. Training loss: 5.080933292194948. Learning Rate: 9.251789876789876e-05\n",
      "Batch 3,000 of 9,631 Elased 0:20:37. Training loss: 5.084098608414332. Learning Rate: 9.2500555000555e-05\n",
      "Batch 3,050 of 9,631 Elased 0:20:58. Training loss: 5.0818682309447745. Learning Rate: 9.248321123321124e-05\n",
      "Batch 3,100 of 9,631 Elased 0:21:19. Training loss: 5.079931851971534. Learning Rate: 9.246586746586748e-05\n",
      "Batch 3,150 of 9,631 Elased 0:21:39. Training loss: 5.072195468781486. Learning Rate: 9.244852369852371e-05\n",
      "Batch 3,200 of 9,631 Elased 0:22:00. Training loss: 5.07237216539681. Learning Rate: 9.243117993117994e-05\n",
      "Batch 3,250 of 9,631 Elased 0:22:21. Training loss: 5.070759204937861. Learning Rate: 9.241383616383617e-05\n",
      "Batch 3,300 of 9,631 Elased 0:22:41. Training loss: 5.073166160439. Learning Rate: 9.23964923964924e-05\n",
      "Batch 3,350 of 9,631 Elased 0:23:02. Training loss: 5.073064391363912. Learning Rate: 9.237914862914864e-05\n",
      "Batch 3,400 of 9,631 Elased 0:23:23. Training loss: 5.0732317688885855. Learning Rate: 9.236180486180487e-05\n",
      "Batch 3,450 of 9,631 Elased 0:23:44. Training loss: 5.073961546939352. Learning Rate: 9.23444610944611e-05\n",
      "Batch 3,500 of 9,631 Elased 0:24:04. Training loss: 5.069735965183803. Learning Rate: 9.232711732711733e-05\n",
      "Batch 3,550 of 9,631 Elased 0:24:25. Training loss: 5.06867498350815. Learning Rate: 9.230977355977356e-05\n",
      "Batch 3,600 of 9,631 Elased 0:24:46. Training loss: 5.069420030845536. Learning Rate: 9.22924297924298e-05\n",
      "Batch 3,650 of 9,631 Elased 0:25:06. Training loss: 5.0740986119884335. Learning Rate: 9.227508602508603e-05\n",
      "Batch 3,700 of 9,631 Elased 0:25:27. Training loss: 5.0721284446200805. Learning Rate: 9.225774225774226e-05\n",
      "Batch 3,750 of 9,631 Elased 0:25:47. Training loss: 5.067872357559204. Learning Rate: 9.22403984903985e-05\n",
      "Batch 3,800 of 9,631 Elased 0:26:08. Training loss: 5.063662200726961. Learning Rate: 9.222305472305473e-05\n",
      "Batch 3,850 of 9,631 Elased 0:26:29. Training loss: 5.057819098125805. Learning Rate: 9.220571095571096e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3,900 of 9,631 Elased 0:26:49. Training loss: 5.053564352744665. Learning Rate: 9.21883671883672e-05\n",
      "Batch 3,950 of 9,631 Elased 0:27:10. Training loss: 5.052273733042464. Learning Rate: 9.217102342102343e-05\n",
      "Batch 4,000 of 9,631 Elased 0:27:31. Training loss: 5.051807138681411. Learning Rate: 9.215367965367966e-05\n",
      "Batch 4,050 of 9,631 Elased 0:27:51. Training loss: 5.050278465424055. Learning Rate: 9.213633588633589e-05\n",
      "Batch 4,100 of 9,631 Elased 0:28:12. Training loss: 5.048903737649685. Learning Rate: 9.211899211899212e-05\n",
      "Batch 4,150 of 9,631 Elased 0:28:33. Training loss: 5.049288994662733. Learning Rate: 9.210164835164835e-05\n",
      "Batch 4,200 of 9,631 Elased 0:28:54. Training loss: 5.047695469742729. Learning Rate: 9.208430458430459e-05\n",
      "Batch 4,250 of 9,631 Elased 0:29:14. Training loss: 5.044794402907876. Learning Rate: 9.206696081696082e-05\n",
      "Batch 4,300 of 9,631 Elased 0:29:35. Training loss: 5.043774085710215. Learning Rate: 9.204961704961705e-05\n",
      "Batch 4,350 of 9,631 Elased 0:29:56. Training loss: 5.041827716662966. Learning Rate: 9.203227328227328e-05\n",
      "Batch 4,400 of 9,631 Elased 0:30:16. Training loss: 5.040816186287186. Learning Rate: 9.201492951492952e-05\n",
      "Batch 4,450 of 9,631 Elased 0:30:37. Training loss: 5.0398047192712845. Learning Rate: 9.199758574758576e-05\n",
      "Batch 4,500 of 9,631 Elased 0:30:58. Training loss: 5.038967415173849. Learning Rate: 9.198024198024199e-05\n",
      "Batch 4,550 of 9,631 Elased 0:31:18. Training loss: 5.034944102580731. Learning Rate: 9.196289821289822e-05\n",
      "Batch 4,600 of 9,631 Elased 0:31:39. Training loss: 5.036289948276852. Learning Rate: 9.194555444555445e-05\n",
      "Batch 4,650 of 9,631 Elased 0:31:59. Training loss: 5.033540203802048. Learning Rate: 9.192821067821068e-05\n",
      "Batch 4,700 of 9,631 Elased 0:32:20. Training loss: 5.0332564361044705. Learning Rate: 9.191086691086691e-05\n",
      "Batch 4,750 of 9,631 Elased 0:32:41. Training loss: 5.030281779490019. Learning Rate: 9.189352314352315e-05\n",
      "Batch 4,800 of 9,631 Elased 0:33:02. Training loss: 5.027418344120185. Learning Rate: 9.187617937617938e-05\n",
      "Batch 4,850 of 9,631 Elased 0:33:23. Training loss: 5.023435063706231. Learning Rate: 9.185883560883561e-05\n",
      "Batch 4,900 of 9,631 Elased 0:33:43. Training loss: 5.02180407475452. Learning Rate: 9.184149184149184e-05\n",
      "Batch 4,950 of 9,631 Elased 0:34:04. Training loss: 5.022236197404187. Learning Rate: 9.182414807414807e-05\n",
      "Batch 5,000 of 9,631 Elased 0:34:24. Training loss: 5.021878594493866. Learning Rate: 9.18068043068043e-05\n",
      "Batch 5,050 of 9,631 Elased 0:34:45. Training loss: 5.020606908751006. Learning Rate: 9.178946053946054e-05\n",
      "Batch 5,100 of 9,631 Elased 0:35:06. Training loss: 5.017044866085053. Learning Rate: 9.177211677211679e-05\n",
      "Batch 5,150 of 9,631 Elased 0:35:26. Training loss: 5.016000078303143. Learning Rate: 9.175477300477301e-05\n",
      "Batch 5,200 of 9,631 Elased 0:35:47. Training loss: 5.015122317580077. Learning Rate: 9.173742923742924e-05\n",
      "Batch 5,250 of 9,631 Elased 0:36:08. Training loss: 5.01438106636774. Learning Rate: 9.172008547008547e-05\n",
      "Batch 5,300 of 9,631 Elased 0:36:28. Training loss: 5.012180665798907. Learning Rate: 9.170274170274172e-05\n",
      "Batch 5,350 of 9,631 Elased 0:36:49. Training loss: 5.012960104541244. Learning Rate: 9.168539793539794e-05\n",
      "Batch 5,400 of 9,631 Elased 0:37:10. Training loss: 5.011876966070246. Learning Rate: 9.166805416805417e-05\n",
      "Batch 5,450 of 9,631 Elased 0:37:30. Training loss: 5.010475945166491. Learning Rate: 9.16507104007104e-05\n",
      "Batch 5,500 of 9,631 Elased 0:37:50. Training loss: 5.009159569133412. Learning Rate: 9.163336663336663e-05\n",
      "Batch 5,550 of 9,631 Elased 0:38:10. Training loss: 5.009454579181499. Learning Rate: 9.161602286602286e-05\n",
      "Batch 5,600 of 9,631 Elased 0:38:31. Training loss: 5.007413766937597. Learning Rate: 9.15986790986791e-05\n",
      "Batch 5,650 of 9,631 Elased 0:38:52. Training loss: 5.0094751996909626. Learning Rate: 9.158133533133533e-05\n",
      "Batch 5,700 of 9,631 Elased 0:39:12. Training loss: 5.007586539544557. Learning Rate: 9.156399156399156e-05\n",
      "Batch 5,750 of 9,631 Elased 0:39:33. Training loss: 5.004361259792162. Learning Rate: 9.15466477966478e-05\n",
      "Batch 5,800 of 9,631 Elased 0:39:54. Training loss: 5.002359844117329. Learning Rate: 9.152930402930404e-05\n",
      "Batch 5,850 of 9,631 Elased 0:40:14. Training loss: 5.000548818906148. Learning Rate: 9.151196026196026e-05\n",
      "Batch 5,900 of 9,631 Elased 0:40:34. Training loss: 4.998282723871328. Learning Rate: 9.149461649461651e-05\n",
      "Batch 5,950 of 9,631 Elased 0:40:55. Training loss: 4.996656923694771. Learning Rate: 9.147727272727274e-05\n",
      "Batch 6,000 of 9,631 Elased 0:41:16. Training loss: 4.9953577705224355. Learning Rate: 9.145992895992897e-05\n",
      "Batch 6,050 of 9,631 Elased 0:41:36. Training loss: 4.994702754730035. Learning Rate: 9.14425851925852e-05\n",
      "Batch 6,100 of 9,631 Elased 0:41:57. Training loss: 4.993442762367061. Learning Rate: 9.142524142524142e-05\n",
      "Batch 6,150 of 9,631 Elased 0:42:18. Training loss: 4.9911907335607015. Learning Rate: 9.140789765789767e-05\n",
      "Batch 6,200 of 9,631 Elased 0:42:38. Training loss: 4.987116377315213. Learning Rate: 9.13905538905539e-05\n",
      "Batch 6,250 of 9,631 Elased 0:43:00. Training loss: 4.985031526603699. Learning Rate: 9.137321012321013e-05\n",
      "Batch 6,300 of 9,631 Elased 0:43:21. Training loss: 4.983396335896992. Learning Rate: 9.135586635586636e-05\n",
      "Batch 6,350 of 9,631 Elased 0:43:42. Training loss: 4.981730678701025. Learning Rate: 9.133852258852258e-05\n",
      "Batch 6,400 of 9,631 Elased 0:44:03. Training loss: 4.978970505669713. Learning Rate: 9.132117882117883e-05\n",
      "Batch 6,450 of 9,631 Elased 0:44:25. Training loss: 4.981055205330368. Learning Rate: 9.130383505383506e-05\n",
      "Batch 6,500 of 9,631 Elased 0:44:46. Training loss: 4.977589814112736. Learning Rate: 9.12864912864913e-05\n",
      "Batch 6,550 of 9,631 Elased 0:45:07. Training loss: 4.976577617521505. Learning Rate: 9.126914751914753e-05\n",
      "Batch 6,600 of 9,631 Elased 0:45:28. Training loss: 4.976034286347303. Learning Rate: 9.125180375180376e-05\n",
      "Batch 6,650 of 9,631 Elased 0:45:50. Training loss: 4.974536875638747. Learning Rate: 9.123445998445999e-05\n",
      "Batch 6,700 of 9,631 Elased 0:46:10. Training loss: 4.974633081279584. Learning Rate: 9.121711621711622e-05\n",
      "Batch 6,750 of 9,631 Elased 0:46:31. Training loss: 4.972739878725123. Learning Rate: 9.119977244977246e-05\n",
      "Batch 6,800 of 9,631 Elased 0:46:52. Training loss: 4.970116627146216. Learning Rate: 9.118242868242869e-05\n",
      "Batch 6,850 of 9,631 Elased 0:47:12. Training loss: 4.967590015021554. Learning Rate: 9.116508491508492e-05\n",
      "Batch 6,900 of 9,631 Elased 0:47:33. Training loss: 4.96730550430823. Learning Rate: 9.114774114774115e-05\n",
      "Batch 6,950 of 9,631 Elased 0:47:54. Training loss: 4.965564491285694. Learning Rate: 9.113039738039738e-05\n",
      "Batch 7,000 of 9,631 Elased 0:48:15. Training loss: 4.963194131101881. Learning Rate: 9.111305361305362e-05\n",
      "Batch 7,050 of 9,631 Elased 0:48:35. Training loss: 4.960540770266919. Learning Rate: 9.109570984570985e-05\n",
      "Batch 7,100 of 9,631 Elased 0:48:56. Training loss: 4.959545568076657. Learning Rate: 9.107836607836608e-05\n",
      "Batch 7,150 of 9,631 Elased 0:49:17. Training loss: 4.957553942720373. Learning Rate: 9.106102231102232e-05\n",
      "Batch 7,200 of 9,631 Elased 0:49:37. Training loss: 4.955685885681047. Learning Rate: 9.104367854367855e-05\n",
      "Batch 7,250 of 9,631 Elased 0:49:58. Training loss: 4.952206922169389. Learning Rate: 9.102633477633478e-05\n",
      "Batch 7,300 of 9,631 Elased 0:50:19. Training loss: 4.951520059729275. Learning Rate: 9.100899100899102e-05\n",
      "Batch 7,350 of 9,631 Elased 0:50:39. Training loss: 4.948875819485203. Learning Rate: 9.099164724164725e-05\n",
      "Batch 7,400 of 9,631 Elased 0:51:00. Training loss: 4.945747626343289. Learning Rate: 9.097430347430348e-05\n",
      "Batch 7,450 of 9,631 Elased 0:51:21. Training loss: 4.943931321425726. Learning Rate: 9.095695970695971e-05\n",
      "Batch 7,500 of 9,631 Elased 0:51:41. Training loss: 4.940388866138458. Learning Rate: 9.093961593961594e-05\n",
      "Batch 7,550 of 9,631 Elased 0:52:02. Training loss: 4.938279453902845. Learning Rate: 9.092227217227217e-05\n",
      "Batch 7,600 of 9,631 Elased 0:52:23. Training loss: 4.938047271370888. Learning Rate: 9.090492840492841e-05\n",
      "Batch 7,650 of 9,631 Elased 0:52:43. Training loss: 4.937611554681865. Learning Rate: 9.088758463758464e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7,700 of 9,631 Elased 0:53:04. Training loss: 4.934968885260743. Learning Rate: 9.087024087024087e-05\n",
      "Batch 7,750 of 9,631 Elased 0:53:24. Training loss: 4.930726354937399. Learning Rate: 9.08528971028971e-05\n",
      "Batch 7,800 of 9,631 Elased 0:53:45. Training loss: 4.92987564600431. Learning Rate: 9.083555333555334e-05\n",
      "Batch 7,850 of 9,631 Elased 0:54:06. Training loss: 4.928829123928288. Learning Rate: 9.081820956820958e-05\n",
      "Batch 7,900 of 9,631 Elased 0:54:27. Training loss: 4.9260391528395155. Learning Rate: 9.080086580086581e-05\n",
      "Batch 7,950 of 9,631 Elased 0:54:47. Training loss: 4.926655254783871. Learning Rate: 9.078352203352204e-05\n",
      "Batch 8,000 of 9,631 Elased 0:55:08. Training loss: 4.925465712189674. Learning Rate: 9.076617826617827e-05\n",
      "Batch 8,050 of 9,631 Elased 0:55:28. Training loss: 4.926021568345727. Learning Rate: 9.07488344988345e-05\n",
      "Batch 8,100 of 9,631 Elased 0:55:49. Training loss: 4.925038940523877. Learning Rate: 9.073149073149073e-05\n",
      "Batch 8,150 of 9,631 Elased 0:56:10. Training loss: 4.924527345142481. Learning Rate: 9.071414696414697e-05\n",
      "Batch 8,200 of 9,631 Elased 0:56:30. Training loss: 4.922973098289676. Learning Rate: 9.06968031968032e-05\n",
      "Batch 8,250 of 9,631 Elased 0:56:51. Training loss: 4.922809463963364. Learning Rate: 9.067945942945943e-05\n",
      "Batch 8,300 of 9,631 Elased 0:57:12. Training loss: 4.922973628475005. Learning Rate: 9.066211566211566e-05\n",
      "Batch 8,350 of 9,631 Elased 0:57:32. Training loss: 4.921680198857884. Learning Rate: 9.064477189477189e-05\n",
      "Batch 8,400 of 9,631 Elased 0:57:53. Training loss: 4.919956159761973. Learning Rate: 9.062742812742812e-05\n",
      "Batch 8,450 of 9,631 Elased 0:58:14. Training loss: 4.918514240310037. Learning Rate: 9.061008436008436e-05\n",
      "Batch 8,500 of 9,631 Elased 0:58:34. Training loss: 4.916475280537325. Learning Rate: 9.05927405927406e-05\n",
      "Batch 8,550 of 9,631 Elased 0:58:55. Training loss: 4.915140659544203. Learning Rate: 9.057539682539683e-05\n",
      "Batch 8,600 of 9,631 Elased 0:59:15. Training loss: 4.915324865829113. Learning Rate: 9.055805305805306e-05\n",
      "Batch 8,650 of 9,631 Elased 0:59:36. Training loss: 4.912907869250788. Learning Rate: 9.05407092907093e-05\n",
      "Batch 8,700 of 9,631 Elased 0:59:57. Training loss: 4.911999805220242. Learning Rate: 9.052336552336554e-05\n",
      "Batch 8,750 of 9,631 Elased 1:00:18. Training loss: 4.910722488866534. Learning Rate: 9.050602175602177e-05\n",
      "Batch 8,800 of 9,631 Elased 1:00:39. Training loss: 4.909543586400422. Learning Rate: 9.0488677988678e-05\n",
      "Batch 8,850 of 9,631 Elased 1:01:00. Training loss: 4.9076722858450506. Learning Rate: 9.047133422133422e-05\n",
      "Batch 8,900 of 9,631 Elased 1:01:20. Training loss: 4.907594654801186. Learning Rate: 9.045399045399045e-05\n",
      "Batch 8,950 of 9,631 Elased 1:01:41. Training loss: 4.905796338912495. Learning Rate: 9.043664668664668e-05\n",
      "Batch 9,000 of 9,631 Elased 1:02:02. Training loss: 4.902974464257558. Learning Rate: 9.041930291930293e-05\n",
      "Batch 9,050 of 9,631 Elased 1:02:22. Training loss: 4.903242972147399. Learning Rate: 9.040195915195915e-05\n",
      "Batch 9,100 of 9,631 Elased 1:02:43. Training loss: 4.901605874968099. Learning Rate: 9.038461538461538e-05\n",
      "Batch 9,150 of 9,631 Elased 1:03:03. Training loss: 4.900655643301583. Learning Rate: 9.036727161727163e-05\n",
      "Batch 9,200 of 9,631 Elased 1:03:24. Training loss: 4.901273814284283. Learning Rate: 9.034992784992786e-05\n",
      "Batch 9,250 of 9,631 Elased 1:03:45. Training loss: 4.899599472638723. Learning Rate: 9.033258408258409e-05\n",
      "Batch 9,300 of 9,631 Elased 1:04:06. Training loss: 4.899247320211062. Learning Rate: 9.031524031524033e-05\n",
      "Batch 9,350 of 9,631 Elased 1:04:27. Training loss: 4.898322701479662. Learning Rate: 9.029789654789656e-05\n",
      "Batch 9,400 of 9,631 Elased 1:04:47. Training loss: 4.897585727788033. Learning Rate: 9.028055278055279e-05\n",
      "Batch 9,450 of 9,631 Elased 1:05:08. Training loss: 4.895448932042197. Learning Rate: 9.026320901320902e-05\n",
      "Batch 9,500 of 9,631 Elased 1:05:28. Training loss: 4.893981594135886. Learning Rate: 9.024586524586525e-05\n",
      "Batch 9,550 of 9,631 Elased 1:05:49. Training loss: 4.892784681719635. Learning Rate: 9.022852147852149e-05\n",
      "Batch 9,600 of 9,631 Elased 1:06:10. Training loss: 4.892397465109825. Learning Rate: 9.021117771117772e-05\n",
      "\n",
      "\n",
      "  Average training loss: 4.89\n",
      "  Training epcoh took: 1:06:22\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b274735b294973b984b9d3f49b67f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=67.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Validation Loss: 4.59\n",
      "  Validation took: 0:00:42\n",
      "\n",
      "======== Epoch 4 / 30 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12510224cfa45c39b789d9581112951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch    50 of 9,631 Elased 0:00:21. Training loss: 4.796392159461975. Learning Rate: 9.018308080808081e-05\n",
      "Batch   100 of 9,631 Elased 0:00:42. Training loss: 4.6754840159416196. Learning Rate: 9.016573704073704e-05\n",
      "Batch   150 of 9,631 Elased 0:01:03. Training loss: 4.669014674822489. Learning Rate: 9.014839327339327e-05\n",
      "Batch   200 of 9,631 Elased 0:01:23. Training loss: 4.706062372922897. Learning Rate: 9.013104950604951e-05\n",
      "Batch   250 of 9,631 Elased 0:01:43. Training loss: 4.658318112373352. Learning Rate: 9.011370573870574e-05\n",
      "Batch   300 of 9,631 Elased 0:02:04. Training loss: 4.62034285624822. Learning Rate: 9.009636197136198e-05\n",
      "Batch   350 of 9,631 Elased 0:02:25. Training loss: 4.606304552214486. Learning Rate: 9.007901820401821e-05\n",
      "Batch   400 of 9,631 Elased 0:02:45. Training loss: 4.623959874510765. Learning Rate: 9.006167443667444e-05\n",
      "Batch   450 of 9,631 Elased 0:03:06. Training loss: 4.615380027029249. Learning Rate: 9.004433066933068e-05\n",
      "Batch   500 of 9,631 Elased 0:03:27. Training loss: 4.620292767047882. Learning Rate: 9.002698690198691e-05\n",
      "Batch   550 of 9,631 Elased 0:03:47. Training loss: 4.615000019073486. Learning Rate: 9.000964313464314e-05\n",
      "Batch   600 of 9,631 Elased 0:04:08. Training loss: 4.619777896404266. Learning Rate: 8.999229936729937e-05\n",
      "Batch   650 of 9,631 Elased 0:04:29. Training loss: 4.616668780033405. Learning Rate: 8.99749555999556e-05\n",
      "Batch   700 of 9,631 Elased 0:04:49. Training loss: 4.613759400163378. Learning Rate: 8.995761183261183e-05\n",
      "Batch   750 of 9,631 Elased 0:05:10. Training loss: 4.599488408088684. Learning Rate: 8.994026806526807e-05\n",
      "Batch   800 of 9,631 Elased 0:05:30. Training loss: 4.59774601906538. Learning Rate: 8.99229242979243e-05\n",
      "Batch   850 of 9,631 Elased 0:05:51. Training loss: 4.590396557415233. Learning Rate: 8.990558053058053e-05\n",
      "Batch   900 of 9,631 Elased 0:06:12. Training loss: 4.596974519093831. Learning Rate: 8.988823676323676e-05\n",
      "Batch   950 of 9,631 Elased 0:06:32. Training loss: 4.590560144876179. Learning Rate: 8.9870892995893e-05\n",
      "Batch 1,000 of 9,631 Elased 0:06:54. Training loss: 4.592341773033142. Learning Rate: 8.985354922854923e-05\n",
      "Batch 1,050 of 9,631 Elased 0:07:14. Training loss: 4.590831563359215. Learning Rate: 8.983620546120547e-05\n",
      "Batch 1,100 of 9,631 Elased 0:07:35. Training loss: 4.593851026621732. Learning Rate: 8.98188616938617e-05\n",
      "Batch 1,150 of 9,631 Elased 0:07:55. Training loss: 4.593323868875919. Learning Rate: 8.980151792651793e-05\n",
      "Batch 1,200 of 9,631 Elased 0:08:16. Training loss: 4.5926090788841245. Learning Rate: 8.978417415917416e-05\n",
      "Batch 1,250 of 9,631 Elased 0:08:37. Training loss: 4.596207763671875. Learning Rate: 8.976683039183039e-05\n",
      "Batch 1,300 of 9,631 Elased 0:08:57. Training loss: 4.594191050712879. Learning Rate: 8.974948662448663e-05\n",
      "Batch 1,400 of 9,631 Elased 0:09:39. Training loss: 4.59232053228787. Learning Rate: 8.97147990897991e-05\n",
      "Batch 1,450 of 9,631 Elased 0:10:00. Training loss: 4.589246488768479. Learning Rate: 8.969745532245532e-05\n",
      "Batch 1,750 of 9,631 Elased 0:12:08. Training loss: 4.592516086578369. Learning Rate: 8.959339271839273e-05\n",
      "Batch 1,800 of 9,631 Elased 0:12:29. Training loss: 4.589331190983454. Learning Rate: 8.957604895104895e-05\n",
      "Batch 1,850 of 9,631 Elased 0:12:49. Training loss: 4.590128281954173. Learning Rate: 8.955870518370518e-05\n",
      "Batch 1,900 of 9,631 Elased 0:13:11. Training loss: 4.58494083115929. Learning Rate: 8.954136141636143e-05\n",
      "Batch 1,950 of 9,631 Elased 0:13:32. Training loss: 4.582654714706616. Learning Rate: 8.952401764901766e-05\n",
      "Batch 2,000 of 9,631 Elased 0:13:53. Training loss: 4.578130850553513. Learning Rate: 8.950667388167389e-05\n",
      "Batch 2,050 of 9,631 Elased 0:14:14. Training loss: 4.578336905270088. Learning Rate: 8.948933011433011e-05\n",
      "Batch 2,100 of 9,631 Elased 0:14:35. Training loss: 4.58220413514546. Learning Rate: 8.947198634698634e-05\n",
      "Batch 2,150 of 9,631 Elased 0:14:56. Training loss: 4.580946531295776. Learning Rate: 8.945464257964259e-05\n",
      "Batch 2,200 of 9,631 Elased 0:15:17. Training loss: 4.5794883587143635. Learning Rate: 8.943729881229882e-05\n",
      "Batch 2,250 of 9,631 Elased 0:15:39. Training loss: 4.57634785132938. Learning Rate: 8.941995504495505e-05\n",
      "Batch 2,300 of 9,631 Elased 0:16:00. Training loss: 4.579011910687322. Learning Rate: 8.940261127761129e-05\n",
      "Batch 2,350 of 9,631 Elased 0:16:21. Training loss: 4.577465187032172. Learning Rate: 8.938526751026752e-05\n",
      "Batch 2,400 of 9,631 Elased 0:16:42. Training loss: 4.57743579586347. Learning Rate: 8.936792374292375e-05\n",
      "Batch 2,450 of 9,631 Elased 0:17:03. Training loss: 4.57462892357184. Learning Rate: 8.935057997557999e-05\n",
      "Batch 2,500 of 9,631 Elased 0:17:24. Training loss: 4.569924544525146. Learning Rate: 8.933323620823622e-05\n",
      "Batch 2,550 of 9,631 Elased 0:17:45. Training loss: 4.575401679581287. Learning Rate: 8.931589244089245e-05\n",
      "Batch 2,600 of 9,631 Elased 0:18:07. Training loss: 4.577078045239816. Learning Rate: 8.929854867354868e-05\n",
      "Batch 2,650 of 9,631 Elased 0:18:28. Training loss: 4.578223836376982. Learning Rate: 8.92812049062049e-05\n",
      "Batch 2,700 of 9,631 Elased 0:18:49. Training loss: 4.581599646850869. Learning Rate: 8.926386113886114e-05\n",
      "Batch 2,750 of 9,631 Elased 0:19:10. Training loss: 4.578672556703741. Learning Rate: 8.924651737151738e-05\n",
      "Batch 2,800 of 9,631 Elased 0:19:31. Training loss: 4.582658773660659. Learning Rate: 8.922917360417361e-05\n",
      "Batch 2,850 of 9,631 Elased 0:19:52. Training loss: 4.584794136850458. Learning Rate: 8.921182983682984e-05\n",
      "Batch 2,900 of 9,631 Elased 0:20:13. Training loss: 4.583816268032995. Learning Rate: 8.919448606948607e-05\n",
      "Batch 2,950 of 9,631 Elased 0:20:34. Training loss: 4.5829187415009836. Learning Rate: 8.917714230214231e-05\n",
      "Batch 3,000 of 9,631 Elased 0:20:55. Training loss: 4.5864824041525525. Learning Rate: 8.915979853479854e-05\n",
      "Batch 3,050 of 9,631 Elased 0:21:17. Training loss: 4.585106447720137. Learning Rate: 8.914245476745478e-05\n",
      "Batch 3,100 of 9,631 Elased 0:21:38. Training loss: 4.584094216515941. Learning Rate: 8.912511100011101e-05\n",
      "Batch 3,150 of 9,631 Elased 0:21:58. Training loss: 4.577629263060434. Learning Rate: 8.910776723276724e-05\n",
      "Batch 3,200 of 9,631 Elased 0:22:20. Training loss: 4.577868735641241. Learning Rate: 8.909042346542347e-05\n",
      "Batch 3,250 of 9,631 Elased 0:22:41. Training loss: 4.579150311249953. Learning Rate: 8.90730796980797e-05\n",
      "Batch 3,300 of 9,631 Elased 0:23:02. Training loss: 4.583460231188572. Learning Rate: 8.905573593073594e-05\n",
      "Batch 3,350 of 9,631 Elased 0:23:23. Training loss: 4.584688459225555. Learning Rate: 8.903839216339217e-05\n",
      "Batch 3,650 of 9,631 Elased 0:25:30. Training loss: 4.590682457571161. Learning Rate: 8.893432955932956e-05\n",
      "Batch 3,700 of 9,631 Elased 0:25:51. Training loss: 4.590334952070906. Learning Rate: 8.89169857919858e-05\n",
      "Batch 3,750 of 9,631 Elased 0:26:12. Training loss: 4.587247085507711. Learning Rate: 8.889964202464203e-05\n",
      "Batch 3,800 of 9,631 Elased 0:26:33. Training loss: 4.584063168513148. Learning Rate: 8.888229825729826e-05\n",
      "Batch 3,850 of 9,631 Elased 0:26:54. Training loss: 4.579503543098252. Learning Rate: 8.88649544899545e-05\n",
      "Batch 3,900 of 9,631 Elased 0:27:15. Training loss: 4.576704093615214. Learning Rate: 8.884761072261073e-05\n",
      "Batch 3,950 of 9,631 Elased 0:27:37. Training loss: 4.577409718610063. Learning Rate: 8.883026695526696e-05\n",
      "Batch 4,000 of 9,631 Elased 0:27:57. Training loss: 4.578449302852154. Learning Rate: 8.881292318792319e-05\n",
      "Batch 4,050 of 9,631 Elased 0:28:18. Training loss: 4.577852149951605. Learning Rate: 8.879557942057942e-05\n",
      "Batch 4,100 of 9,631 Elased 0:28:40. Training loss: 4.577676233663792. Learning Rate: 8.877823565323565e-05\n",
      "Batch 4,150 of 9,631 Elased 0:29:01. Training loss: 4.578014851650559. Learning Rate: 8.876089188589189e-05\n",
      "Batch 4,200 of 9,631 Elased 0:29:22. Training loss: 4.576832321030753. Learning Rate: 8.874354811854812e-05\n",
      "Batch 4,250 of 9,631 Elased 0:29:43. Training loss: 4.575689866795259. Learning Rate: 8.872620435120435e-05\n",
      "Batch 4,300 of 9,631 Elased 0:30:04. Training loss: 4.574842221736908. Learning Rate: 8.870886058386058e-05\n",
      "Batch 4,350 of 9,631 Elased 0:30:24. Training loss: 4.5739964916514255. Learning Rate: 8.869151681651682e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4,400 of 9,631 Elased 0:30:45. Training loss: 4.574059722044251. Learning Rate: 8.867417304917305e-05\n",
      "Batch 4,450 of 9,631 Elased 0:31:07. Training loss: 4.573829006130776. Learning Rate: 8.86568292818293e-05\n",
      "Batch 4,500 of 9,631 Elased 0:31:28. Training loss: 4.575046408547295. Learning Rate: 8.863948551448552e-05\n",
      "Batch 4,550 of 9,631 Elased 0:31:49. Training loss: 4.572210552404215. Learning Rate: 8.862214174714175e-05\n",
      "Batch 4,600 of 9,631 Elased 0:32:10. Training loss: 4.574372013392655. Learning Rate: 8.860479797979798e-05\n",
      "Batch 4,650 of 9,631 Elased 0:32:31. Training loss: 4.572594802456518. Learning Rate: 8.858745421245421e-05\n",
      "Batch 4,700 of 9,631 Elased 0:32:52. Training loss: 4.573030312872947. Learning Rate: 8.857011044511046e-05\n",
      "Batch 4,750 of 9,631 Elased 0:33:13. Training loss: 4.571064070300052. Learning Rate: 8.855276667776668e-05\n",
      "Batch 4,800 of 9,631 Elased 0:33:35. Training loss: 4.568117929250002. Learning Rate: 8.853542291042291e-05\n",
      "Batch 4,850 of 9,631 Elased 0:33:56. Training loss: 4.565235749716611. Learning Rate: 8.851807914307914e-05\n",
      "Batch 4,900 of 9,631 Elased 0:34:17. Training loss: 4.564578615889257. Learning Rate: 8.850073537573537e-05\n",
      "Batch 4,950 of 9,631 Elased 0:34:38. Training loss: 4.565025402945701. Learning Rate: 8.84833916083916e-05\n",
      "Batch 5,000 of 9,631 Elased 0:34:59. Training loss: 4.564993494224549. Learning Rate: 8.846604784104784e-05\n",
      "Batch 5,050 of 9,631 Elased 0:35:21. Training loss: 4.564742625302608. Learning Rate: 8.844870407370409e-05\n",
      "Batch 5,100 of 9,631 Elased 0:35:42. Training loss: 4.561919089719361. Learning Rate: 8.843136030636032e-05\n",
      "Batch 5,150 of 9,631 Elased 0:36:03. Training loss: 4.561534430587176. Learning Rate: 8.841401653901655e-05\n",
      "Batch 5,200 of 9,631 Elased 0:36:24. Training loss: 4.560975072292181. Learning Rate: 8.839667277167278e-05\n",
      "Batch 5,250 of 9,631 Elased 0:36:45. Training loss: 4.56068673696972. Learning Rate: 8.8379329004329e-05\n",
      "Batch 5,300 of 9,631 Elased 0:37:06. Training loss: 4.558832990628368. Learning Rate: 8.836198523698525e-05\n",
      "Batch 5,350 of 9,631 Elased 0:37:27. Training loss: 4.560219217683668. Learning Rate: 8.834464146964148e-05\n",
      "Batch 5,400 of 9,631 Elased 0:37:49. Training loss: 4.561065129350733. Learning Rate: 8.83272977022977e-05\n",
      "Batch 5,450 of 9,631 Elased 0:38:10. Training loss: 4.560652400419253. Learning Rate: 8.830995393495394e-05\n",
      "Batch 5,500 of 9,631 Elased 0:38:31. Training loss: 4.560434387900613. Learning Rate: 8.829261016761016e-05\n",
      "Batch 5,550 of 9,631 Elased 0:38:52. Training loss: 4.560895022529739. Learning Rate: 8.827526640026641e-05\n",
      "Batch 5,600 of 9,631 Elased 0:39:13. Training loss: 4.559850680657796. Learning Rate: 8.825792263292264e-05\n",
      "Batch 5,650 of 9,631 Elased 0:39:34. Training loss: 4.562040538745644. Learning Rate: 8.824057886557887e-05\n",
      "Batch 5,700 of 9,631 Elased 0:39:55. Training loss: 4.560969260366339. Learning Rate: 8.822323509823511e-05\n",
      "Batch 5,750 of 9,631 Elased 0:40:16. Training loss: 4.559049870698348. Learning Rate: 8.820589133089134e-05\n",
      "Batch 5,800 of 9,631 Elased 0:40:37. Training loss: 4.558119895745968. Learning Rate: 8.818854756354757e-05\n",
      "Batch 5,850 of 9,631 Elased 0:40:58. Training loss: 4.557174625111442. Learning Rate: 8.817120379620381e-05\n",
      "Batch 5,900 of 9,631 Elased 0:41:20. Training loss: 4.5554690195342245. Learning Rate: 8.815386002886004e-05\n",
      "Batch 5,950 of 9,631 Elased 0:41:41. Training loss: 4.554535801030006. Learning Rate: 8.813651626151627e-05\n",
      "Batch 6,050 of 9,631 Elased 0:42:24. Training loss: 4.5527071182786925. Learning Rate: 8.810182872682873e-05\n",
      "Batch 6,100 of 9,631 Elased 0:42:45. Training loss: 4.552335287430247. Learning Rate: 8.808448495948496e-05\n",
      "Batch 6,150 of 9,631 Elased 0:43:06. Training loss: 4.55095972600022. Learning Rate: 8.80671411921412e-05\n",
      "Batch 6,200 of 9,631 Elased 0:43:27. Training loss: 4.548147466913346. Learning Rate: 8.804979742479743e-05\n",
      "Batch 6,500 of 9,631 Elased 0:45:34. Training loss: 4.542362104929411. Learning Rate: 8.794573482073483e-05\n",
      "Batch 6,550 of 9,631 Elased 0:45:55. Training loss: 4.541954810746754. Learning Rate: 8.792839105339106e-05\n",
      "Batch 6,600 of 9,631 Elased 0:46:16. Training loss: 4.542026283199137. Learning Rate: 8.791104728604729e-05\n",
      "Batch 6,650 of 9,631 Elased 0:46:37. Training loss: 4.540959927157352. Learning Rate: 8.789370351870352e-05\n",
      "Batch 6,700 of 9,631 Elased 0:46:59. Training loss: 4.541907650392447. Learning Rate: 8.787635975135976e-05\n",
      "Batch 6,750 of 9,631 Elased 0:47:19. Training loss: 4.540957328019319. Learning Rate: 8.785901598401599e-05\n",
      "Batch 6,800 of 9,631 Elased 0:47:41. Training loss: 4.539413989081102. Learning Rate: 8.784167221667222e-05\n",
      "Batch 6,850 of 9,631 Elased 0:48:02. Training loss: 4.538006366534825. Learning Rate: 8.782432844932845e-05\n",
      "Batch 6,900 of 9,631 Elased 0:48:23. Training loss: 4.538215418794881. Learning Rate: 8.780698468198468e-05\n",
      "Batch 6,950 of 9,631 Elased 0:48:44. Training loss: 4.536873833086851. Learning Rate: 8.778964091464091e-05\n",
      "Batch 7,000 of 9,631 Elased 0:49:06. Training loss: 4.535321946178164. Learning Rate: 8.777229714729715e-05\n",
      "Batch 7,050 of 9,631 Elased 0:49:27. Training loss: 4.533452732478473. Learning Rate: 8.775495337995338e-05\n",
      "Batch 7,100 of 9,631 Elased 0:49:48. Training loss: 4.533090275207036. Learning Rate: 8.773760961260962e-05\n",
      "Batch 7,150 of 9,631 Elased 0:50:09. Training loss: 4.53161040329433. Learning Rate: 8.772026584526585e-05\n",
      "Batch 7,200 of 9,631 Elased 0:50:30. Training loss: 4.53095527023077. Learning Rate: 8.770292207792208e-05\n",
      "Batch 7,250 of 9,631 Elased 0:50:51. Training loss: 4.528579941387834. Learning Rate: 8.768557831057832e-05\n",
      "Batch 7,300 of 9,631 Elased 0:51:12. Training loss: 4.528334809198771. Learning Rate: 8.766823454323455e-05\n",
      "Batch 7,350 of 9,631 Elased 0:51:33. Training loss: 4.526294310044269. Learning Rate: 8.765089077589078e-05\n",
      "Batch 7,400 of 9,631 Elased 0:51:54. Training loss: 4.523882091464223. Learning Rate: 8.763354700854701e-05\n",
      "Batch 7,450 of 9,631 Elased 0:52:15. Training loss: 4.522820114673384. Learning Rate: 8.761620324120324e-05\n",
      "Batch 7,500 of 9,631 Elased 0:52:36. Training loss: 4.520394614887238. Learning Rate: 8.759885947385947e-05\n",
      "Batch 7,550 of 9,631 Elased 0:52:57. Training loss: 4.519327840994525. Learning Rate: 8.758151570651571e-05\n",
      "Batch 7,600 of 9,631 Elased 0:53:16. Training loss: 4.519575398156517. Learning Rate: 8.756417193917194e-05\n",
      "Batch 7,650 of 9,631 Elased 0:53:35. Training loss: 4.519679379556694. Learning Rate: 8.754682817182817e-05\n",
      "Batch 7,700 of 9,631 Elased 0:53:55. Training loss: 4.517631675949344. Learning Rate: 8.75294844044844e-05\n",
      "Batch 7,750 of 9,631 Elased 0:54:14. Training loss: 4.514407742684887. Learning Rate: 8.751214063714064e-05\n",
      "Batch 7,800 of 9,631 Elased 0:54:33. Training loss: 4.514514647997343. Learning Rate: 8.749479686979687e-05\n",
      "Batch 7,850 of 9,631 Elased 0:54:53. Training loss: 4.514345971550911. Learning Rate: 8.747745310245312e-05\n",
      "Batch 7,900 of 9,631 Elased 0:55:12. Training loss: 4.512456409267233. Learning Rate: 8.746010933510935e-05\n",
      "Batch 7,950 of 9,631 Elased 0:55:31. Training loss: 4.5139463264537305. Learning Rate: 8.744276556776557e-05\n",
      "Batch 8,000 of 9,631 Elased 0:55:50. Training loss: 4.51299701565504. Learning Rate: 8.74254218004218e-05\n",
      "Batch 8,050 of 9,631 Elased 0:56:10. Training loss: 4.514452197773856. Learning Rate: 8.740807803307803e-05\n",
      "Batch 8,100 of 9,631 Elased 0:56:29. Training loss: 4.514015133704668. Learning Rate: 8.739073426573428e-05\n",
      "Batch 8,150 of 9,631 Elased 0:56:48. Training loss: 4.51394366998614. Learning Rate: 8.73733904983905e-05\n",
      "Batch 8,200 of 9,631 Elased 0:57:08. Training loss: 4.512804772970153. Learning Rate: 8.735604673104673e-05\n",
      "Batch 8,250 of 9,631 Elased 0:57:27. Training loss: 4.513139469262326. Learning Rate: 8.733870296370296e-05\n",
      "Batch 8,300 of 9,631 Elased 0:57:46. Training loss: 4.513599825192647. Learning Rate: 8.732135919635919e-05\n",
      "Batch 8,350 of 9,631 Elased 0:58:06. Training loss: 4.513017143032508. Learning Rate: 8.730401542901542e-05\n",
      "Batch 8,400 of 9,631 Elased 0:58:25. Training loss: 4.511920848630724. Learning Rate: 8.728667166167167e-05\n",
      "Batch 8,450 of 9,631 Elased 0:58:45. Training loss: 4.511155621991355. Learning Rate: 8.726932789432791e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8,500 of 9,631 Elased 0:59:04. Training loss: 4.509817350247327. Learning Rate: 8.725198412698414e-05\n",
      "Batch 8,550 of 9,631 Elased 0:59:23. Training loss: 4.508771184675875. Learning Rate: 8.723464035964037e-05\n",
      "Batch 8,600 of 9,631 Elased 0:59:43. Training loss: 4.50931403722874. Learning Rate: 8.72172965922966e-05\n",
      "Batch 8,650 of 9,631 Elased 1:00:03. Training loss: 4.507412620748399. Learning Rate: 8.719995282495283e-05\n",
      "Batch 8,700 of 9,631 Elased 1:00:22. Training loss: 4.506397217580642. Learning Rate: 8.718260905760907e-05\n",
      "Batch 8,750 of 9,631 Elased 1:00:42. Training loss: 4.505956056513105. Learning Rate: 8.71652652902653e-05\n",
      "Batch 8,800 of 9,631 Elased 1:01:01. Training loss: 4.5055230041796515. Learning Rate: 8.714792152292153e-05\n",
      "Batch 8,850 of 9,631 Elased 1:01:20. Training loss: 4.504690280176152. Learning Rate: 8.713057775557776e-05\n",
      "Batch 8,900 of 9,631 Elased 1:01:39. Training loss: 4.505335431206093. Learning Rate: 8.711323398823398e-05\n",
      "Batch 8,950 of 9,631 Elased 1:01:59. Training loss: 4.503961373121379. Learning Rate: 8.709589022089023e-05\n",
      "Batch 9,000 of 9,631 Elased 1:02:18. Training loss: 4.5019621325069. Learning Rate: 8.707854645354646e-05\n",
      "Batch 9,050 of 9,631 Elased 1:02:37. Training loss: 4.50276681697171. Learning Rate: 8.706120268620269e-05\n",
      "Batch 9,100 of 9,631 Elased 1:02:57. Training loss: 4.50157646724156. Learning Rate: 8.704385891885893e-05\n",
      "Batch 9,150 of 9,631 Elased 1:03:16. Training loss: 4.5014357498565. Learning Rate: 8.702651515151516e-05\n",
      "Batch 9,200 of 9,631 Elased 1:03:35. Training loss: 4.502459808380707. Learning Rate: 8.700917138417139e-05\n",
      "Batch 9,250 of 9,631 Elased 1:03:54. Training loss: 4.501551414799046. Learning Rate: 8.699182761682763e-05\n",
      "Batch 9,300 of 9,631 Elased 1:04:14. Training loss: 4.501692875944158. Learning Rate: 8.697448384948386e-05\n",
      "Batch 9,350 of 9,631 Elased 1:04:33. Training loss: 4.501185194311295. Learning Rate: 8.695714008214009e-05\n",
      "Batch 9,400 of 9,631 Elased 1:04:52. Training loss: 4.500843712725538. Learning Rate: 8.693979631479632e-05\n",
      "Batch 9,450 of 9,631 Elased 1:05:11. Training loss: 4.499482923638884. Learning Rate: 8.692245254745255e-05\n",
      "Batch 9,500 of 9,631 Elased 1:05:31. Training loss: 4.498500666693637. Learning Rate: 8.690510878010878e-05\n",
      "Batch 9,550 of 9,631 Elased 1:05:50. Training loss: 4.497448373065569. Learning Rate: 8.688776501276502e-05\n",
      "Batch 9,600 of 9,631 Elased 1:06:09. Training loss: 4.4978556757917. Learning Rate: 8.687042124542125e-05\n",
      "\n",
      "\n",
      "  Average training loss: 4.50\n",
      "  Training epcoh took: 1:06:20\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c72971ea9c4ff68c18acf491afb884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=67.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Validation Loss: 4.33\n",
      "  Validation took: 0:00:40\n",
      "\n",
      "======== Epoch 5 / 30 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f0444094a648719fd7b00ec73460a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch    50 of 9,631 Elased 0:00:19. Training loss: 4.401241111755371. Learning Rate: 8.684232434232434e-05\n",
      "Batch   100 of 9,631 Elased 0:00:38. Training loss: 4.363451952934265. Learning Rate: 8.682498057498057e-05\n",
      "Batch   150 of 9,631 Elased 0:00:57. Training loss: 4.383687890370687. Learning Rate: 8.680763680763681e-05\n",
      "Batch   200 of 9,631 Elased 0:01:17. Training loss: 4.431978179216385. Learning Rate: 8.679029304029304e-05\n",
      "Batch   250 of 9,631 Elased 0:01:36. Training loss: 4.367943364143372. Learning Rate: 8.677294927294928e-05\n",
      "Batch   300 of 9,631 Elased 0:01:55. Training loss: 4.3344782201449075. Learning Rate: 8.675560550560551e-05\n",
      "Batch   350 of 9,631 Elased 0:02:14. Training loss: 4.33043340751103. Learning Rate: 8.673826173826174e-05\n",
      "Batch   400 of 9,631 Elased 0:02:33. Training loss: 4.337765110731125. Learning Rate: 8.672091797091797e-05\n",
      "Batch   450 of 9,631 Elased 0:02:52. Training loss: 4.333973039521111. Learning Rate: 8.670357420357421e-05\n",
      "Batch   500 of 9,631 Elased 0:03:11. Training loss: 4.337606421947479. Learning Rate: 8.668623043623044e-05\n",
      "Batch   550 of 9,631 Elased 0:03:30. Training loss: 4.330877859809182. Learning Rate: 8.666888666888667e-05\n",
      "Batch   600 of 9,631 Elased 0:03:49. Training loss: 4.337569161653518. Learning Rate: 8.66515429015429e-05\n",
      "Batch   650 of 9,631 Elased 0:04:09. Training loss: 4.332558551568251. Learning Rate: 8.663419913419913e-05\n",
      "Batch   700 of 9,631 Elased 0:04:28. Training loss: 4.327458175250462. Learning Rate: 8.661685536685537e-05\n",
      "Batch   750 of 9,631 Elased 0:04:47. Training loss: 4.3139134772618615. Learning Rate: 8.65995115995116e-05\n",
      "Batch   800 of 9,631 Elased 0:05:06. Training loss: 4.310833051502705. Learning Rate: 8.658216783216783e-05\n",
      "Batch   850 of 9,631 Elased 0:05:25. Training loss: 4.299952740388758. Learning Rate: 8.656482406482406e-05\n",
      "Batch   900 of 9,631 Elased 0:05:44. Training loss: 4.3091930466228066. Learning Rate: 8.65474802974803e-05\n",
      "Batch   950 of 9,631 Elased 0:06:03. Training loss: 4.30205218114351. Learning Rate: 8.653013653013653e-05\n",
      "Batch 1,000 of 9,631 Elased 0:06:23. Training loss: 4.302858019828796. Learning Rate: 8.651279276279278e-05\n",
      "Batch 1,050 of 9,631 Elased 0:06:42. Training loss: 4.301465026764642. Learning Rate: 8.6495448995449e-05\n",
      "Batch 1,100 of 9,631 Elased 0:07:01. Training loss: 4.306141319274903. Learning Rate: 8.647810522810524e-05\n",
      "Batch 1,150 of 9,631 Elased 0:07:20. Training loss: 4.306100819214531. Learning Rate: 8.646076146076147e-05\n",
      "Batch 1,200 of 9,631 Elased 0:07:40. Training loss: 4.30589904030164. Learning Rate: 8.64434176934177e-05\n",
      "Batch 1,250 of 9,631 Elased 0:07:59. Training loss: 4.309316217231751. Learning Rate: 8.642607392607392e-05\n",
      "Batch 1,300 of 9,631 Elased 0:08:18. Training loss: 4.308957891280834. Learning Rate: 8.640873015873017e-05\n",
      "Batch 1,350 of 9,631 Elased 0:08:37. Training loss: 4.310759816699558. Learning Rate: 8.63913863913864e-05\n",
      "Batch 1,400 of 9,631 Elased 0:08:57. Training loss: 4.3100733329568595. Learning Rate: 8.637404262404262e-05\n",
      "Batch 1,450 of 9,631 Elased 0:09:16. Training loss: 4.307055657649863. Learning Rate: 8.635669885669885e-05\n",
      "Batch 1,500 of 9,631 Elased 0:09:35. Training loss: 4.301523895899455. Learning Rate: 8.633935508935508e-05\n",
      "Batch 1,550 of 9,631 Elased 0:09:55. Training loss: 4.30065258518342. Learning Rate: 8.632201132201133e-05\n",
      "Batch 1,600 of 9,631 Elased 0:10:14. Training loss: 4.306313027143478. Learning Rate: 8.630466755466756e-05\n",
      "Batch 1,650 of 9,631 Elased 0:10:33. Training loss: 4.314529858791467. Learning Rate: 8.62873237873238e-05\n",
      "Batch 1,700 of 9,631 Elased 0:10:52. Training loss: 4.309767068133635. Learning Rate: 8.626998001998003e-05\n",
      "Batch 1,750 of 9,631 Elased 0:11:11. Training loss: 4.3079039600917275. Learning Rate: 8.625263625263626e-05\n",
      "Batch 1,800 of 9,631 Elased 0:11:30. Training loss: 4.303819929891162. Learning Rate: 8.623529248529249e-05\n",
      "Batch 1,850 of 9,631 Elased 0:11:49. Training loss: 4.306763060543989. Learning Rate: 8.621794871794873e-05\n",
      "Batch 1,900 of 9,631 Elased 0:12:09. Training loss: 4.3046924591064455. Learning Rate: 8.620060495060496e-05\n",
      "Batch 1,950 of 9,631 Elased 0:12:28. Training loss: 4.300817890167236. Learning Rate: 8.618326118326119e-05\n",
      "Batch 2,000 of 9,631 Elased 0:12:47. Training loss: 4.295229762077332. Learning Rate: 8.616591741591742e-05\n",
      "Batch 2,050 of 9,631 Elased 0:13:06. Training loss: 4.29563735357145. Learning Rate: 8.614857364857365e-05\n",
      "Batch 2,100 of 9,631 Elased 0:13:26. Training loss: 4.298272421927679. Learning Rate: 8.613122988122988e-05\n",
      "Batch 2,150 of 9,631 Elased 0:13:45. Training loss: 4.299082457742026. Learning Rate: 8.611388611388612e-05\n",
      "Batch 2,200 of 9,631 Elased 0:14:04. Training loss: 4.299596825296229. Learning Rate: 8.609654234654235e-05\n",
      "Batch 2,250 of 9,631 Elased 0:14:24. Training loss: 4.2978972148895265. Learning Rate: 8.607919857919858e-05\n",
      "Batch 2,300 of 9,631 Elased 0:14:43. Training loss: 4.298554924985637. Learning Rate: 8.606185481185482e-05\n",
      "Batch 2,350 of 9,631 Elased 0:15:02. Training loss: 4.295147616102341. Learning Rate: 8.604451104451105e-05\n",
      "Batch 2,400 of 9,631 Elased 0:15:22. Training loss: 4.294244687060515. Learning Rate: 8.602716727716729e-05\n",
      "Batch 2,450 of 9,631 Elased 0:15:41. Training loss: 4.2913356826743305. Learning Rate: 8.600982350982352e-05\n",
      "Batch 2,500 of 9,631 Elased 0:16:01. Training loss: 4.287548379993439. Learning Rate: 8.599247974247975e-05\n",
      "Batch 2,550 of 9,631 Elased 0:16:20. Training loss: 4.29280569226134. Learning Rate: 8.597513597513598e-05\n",
      "Batch 2,600 of 9,631 Elased 0:16:39. Training loss: 4.293945266008377. Learning Rate: 8.595779220779221e-05\n",
      "Batch 2,650 of 9,631 Elased 0:16:58. Training loss: 4.29536704756179. Learning Rate: 8.594044844044844e-05\n",
      "Batch 2,700 of 9,631 Elased 0:17:17. Training loss: 4.299498360775135. Learning Rate: 8.592310467310468e-05\n",
      "Batch 2,750 of 9,631 Elased 0:17:37. Training loss: 4.29642031843012. Learning Rate: 8.590576090576091e-05\n",
      "Batch 2,800 of 9,631 Elased 0:17:56. Training loss: 4.298022604073797. Learning Rate: 8.588841713841714e-05\n",
      "Batch 2,850 of 9,631 Elased 0:18:16. Training loss: 4.300672329266866. Learning Rate: 8.587107337107337e-05\n",
      "Batch 2,900 of 9,631 Elased 0:18:35. Training loss: 4.300282485402864. Learning Rate: 8.585372960372961e-05\n",
      "Batch 2,950 of 9,631 Elased 0:18:54. Training loss: 4.299395643250417. Learning Rate: 8.583638583638584e-05\n",
      "Batch 3,000 of 9,631 Elased 0:19:13. Training loss: 4.30118710788091. Learning Rate: 8.581904206904208e-05\n",
      "Batch 3,050 of 9,631 Elased 0:19:32. Training loss: 4.300002183210655. Learning Rate: 8.580169830169831e-05\n",
      "Batch 3,100 of 9,631 Elased 0:19:52. Training loss: 4.299842724569382. Learning Rate: 8.578435453435454e-05\n",
      "Batch 3,150 of 9,631 Elased 0:20:11. Training loss: 4.294083599287366. Learning Rate: 8.576701076701077e-05\n",
      "Batch 3,200 of 9,631 Elased 0:20:30. Training loss: 4.2959949488192795. Learning Rate: 8.5749666999667e-05\n",
      "Batch 3,250 of 9,631 Elased 0:20:49. Training loss: 4.294964099810674. Learning Rate: 8.573232323232324e-05\n",
      "Batch 3,300 of 9,631 Elased 0:21:08. Training loss: 4.298328582807021. Learning Rate: 8.571497946497947e-05\n",
      "Batch 3,350 of 9,631 Elased 0:21:28. Training loss: 4.298576393340951. Learning Rate: 8.56976356976357e-05\n",
      "Batch 3,400 of 9,631 Elased 0:21:47. Training loss: 4.298642975652919. Learning Rate: 8.568029193029193e-05\n",
      "Batch 3,450 of 9,631 Elased 0:22:06. Training loss: 4.2987191287330955. Learning Rate: 8.566294816294816e-05\n",
      "Batch 3,500 of 9,631 Elased 0:22:26. Training loss: 4.296023547445024. Learning Rate: 8.564560439560439e-05\n",
      "Batch 3,550 of 9,631 Elased 0:22:45. Training loss: 4.296029340112713. Learning Rate: 8.562826062826063e-05\n",
      "Batch 3,600 of 9,631 Elased 0:23:05. Training loss: 4.297204569180806. Learning Rate: 8.561091686091686e-05\n",
      "Batch 3,650 of 9,631 Elased 0:23:24. Training loss: 4.301029500961303. Learning Rate: 8.55935730935731e-05\n",
      "Batch 3,700 of 9,631 Elased 0:23:43. Training loss: 4.2997569776225735. Learning Rate: 8.557622932622933e-05\n",
      "Batch 3,750 of 9,631 Elased 0:24:03. Training loss: 4.2970500658035276. Learning Rate: 8.555888555888556e-05\n",
      "Batch 3,800 of 9,631 Elased 0:24:22. Training loss: 4.2943340478445355. Learning Rate: 8.554154179154179e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3,850 of 9,631 Elased 0:24:41. Training loss: 4.290315037392951. Learning Rate: 8.552419802419804e-05\n",
      "Batch 3,900 of 9,631 Elased 0:25:00. Training loss: 4.288494435823881. Learning Rate: 8.550685425685426e-05\n",
      "Batch 3,950 of 9,631 Elased 0:25:19. Training loss: 4.289129205896884. Learning Rate: 8.54895104895105e-05\n",
      "Batch 4,000 of 9,631 Elased 0:25:38. Training loss: 4.289208223223686. Learning Rate: 8.547216672216672e-05\n",
      "Batch 4,050 of 9,631 Elased 0:25:58. Training loss: 4.287828343120622. Learning Rate: 8.545482295482295e-05\n",
      "Batch 4,100 of 9,631 Elased 0:26:17. Training loss: 4.286720957407137. Learning Rate: 8.54374791874792e-05\n",
      "Batch 4,150 of 9,631 Elased 0:26:36. Training loss: 4.286735284989138. Learning Rate: 8.542013542013542e-05\n",
      "Batch 4,200 of 9,631 Elased 0:26:56. Training loss: 4.2856574523448945. Learning Rate: 8.540279165279165e-05\n",
      "Batch 4,250 of 9,631 Elased 0:27:15. Training loss: 4.284594163894654. Learning Rate: 8.538544788544788e-05\n",
      "Batch 4,300 of 9,631 Elased 0:27:34. Training loss: 4.284117545138958. Learning Rate: 8.536810411810413e-05\n",
      "Batch 4,350 of 9,631 Elased 0:27:54. Training loss: 4.283461713297614. Learning Rate: 8.535076035076036e-05\n",
      "Batch 4,400 of 9,631 Elased 0:28:13. Training loss: 4.283852006467906. Learning Rate: 8.53334165834166e-05\n",
      "Batch 4,450 of 9,631 Elased 0:28:33. Training loss: 4.284517940135484. Learning Rate: 8.531607281607283e-05\n",
      "Batch 4,500 of 9,631 Elased 0:28:52. Training loss: 4.285123838159773. Learning Rate: 8.529872904872906e-05\n",
      "Batch 4,550 of 9,631 Elased 0:29:12. Training loss: 4.28206364778372. Learning Rate: 8.528138528138529e-05\n",
      "Batch 4,600 of 9,631 Elased 0:29:31. Training loss: 4.283182460121487. Learning Rate: 8.526404151404151e-05\n",
      "Batch 4,650 of 9,631 Elased 0:29:50. Training loss: 4.280572195565829. Learning Rate: 8.524669774669774e-05\n",
      "Batch 4,700 of 9,631 Elased 0:30:10. Training loss: 4.281035856896258. Learning Rate: 8.522935397935399e-05\n",
      "Batch 4,750 of 9,631 Elased 0:30:29. Training loss: 4.279424288649308. Learning Rate: 8.521201021201022e-05\n",
      "Batch 4,800 of 9,631 Elased 0:30:48. Training loss: 4.277434477905432. Learning Rate: 8.519466644466645e-05\n",
      "Batch 4,850 of 9,631 Elased 0:31:07. Training loss: 4.275096934407028. Learning Rate: 8.517732267732267e-05\n",
      "Batch 4,900 of 9,631 Elased 0:31:27. Training loss: 4.27517567673508. Learning Rate: 8.51599789099789e-05\n",
      "Batch 4,950 of 9,631 Elased 0:31:46. Training loss: 4.275346691873338. Learning Rate: 8.514263514263515e-05\n",
      "Batch 5,000 of 9,631 Elased 0:32:05. Training loss: 4.275950871896744. Learning Rate: 8.512529137529138e-05\n",
      "Batch 5,050 of 9,631 Elased 0:32:24. Training loss: 4.276187315364876. Learning Rate: 8.510794760794762e-05\n",
      "Batch 5,100 of 9,631 Elased 0:32:44. Training loss: 4.273551192564123. Learning Rate: 8.509060384060385e-05\n",
      "Batch 5,150 of 9,631 Elased 0:33:03. Training loss: 4.2743514981316135. Learning Rate: 8.507326007326008e-05\n",
      "Batch 5,200 of 9,631 Elased 0:33:22. Training loss: 4.273977612486252. Learning Rate: 8.505591630591631e-05\n",
      "Batch 5,250 of 9,631 Elased 0:33:42. Training loss: 4.274033895265489. Learning Rate: 8.503857253857255e-05\n",
      "Batch 5,300 of 9,631 Elased 0:34:01. Training loss: 4.272453665148537. Learning Rate: 8.502122877122878e-05\n",
      "Batch 5,350 of 9,631 Elased 0:34:20. Training loss: 4.274272918701172. Learning Rate: 8.500388500388501e-05\n",
      "Batch 5,400 of 9,631 Elased 0:34:39. Training loss: 4.273851618325269. Learning Rate: 8.498654123654124e-05\n",
      "Batch 5,450 of 9,631 Elased 0:34:59. Training loss: 4.27364396458372. Learning Rate: 8.496919746919747e-05\n",
      "Batch 5,500 of 9,631 Elased 0:35:18. Training loss: 4.273047259027308. Learning Rate: 8.49518537018537e-05\n",
      "Batch 5,550 of 9,631 Elased 0:35:38. Training loss: 4.2731014755609875. Learning Rate: 8.493450993450994e-05\n",
      "Batch 5,600 of 9,631 Elased 0:35:57. Training loss: 4.2718405434915. Learning Rate: 8.491716616716617e-05\n",
      "Batch 5,650 of 9,631 Elased 0:36:16. Training loss: 4.273903733903328. Learning Rate: 8.48998223998224e-05\n",
      "Batch 5,700 of 9,631 Elased 0:36:35. Training loss: 4.273315780497434. Learning Rate: 8.488247863247864e-05\n",
      "Batch 5,750 of 9,631 Elased 0:36:54. Training loss: 4.2719294356885165. Learning Rate: 8.486513486513487e-05\n",
      "Batch 5,800 of 9,631 Elased 0:37:13. Training loss: 4.271489182710647. Learning Rate: 8.484779109779111e-05\n",
      "Batch 5,850 of 9,631 Elased 0:37:33. Training loss: 4.271433160447667. Learning Rate: 8.483044733044734e-05\n",
      "Batch 5,900 of 9,631 Elased 0:37:52. Training loss: 4.270022505824849. Learning Rate: 8.481310356310357e-05\n",
      "Batch 5,950 of 9,631 Elased 0:38:11. Training loss: 4.269784948204745. Learning Rate: 8.47957597957598e-05\n",
      "Batch 6,000 of 9,631 Elased 0:38:30. Training loss: 4.268798257152239. Learning Rate: 8.477841602841603e-05\n",
      "Batch 6,050 of 9,631 Elased 0:38:50. Training loss: 4.268670514556002. Learning Rate: 8.476107226107226e-05\n",
      "Batch 6,100 of 9,631 Elased 0:39:09. Training loss: 4.2686631450105885. Learning Rate: 8.47437284937285e-05\n",
      "Batch 6,150 of 9,631 Elased 0:39:28. Training loss: 4.267453472749974. Learning Rate: 8.472638472638473e-05\n",
      "Batch 6,200 of 9,631 Elased 0:39:47. Training loss: 4.264700191251693. Learning Rate: 8.470904095904096e-05\n",
      "Batch 6,250 of 9,631 Elased 0:40:07. Training loss: 4.264058473014831. Learning Rate: 8.469169719169719e-05\n",
      "Batch 6,300 of 9,631 Elased 0:40:26. Training loss: 4.263907787724147. Learning Rate: 8.467435342435343e-05\n",
      "Batch 6,350 of 9,631 Elased 0:40:45. Training loss: 4.262859168052674. Learning Rate: 8.465700965700966e-05\n",
      "Batch 6,400 of 9,631 Elased 0:41:04. Training loss: 4.260914737693965. Learning Rate: 8.46396658896659e-05\n",
      "Batch 6,450 of 9,631 Elased 0:41:24. Training loss: 4.262391754823137. Learning Rate: 8.462232212232213e-05\n",
      "Batch 6,500 of 9,631 Elased 0:41:43. Training loss: 4.260088404435378. Learning Rate: 8.460497835497836e-05\n",
      "Batch 6,550 of 9,631 Elased 0:42:02. Training loss: 4.259677593107442. Learning Rate: 8.458763458763459e-05\n",
      "Batch 6,600 of 9,631 Elased 0:42:22. Training loss: 4.260855180964326. Learning Rate: 8.457029082029082e-05\n",
      "Batch 6,650 of 9,631 Elased 0:42:41. Training loss: 4.260172887135269. Learning Rate: 8.455294705294706e-05\n",
      "Batch 6,700 of 9,631 Elased 0:43:00. Training loss: 4.260996815090749. Learning Rate: 8.45356032856033e-05\n",
      "Batch 6,750 of 9,631 Elased 0:43:19. Training loss: 4.260304522655629. Learning Rate: 8.451825951825952e-05\n",
      "Batch 6,800 of 9,631 Elased 0:43:39. Training loss: 4.258542584496386. Learning Rate: 8.450091575091575e-05\n",
      "Batch 6,850 of 9,631 Elased 0:43:58. Training loss: 4.257553659285942. Learning Rate: 8.448357198357198e-05\n",
      "Batch 6,900 of 9,631 Elased 0:44:17. Training loss: 4.257411560666734. Learning Rate: 8.446622821622821e-05\n",
      "Batch 6,950 of 9,631 Elased 0:44:37. Training loss: 4.25655880420328. Learning Rate: 8.444888444888445e-05\n",
      "Batch 7,000 of 9,631 Elased 0:44:56. Training loss: 4.255450475658689. Learning Rate: 8.443154068154068e-05\n",
      "Batch 7,050 of 9,631 Elased 0:45:15. Training loss: 4.25413178227472. Learning Rate: 8.441419691419693e-05\n",
      "Batch 7,100 of 9,631 Elased 0:45:34. Training loss: 4.2538489565043385. Learning Rate: 8.439685314685315e-05\n",
      "Batch 7,150 of 9,631 Elased 0:45:53. Training loss: 4.2530398699953835. Learning Rate: 8.437950937950938e-05\n",
      "Batch 7,200 of 9,631 Elased 0:46:13. Training loss: 4.252736539476448. Learning Rate: 8.436216561216561e-05\n",
      "Batch 7,250 of 9,631 Elased 0:46:32. Training loss: 4.250960350398359. Learning Rate: 8.434482184482186e-05\n",
      "Batch 7,300 of 9,631 Elased 0:46:51. Training loss: 4.25079267883954. Learning Rate: 8.432747807747809e-05\n",
      "Batch 7,350 of 9,631 Elased 0:47:10. Training loss: 4.249506877360701. Learning Rate: 8.431013431013431e-05\n",
      "Batch 7,400 of 9,631 Elased 0:47:29. Training loss: 4.247503947663952. Learning Rate: 8.429279054279054e-05\n",
      "Batch 7,450 of 9,631 Elased 0:47:48. Training loss: 4.247077593099351. Learning Rate: 8.427544677544677e-05\n",
      "Batch 7,500 of 9,631 Elased 0:48:08. Training loss: 4.245131223233541. Learning Rate: 8.425810300810302e-05\n",
      "Batch 7,550 of 9,631 Elased 0:48:27. Training loss: 4.245086928708663. Learning Rate: 8.424075924075925e-05\n",
      "Batch 7,600 of 9,631 Elased 0:48:46. Training loss: 4.245488346815109. Learning Rate: 8.422341547341547e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7,650 of 9,631 Elased 0:49:06. Training loss: 4.245328917472191. Learning Rate: 8.42060717060717e-05\n",
      "Batch 7,700 of 9,631 Elased 0:49:25. Training loss: 4.24335516979168. Learning Rate: 8.418872793872795e-05\n",
      "Batch 7,750 of 9,631 Elased 0:49:44. Training loss: 4.240615812516982. Learning Rate: 8.417138417138418e-05\n",
      "Batch 7,800 of 9,631 Elased 0:50:04. Training loss: 4.2409204136102625. Learning Rate: 8.415404040404042e-05\n",
      "Batch 7,850 of 9,631 Elased 0:50:23. Training loss: 4.241128661404749. Learning Rate: 8.413669663669665e-05\n",
      "Batch 7,900 of 9,631 Elased 0:50:43. Training loss: 4.238885245624977. Learning Rate: 8.411935286935288e-05\n",
      "Batch 7,950 of 9,631 Elased 0:51:02. Training loss: 4.240246759420671. Learning Rate: 8.41020091020091e-05\n",
      "Batch 8,000 of 9,631 Elased 0:51:21. Training loss: 4.239905813604593. Learning Rate: 8.408466533466534e-05\n",
      "Batch 8,050 of 9,631 Elased 0:51:40. Training loss: 4.241308164922347. Learning Rate: 8.406732156732156e-05\n",
      "Batch 8,100 of 9,631 Elased 0:52:00. Training loss: 4.241197769494704. Learning Rate: 8.404997779997781e-05\n",
      "Batch 8,150 of 9,631 Elased 0:52:19. Training loss: 4.241771013107768. Learning Rate: 8.403263403263404e-05\n",
      "Batch 8,200 of 9,631 Elased 0:52:38. Training loss: 4.240536417844819. Learning Rate: 8.401529026529027e-05\n",
      "Batch 8,250 of 9,631 Elased 0:52:58. Training loss: 4.240795545000019. Learning Rate: 8.39979464979465e-05\n",
      "Batch 8,300 of 9,631 Elased 0:53:17. Training loss: 4.240944045423025. Learning Rate: 8.398060273060272e-05\n",
      "Batch 8,350 of 9,631 Elased 0:53:36. Training loss: 4.240759649904902. Learning Rate: 8.396325896325897e-05\n",
      "Batch 8,400 of 9,631 Elased 0:53:56. Training loss: 4.239719376649175. Learning Rate: 8.39459151959152e-05\n",
      "Batch 8,450 of 9,631 Elased 0:54:15. Training loss: 4.238807793538246. Learning Rate: 8.392857142857144e-05\n",
      "Batch 8,500 of 9,631 Elased 0:54:35. Training loss: 4.237536136683295. Learning Rate: 8.391122766122767e-05\n",
      "Batch 8,550 of 9,631 Elased 0:54:54. Training loss: 4.236764863806162. Learning Rate: 8.38938838938839e-05\n",
      "Batch 8,600 of 9,631 Elased 0:55:14. Training loss: 4.237449733379275. Learning Rate: 8.387654012654013e-05\n",
      "Batch 8,650 of 9,631 Elased 0:55:33. Training loss: 4.235742799257268. Learning Rate: 8.385919635919637e-05\n",
      "Batch 8,700 of 9,631 Elased 0:55:52. Training loss: 4.235500129721631. Learning Rate: 8.38418525918526e-05\n",
      "Batch 8,750 of 9,631 Elased 0:56:11. Training loss: 4.23487653805869. Learning Rate: 8.382450882450883e-05\n",
      "Batch 8,800 of 9,631 Elased 0:56:30. Training loss: 4.234414539120414. Learning Rate: 8.380716505716506e-05\n",
      "Batch 8,850 of 9,631 Elased 0:56:50. Training loss: 4.233521296964527. Learning Rate: 8.378982128982129e-05\n",
      "Batch 8,900 of 9,631 Elased 0:57:09. Training loss: 4.23389482074909. Learning Rate: 8.377247752247752e-05\n",
      "Batch 8,950 of 9,631 Elased 0:57:28. Training loss: 4.233352892225681. Learning Rate: 8.375513375513376e-05\n",
      "Batch 9,000 of 9,631 Elased 0:57:48. Training loss: 4.231650593704647. Learning Rate: 8.373778998778999e-05\n",
      "Batch 9,050 of 9,631 Elased 0:58:07. Training loss: 4.232244876761463. Learning Rate: 8.372044622044623e-05\n",
      "Batch 9,100 of 9,631 Elased 0:58:26. Training loss: 4.231378956758059. Learning Rate: 8.370310245310246e-05\n",
      "Batch 9,150 of 9,631 Elased 0:58:45. Training loss: 4.231293177109598. Learning Rate: 8.368575868575869e-05\n",
      "Batch 9,200 of 9,631 Elased 0:59:05. Training loss: 4.232414927093879. Learning Rate: 8.366841491841493e-05\n",
      "Batch 9,250 of 9,631 Elased 0:59:24. Training loss: 4.2316439338374785. Learning Rate: 8.365107115107116e-05\n",
      "Batch 9,300 of 9,631 Elased 0:59:43. Training loss: 4.231943537496751. Learning Rate: 8.363372738372739e-05\n",
      "Batch 9,350 of 9,631 Elased 1:00:02. Training loss: 4.231551375440098. Learning Rate: 8.361638361638362e-05\n",
      "Batch 9,400 of 9,631 Elased 1:00:22. Training loss: 4.231259481703981. Learning Rate: 8.359903984903985e-05\n",
      "Batch 9,450 of 9,631 Elased 1:00:41. Training loss: 4.230370665858032. Learning Rate: 8.358169608169608e-05\n",
      "Batch 9,500 of 9,631 Elased 1:01:01. Training loss: 4.2297513223949235. Learning Rate: 8.356435231435232e-05\n",
      "Batch 9,550 of 9,631 Elased 1:01:20. Training loss: 4.228679367185263. Learning Rate: 8.354700854700855e-05\n",
      "Batch 9,600 of 9,631 Elased 1:01:39. Training loss: 4.228985735227664. Learning Rate: 8.352966477966478e-05\n",
      "\n",
      "\n",
      "  Average training loss: 4.23\n",
      "  Training epcoh took: 1:01:51\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2fba9abce804dd39c70c5834ca3db2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=67.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Validation Loss: 4.10\n",
      "  Validation took: 0:00:40\n",
      "\n",
      "======== Epoch 6 / 30 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa57163e4344439ac03898dcdb59c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch    50 of 9,631 Elased 0:00:19. Training loss: 4.1862328910827635. Learning Rate: 8.350156787656787e-05\n",
      "Batch   100 of 9,631 Elased 0:00:38. Training loss: 4.14346019744873. Learning Rate: 8.348422410922411e-05\n",
      "Batch   150 of 9,631 Elased 0:00:57. Training loss: 4.1523012320200605. Learning Rate: 8.346688034188034e-05\n",
      "Batch   200 of 9,631 Elased 0:01:16. Training loss: 4.201073573827744. Learning Rate: 8.344953657453659e-05\n",
      "Batch   250 of 9,631 Elased 0:01:36. Training loss: 4.142254511833191. Learning Rate: 8.343219280719282e-05\n",
      "Batch   300 of 9,631 Elased 0:01:55. Training loss: 4.115997046629587. Learning Rate: 8.341484903984904e-05\n",
      "Batch   350 of 9,631 Elased 0:02:14. Training loss: 4.109775610651289. Learning Rate: 8.339750527250527e-05\n",
      "Batch   400 of 9,631 Elased 0:02:33. Training loss: 4.124287500977516. Learning Rate: 8.338016150516152e-05\n",
      "Batch   450 of 9,631 Elased 0:02:52. Training loss: 4.120228755739. Learning Rate: 8.336281773781775e-05\n",
      "Batch   500 of 9,631 Elased 0:03:11. Training loss: 4.124695254325867. Learning Rate: 8.334547397047398e-05\n",
      "Batch   550 of 9,631 Elased 0:03:30. Training loss: 4.116684067899531. Learning Rate: 8.33281302031302e-05\n",
      "Batch   600 of 9,631 Elased 0:03:49. Training loss: 4.120564812421799. Learning Rate: 8.331078643578643e-05\n",
      "Batch   650 of 9,631 Elased 0:04:08. Training loss: 4.120005505268391. Learning Rate: 8.329344266844266e-05\n",
      "Batch   700 of 9,631 Elased 0:04:27. Training loss: 4.116276539053236. Learning Rate: 8.32760989010989e-05\n",
      "Batch   750 of 9,631 Elased 0:04:46. Training loss: 4.103026493708293. Learning Rate: 8.325875513375514e-05\n",
      "Batch   800 of 9,631 Elased 0:05:06. Training loss: 4.101238255500793. Learning Rate: 8.324141136641136e-05\n",
      "Batch   850 of 9,631 Elased 0:05:25. Training loss: 4.094050153003019. Learning Rate: 8.322406759906761e-05\n",
      "Batch   900 of 9,631 Elased 0:05:44. Training loss: 4.097498517831166. Learning Rate: 8.320672383172384e-05\n",
      "Batch   950 of 9,631 Elased 0:06:04. Training loss: 4.0914818984583805. Learning Rate: 8.318938006438008e-05\n",
      "Batch 1,000 of 9,631 Elased 0:06:23. Training loss: 4.090514240026474. Learning Rate: 8.317203629703631e-05\n",
      "Batch 1,050 of 9,631 Elased 0:06:43. Training loss: 4.090910976500738. Learning Rate: 8.315469252969254e-05\n",
      "Batch 1,100 of 9,631 Elased 0:07:02. Training loss: 4.0923338499936195. Learning Rate: 8.313734876234877e-05\n",
      "Batch 1,150 of 9,631 Elased 0:07:22. Training loss: 4.092126078398332. Learning Rate: 8.3120004995005e-05\n",
      "Batch 1,200 of 9,631 Elased 0:07:41. Training loss: 4.095801840821902. Learning Rate: 8.310266122766123e-05\n",
      "Batch 1,250 of 9,631 Elased 0:08:01. Training loss: 4.098813548088073. Learning Rate: 8.308531746031747e-05\n",
      "Batch 1,300 of 9,631 Elased 0:08:20. Training loss: 4.101228119043204. Learning Rate: 8.30679736929737e-05\n",
      "Batch 1,350 of 9,631 Elased 0:08:39. Training loss: 4.099513622036687. Learning Rate: 8.305062992562993e-05\n",
      "Batch 1,400 of 9,631 Elased 0:08:59. Training loss: 4.100044052600861. Learning Rate: 8.303328615828616e-05\n",
      "Batch 1,450 of 9,631 Elased 0:09:18. Training loss: 4.097531533652338. Learning Rate: 8.301594239094239e-05\n",
      "Batch 1,500 of 9,631 Elased 0:09:37. Training loss: 4.093909014781316. Learning Rate: 8.299859862359863e-05\n",
      "Batch 1,550 of 9,631 Elased 0:09:56. Training loss: 4.094049963412746. Learning Rate: 8.298125485625486e-05\n",
      "Batch 1,600 of 9,631 Elased 0:10:16. Training loss: 4.099664343222976. Learning Rate: 8.29639110889111e-05\n",
      "Batch 1,650 of 9,631 Elased 0:10:35. Training loss: 4.107195689533696. Learning Rate: 8.294656732156733e-05\n",
      "Batch 1,700 of 9,631 Elased 0:10:54. Training loss: 4.1038041328682615. Learning Rate: 8.292922355422356e-05\n",
      "Batch 1,750 of 9,631 Elased 0:11:13. Training loss: 4.1032328377451215. Learning Rate: 8.291187978687979e-05\n",
      "Batch 1,800 of 9,631 Elased 0:11:32. Training loss: 4.10091898686356. Learning Rate: 8.289453601953603e-05\n",
      "Batch 1,850 of 9,631 Elased 0:11:52. Training loss: 4.103479920400155. Learning Rate: 8.287719225219226e-05\n",
      "Batch 1,900 of 9,631 Elased 0:12:11. Training loss: 4.1012084708088326. Learning Rate: 8.285984848484849e-05\n",
      "Batch 1,950 of 9,631 Elased 0:12:31. Training loss: 4.097201908735128. Learning Rate: 8.284250471750472e-05\n",
      "Batch 2,000 of 9,631 Elased 0:12:50. Training loss: 4.092351776540279. Learning Rate: 8.282516095016095e-05\n",
      "Batch 2,050 of 9,631 Elased 0:13:09. Training loss: 4.092384392052162. Learning Rate: 8.280781718281718e-05\n",
      "Batch 2,100 of 9,631 Elased 0:13:28. Training loss: 4.09532362432707. Learning Rate: 8.279047341547342e-05\n",
      "Batch 2,150 of 9,631 Elased 0:13:48. Training loss: 4.094870511154796. Learning Rate: 8.277312964812965e-05\n",
      "Batch 2,200 of 9,631 Elased 0:14:07. Training loss: 4.094515922015364. Learning Rate: 8.275578588078588e-05\n",
      "Batch 2,250 of 9,631 Elased 0:14:26. Training loss: 4.0925673376189335. Learning Rate: 8.273844211344212e-05\n",
      "Batch 2,300 of 9,631 Elased 0:14:46. Training loss: 4.093032032665999. Learning Rate: 8.272109834609835e-05\n",
      "Batch 2,350 of 9,631 Elased 0:15:05. Training loss: 4.090328535982903. Learning Rate: 8.270375457875458e-05\n",
      "Batch 2,400 of 9,631 Elased 0:15:24. Training loss: 4.091062646359205. Learning Rate: 8.268641081141082e-05\n",
      "Batch 2,450 of 9,631 Elased 0:15:43. Training loss: 4.089474927016667. Learning Rate: 8.266906704406705e-05\n",
      "Batch 2,500 of 9,631 Elased 0:16:03. Training loss: 4.086031904935837. Learning Rate: 8.265172327672328e-05\n",
      "Batch 2,550 of 9,631 Elased 0:16:22. Training loss: 4.090340903366314. Learning Rate: 8.263437950937951e-05\n",
      "Batch 2,600 of 9,631 Elased 0:16:41. Training loss: 4.0916060225321695. Learning Rate: 8.261703574203574e-05\n",
      "Batch 2,650 of 9,631 Elased 0:17:01. Training loss: 4.093371078878079. Learning Rate: 8.259969197469198e-05\n",
      "Batch 2,700 of 9,631 Elased 0:17:20. Training loss: 4.096749872499042. Learning Rate: 8.258234820734821e-05\n",
      "Batch 2,750 of 9,631 Elased 0:17:39. Training loss: 4.094602966698734. Learning Rate: 8.256500444000444e-05\n",
      "Batch 2,800 of 9,631 Elased 0:17:58. Training loss: 4.095402261061328. Learning Rate: 8.254766067266067e-05\n",
      "Batch 2,850 of 9,631 Elased 0:18:18. Training loss: 4.096851180938252. Learning Rate: 8.25303169053169e-05\n",
      "Batch 2,900 of 9,631 Elased 0:18:38. Training loss: 4.096790790516755. Learning Rate: 8.251297313797314e-05\n",
      "Batch 2,950 of 9,631 Elased 0:18:57. Training loss: 4.095939347663168. Learning Rate: 8.249562937062939e-05\n",
      "Batch 3,000 of 9,631 Elased 0:19:16. Training loss: 4.098675711194674. Learning Rate: 8.247828560328562e-05\n",
      "Batch 3,050 of 9,631 Elased 0:19:35. Training loss: 4.098211384249515. Learning Rate: 8.246094183594184e-05\n",
      "Batch 3,100 of 9,631 Elased 0:19:54. Training loss: 4.098288398212002. Learning Rate: 8.244359806859807e-05\n",
      "Batch 3,150 of 9,631 Elased 0:20:14. Training loss: 4.09307490806731. Learning Rate: 8.24262543012543e-05\n",
      "Batch 3,200 of 9,631 Elased 0:20:33. Training loss: 4.094637794755399. Learning Rate: 8.240891053391053e-05\n",
      "Batch 3,250 of 9,631 Elased 0:20:53. Training loss: 4.094443546478565. Learning Rate: 8.239156676656677e-05\n",
      "Batch 3,300 of 9,631 Elased 0:21:12. Training loss: 4.097511891776866. Learning Rate: 8.2374222999223e-05\n",
      "Batch 3,350 of 9,631 Elased 0:21:31. Training loss: 4.09717851756224. Learning Rate: 8.235687923187923e-05\n",
      "Batch 3,400 of 9,631 Elased 0:21:51. Training loss: 4.098556423643056. Learning Rate: 8.233953546453546e-05\n",
      "Batch 3,450 of 9,631 Elased 0:22:10. Training loss: 4.100045281873233. Learning Rate: 8.232219169719169e-05\n",
      "Batch 3,500 of 9,631 Elased 0:22:30. Training loss: 4.09792609497479. Learning Rate: 8.230484792984793e-05\n",
      "Batch 3,550 of 9,631 Elased 0:22:49. Training loss: 4.097812459737482. Learning Rate: 8.228750416250416e-05\n",
      "Batch 3,600 of 9,631 Elased 0:23:09. Training loss: 4.099216742085086. Learning Rate: 8.227016039516041e-05\n",
      "Batch 3,650 of 9,631 Elased 0:23:28. Training loss: 4.103291977562317. Learning Rate: 8.225281662781664e-05\n",
      "Batch 3,700 of 9,631 Elased 0:23:47. Training loss: 4.102398147937413. Learning Rate: 8.223547286047287e-05\n",
      "Batch 3,750 of 9,631 Elased 0:24:06. Training loss: 4.100000033092499. Learning Rate: 8.22181290931291e-05\n",
      "Batch 3,800 of 9,631 Elased 0:24:26. Training loss: 4.097573821638759. Learning Rate: 8.220078532578534e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3,850 of 9,631 Elased 0:24:45. Training loss: 4.094150936851253. Learning Rate: 8.218344155844157e-05\n",
      "Batch 3,900 of 9,631 Elased 0:25:05. Training loss: 4.091620457875423. Learning Rate: 8.21660977910978e-05\n",
      "Batch 3,950 of 9,631 Elased 0:25:24. Training loss: 4.092215144815325. Learning Rate: 8.214875402375403e-05\n",
      "Batch 4,000 of 9,631 Elased 0:25:44. Training loss: 4.093045740693808. Learning Rate: 8.213141025641025e-05\n",
      "Batch 4,050 of 9,631 Elased 0:26:03. Training loss: 4.092073540363783. Learning Rate: 8.211406648906648e-05\n",
      "Batch 4,100 of 9,631 Elased 0:26:23. Training loss: 4.0908820826542085. Learning Rate: 8.209672272172273e-05\n",
      "Batch 4,150 of 9,631 Elased 0:26:42. Training loss: 4.0908690910167005. Learning Rate: 8.207937895437896e-05\n",
      "Batch 4,200 of 9,631 Elased 0:27:01. Training loss: 4.090260638310796. Learning Rate: 8.206203518703519e-05\n",
      "Batch 4,250 of 9,631 Elased 0:27:20. Training loss: 4.089904579358943. Learning Rate: 8.204469141969143e-05\n",
      "Batch 4,300 of 9,631 Elased 0:27:39. Training loss: 4.089871875214023. Learning Rate: 8.202734765234766e-05\n",
      "Batch 4,350 of 9,631 Elased 0:27:59. Training loss: 4.089546233675946. Learning Rate: 8.20100038850039e-05\n",
      "Batch 4,400 of 9,631 Elased 0:28:18. Training loss: 4.090779202824289. Learning Rate: 8.199266011766013e-05\n",
      "Batch 4,450 of 9,631 Elased 0:28:38. Training loss: 4.091467697941855. Learning Rate: 8.197531635031636e-05\n",
      "Batch 4,500 of 9,631 Elased 0:28:57. Training loss: 4.092557004160351. Learning Rate: 8.195797258297259e-05\n",
      "Batch 4,550 of 9,631 Elased 0:29:16. Training loss: 4.08992084469114. Learning Rate: 8.194062881562882e-05\n",
      "Batch 4,600 of 9,631 Elased 0:29:36. Training loss: 4.092168545645216. Learning Rate: 8.192328504828505e-05\n",
      "Batch 4,650 of 9,631 Elased 0:29:55. Training loss: 4.090289813703106. Learning Rate: 8.190594128094129e-05\n",
      "Batch 4,700 of 9,631 Elased 0:30:14. Training loss: 4.090813383777091. Learning Rate: 8.188859751359752e-05\n",
      "Batch 4,750 of 9,631 Elased 0:30:33. Training loss: 4.090256214267329. Learning Rate: 8.187125374625375e-05\n",
      "Batch 4,800 of 9,631 Elased 0:30:53. Training loss: 4.088258181934555. Learning Rate: 8.185390997890998e-05\n",
      "Batch 4,850 of 9,631 Elased 0:31:13. Training loss: 4.0866574285694. Learning Rate: 8.18365662115662e-05\n",
      "Batch 4,900 of 9,631 Elased 0:31:32. Training loss: 4.086536336212742. Learning Rate: 8.181922244422245e-05\n",
      "Batch 4,950 of 9,631 Elased 0:31:51. Training loss: 4.086206537087758. Learning Rate: 8.180187867687868e-05\n",
      "Batch 5,000 of 9,631 Elased 0:32:10. Training loss: 4.086886752438545. Learning Rate: 8.178453490953492e-05\n",
      "Batch 5,050 of 9,631 Elased 0:32:30. Training loss: 4.086817300838999. Learning Rate: 8.176719114219115e-05\n",
      "Batch 5,100 of 9,631 Elased 0:32:49. Training loss: 4.084160285066156. Learning Rate: 8.174984737484738e-05\n",
      "Batch 5,150 of 9,631 Elased 0:33:08. Training loss: 4.0850859248290945. Learning Rate: 8.173250360750361e-05\n",
      "Batch 5,200 of 9,631 Elased 0:33:28. Training loss: 4.084930008581051. Learning Rate: 8.171515984015985e-05\n",
      "Batch 5,250 of 9,631 Elased 0:33:47. Training loss: 4.085338659717923. Learning Rate: 8.169781607281608e-05\n",
      "Batch 5,300 of 9,631 Elased 0:34:06. Training loss: 4.085054382220754. Learning Rate: 8.168047230547231e-05\n",
      "Batch 5,350 of 9,631 Elased 0:34:25. Training loss: 4.08685630916435. Learning Rate: 8.166312853812854e-05\n",
      "Batch 5,400 of 9,631 Elased 0:34:44. Training loss: 4.086773416223349. Learning Rate: 8.164578477078477e-05\n",
      "Batch 5,450 of 9,631 Elased 0:35:03. Training loss: 4.086477268223369. Learning Rate: 8.1628441003441e-05\n",
      "Batch 5,500 of 9,631 Elased 0:35:23. Training loss: 4.086221004941247. Learning Rate: 8.161109723609724e-05\n",
      "Batch 5,550 of 9,631 Elased 0:35:42. Training loss: 4.086449604829152. Learning Rate: 8.159375346875347e-05\n",
      "Batch 5,600 of 9,631 Elased 0:36:01. Training loss: 4.0856269553516595. Learning Rate: 8.15764097014097e-05\n",
      "Batch 5,650 of 9,631 Elased 0:36:20. Training loss: 4.087223603556642. Learning Rate: 8.155906593406594e-05\n",
      "Batch 5,700 of 9,631 Elased 0:36:39. Training loss: 4.086247543723959. Learning Rate: 8.154172216672217e-05\n",
      "Batch 5,750 of 9,631 Elased 0:36:59. Training loss: 4.084902926382811. Learning Rate: 8.15243783993784e-05\n",
      "Batch 5,800 of 9,631 Elased 0:37:18. Training loss: 4.084924383142899. Learning Rate: 8.150703463203464e-05\n",
      "Batch 5,850 of 9,631 Elased 0:37:37. Training loss: 4.0850268582604885. Learning Rate: 8.148969086469087e-05\n",
      "Batch 5,900 of 9,631 Elased 0:37:57. Training loss: 4.084168518377563. Learning Rate: 8.14723470973471e-05\n",
      "Batch 5,950 of 9,631 Elased 0:38:16. Training loss: 4.083758430701344. Learning Rate: 8.145500333000333e-05\n",
      "Batch 6,000 of 9,631 Elased 0:38:35. Training loss: 4.08264112863938. Learning Rate: 8.143765956265956e-05\n",
      "Batch 6,050 of 9,631 Elased 0:38:54. Training loss: 4.082809670877851. Learning Rate: 8.14203157953158e-05\n",
      "Batch 6,100 of 9,631 Elased 0:39:14. Training loss: 4.083213825049947. Learning Rate: 8.140297202797203e-05\n",
      "Batch 6,150 of 9,631 Elased 0:39:33. Training loss: 4.082764253674484. Learning Rate: 8.138562826062826e-05\n",
      "Batch 6,200 of 9,631 Elased 0:39:52. Training loss: 4.080320138988957. Learning Rate: 8.136828449328449e-05\n",
      "Batch 6,250 of 9,631 Elased 0:40:12. Training loss: 4.0792929042243955. Learning Rate: 8.135094072594072e-05\n",
      "Batch 6,300 of 9,631 Elased 0:40:31. Training loss: 4.079078406473947. Learning Rate: 8.133359695859696e-05\n",
      "Batch 6,350 of 9,631 Elased 0:40:50. Training loss: 4.078125670481855. Learning Rate: 8.13162531912532e-05\n",
      "Batch 6,400 of 9,631 Elased 0:41:10. Training loss: 4.076753057111055. Learning Rate: 8.129890942390944e-05\n",
      "Batch 6,450 of 9,631 Elased 0:41:29. Training loss: 4.077951165705688. Learning Rate: 8.128156565656566e-05\n",
      "Batch 6,500 of 9,631 Elased 0:41:48. Training loss: 4.075577501938893. Learning Rate: 8.12642218892219e-05\n",
      "Batch 6,550 of 9,631 Elased 0:42:07. Training loss: 4.075457939537427. Learning Rate: 8.124687812187812e-05\n",
      "Batch 6,600 of 9,631 Elased 0:42:27. Training loss: 4.076303054112376. Learning Rate: 8.122953435453435e-05\n",
      "Batch 6,650 of 9,631 Elased 0:42:46. Training loss: 4.076110514429279. Learning Rate: 8.12121905871906e-05\n",
      "Batch 6,700 of 9,631 Elased 0:43:05. Training loss: 4.0772206381719505. Learning Rate: 8.119484681984682e-05\n",
      "Batch 6,750 of 9,631 Elased 0:43:24. Training loss: 4.0770474190358765. Learning Rate: 8.117750305250305e-05\n",
      "Batch 6,800 of 9,631 Elased 0:43:44. Training loss: 4.075689673826975. Learning Rate: 8.116015928515928e-05\n",
      "Batch 6,850 of 9,631 Elased 0:44:03. Training loss: 4.074617444389928. Learning Rate: 8.114281551781551e-05\n",
      "Batch 6,900 of 9,631 Elased 0:44:22. Training loss: 4.0744509724084885. Learning Rate: 8.112547175047176e-05\n",
      "Batch 6,950 of 9,631 Elased 0:44:41. Training loss: 4.074109992346318. Learning Rate: 8.110812798312798e-05\n",
      "Batch 7,000 of 9,631 Elased 0:45:01. Training loss: 4.072877020546368. Learning Rate: 8.109078421578423e-05\n",
      "Batch 7,050 of 9,631 Elased 0:45:20. Training loss: 4.07174190735986. Learning Rate: 8.107344044844046e-05\n",
      "Batch 7,100 of 9,631 Elased 0:45:39. Training loss: 4.071713798835244. Learning Rate: 8.105609668109669e-05\n",
      "Batch 7,150 of 9,631 Elased 0:45:59. Training loss: 4.071075654546697. Learning Rate: 8.103875291375292e-05\n",
      "Batch 7,200 of 9,631 Elased 0:46:18. Training loss: 4.07093170730604. Learning Rate: 8.102140914640916e-05\n",
      "Batch 7,250 of 9,631 Elased 0:46:37. Training loss: 4.069589625079057. Learning Rate: 8.100406537906539e-05\n",
      "Batch 7,300 of 9,631 Elased 0:46:56. Training loss: 4.069676776111942. Learning Rate: 8.098672161172162e-05\n",
      "Batch 7,350 of 9,631 Elased 0:47:16. Training loss: 4.068428269616601. Learning Rate: 8.096937784437785e-05\n",
      "Batch 7,400 of 9,631 Elased 0:47:35. Training loss: 4.0666422126905335. Learning Rate: 8.095203407703408e-05\n",
      "Batch 7,450 of 9,631 Elased 0:47:54. Training loss: 4.06645433203486. Learning Rate: 8.09346903096903e-05\n",
      "Batch 7,500 of 9,631 Elased 0:48:13. Training loss: 4.06502044672966. Learning Rate: 8.091734654234655e-05\n",
      "Batch 7,550 of 9,631 Elased 0:48:33. Training loss: 4.064496476113401. Learning Rate: 8.090000277500278e-05\n",
      "Batch 7,600 of 9,631 Elased 0:48:52. Training loss: 4.064751187233548. Learning Rate: 8.0882659007659e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7,650 of 9,631 Elased 0:49:11. Training loss: 4.064650691085392. Learning Rate: 8.086531524031525e-05\n",
      "Batch 7,700 of 9,631 Elased 0:49:31. Training loss: 4.06297746557694. Learning Rate: 8.084797147297148e-05\n",
      "Batch 7,750 of 9,631 Elased 0:49:50. Training loss: 4.060328922133292. Learning Rate: 8.083062770562772e-05\n",
      "Batch 7,800 of 9,631 Elased 0:50:09. Training loss: 4.060509868753262. Learning Rate: 8.081328393828395e-05\n",
      "Batch 7,850 of 9,631 Elased 0:50:29. Training loss: 4.060695309988253. Learning Rate: 8.079594017094018e-05\n",
      "Batch 7,900 of 9,631 Elased 0:50:48. Training loss: 4.05870810368393. Learning Rate: 8.077859640359641e-05\n",
      "Batch 7,950 of 9,631 Elased 0:51:07. Training loss: 4.060164743024598. Learning Rate: 8.076125263625264e-05\n",
      "Batch 8,000 of 9,631 Elased 0:51:27. Training loss: 4.059908874198794. Learning Rate: 8.074390886890887e-05\n",
      "Batch 8,050 of 9,631 Elased 0:51:46. Training loss: 4.061252252759401. Learning Rate: 8.072656510156511e-05\n",
      "Batch 8,100 of 9,631 Elased 0:52:05. Training loss: 4.061400389715478. Learning Rate: 8.070922133422134e-05\n",
      "Batch 8,150 of 9,631 Elased 0:52:25. Training loss: 4.062375863505287. Learning Rate: 8.069187756687757e-05\n",
      "Batch 8,200 of 9,631 Elased 0:52:44. Training loss: 4.061507382494647. Learning Rate: 8.06745337995338e-05\n",
      "Batch 8,250 of 9,631 Elased 0:53:03. Training loss: 4.062140586202795. Learning Rate: 8.065719003219003e-05\n",
      "Batch 8,300 of 9,631 Elased 0:53:22. Training loss: 4.062113643338881. Learning Rate: 8.063984626484627e-05\n",
      "Batch 8,350 of 9,631 Elased 0:53:41. Training loss: 4.061747880852865. Learning Rate: 8.06225024975025e-05\n",
      "Batch 8,400 of 9,631 Elased 0:54:01. Training loss: 4.061125592035907. Learning Rate: 8.060515873015874e-05\n",
      "Batch 8,450 of 9,631 Elased 0:54:20. Training loss: 4.060274725143726. Learning Rate: 8.058781496281497e-05\n",
      "Batch 8,500 of 9,631 Elased 0:54:40. Training loss: 4.059517810779459. Learning Rate: 8.05704711954712e-05\n",
      "Batch 8,550 of 9,631 Elased 0:54:59. Training loss: 4.058926995469813. Learning Rate: 8.055312742812743e-05\n",
      "Batch 8,600 of 9,631 Elased 0:55:19. Training loss: 4.059336493444997. Learning Rate: 8.053578366078366e-05\n",
      "Batch 8,650 of 9,631 Elased 0:55:38. Training loss: 4.057902268109294. Learning Rate: 8.05184398934399e-05\n",
      "Batch 8,700 of 9,631 Elased 0:55:57. Training loss: 4.057619802088573. Learning Rate: 8.050109612609613e-05\n",
      "Batch 8,750 of 9,631 Elased 0:56:16. Training loss: 4.057123430047716. Learning Rate: 8.048375235875236e-05\n",
      "Batch 8,800 of 9,631 Elased 0:56:36. Training loss: 4.056517195986076. Learning Rate: 8.046640859140859e-05\n",
      "Batch 8,850 of 9,631 Elased 0:56:55. Training loss: 4.056042266067139. Learning Rate: 8.044906482406482e-05\n",
      "Batch 8,900 of 9,631 Elased 0:57:14. Training loss: 4.056314175678104. Learning Rate: 8.043172105672106e-05\n",
      "Batch 8,950 of 9,631 Elased 0:57:34. Training loss: 4.055536636307253. Learning Rate: 8.041437728937729e-05\n",
      "Batch 9,000 of 9,631 Elased 0:57:53. Training loss: 4.053867774552769. Learning Rate: 8.039703352203352e-05\n",
      "Batch 9,050 of 9,631 Elased 0:58:12. Training loss: 4.05451947927475. Learning Rate: 8.037968975468976e-05\n",
      "Batch 9,100 of 9,631 Elased 0:58:31. Training loss: 4.054129280533109. Learning Rate: 8.036234598734599e-05\n",
      "Batch 9,150 of 9,631 Elased 0:58:51. Training loss: 4.054091345065278. Learning Rate: 8.034500222000222e-05\n",
      "Batch 9,200 of 9,631 Elased 0:59:10. Training loss: 4.055059668538363. Learning Rate: 8.032765845265846e-05\n",
      "Batch 9,250 of 9,631 Elased 0:59:29. Training loss: 4.054438229444864. Learning Rate: 8.03103146853147e-05\n",
      "Batch 9,300 of 9,631 Elased 0:59:48. Training loss: 4.054797172559205. Learning Rate: 8.029297091797092e-05\n",
      "Batch 9,350 of 9,631 Elased 1:00:08. Training loss: 4.054581180600559. Learning Rate: 8.027562715062715e-05\n",
      "Batch 9,400 of 9,631 Elased 1:00:27. Training loss: 4.05458443092539. Learning Rate: 8.025828338328338e-05\n",
      "Batch 9,450 of 9,631 Elased 1:00:47. Training loss: 4.053888874949601. Learning Rate: 8.024093961593961e-05\n",
      "Batch 9,500 of 9,631 Elased 1:01:06. Training loss: 4.053642006058442. Learning Rate: 8.022359584859585e-05\n",
      "Batch 9,550 of 9,631 Elased 1:01:26. Training loss: 4.052915076747615. Learning Rate: 8.020625208125208e-05\n",
      "Batch 9,600 of 9,631 Elased 1:01:45. Training loss: 4.053308301133414. Learning Rate: 8.018890831390831e-05\n",
      "\n",
      "\n",
      "  Average training loss: 4.05\n",
      "  Training epcoh took: 1:01:57\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b69ab15b18476bb152e3ec90d24c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=67.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Validation Loss: 3.96\n",
      "  Validation took: 0:00:39\n",
      "\n",
      "======== Epoch 7 / 30 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7420f5f695bc4662a85a654b5f0c80c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch    50 of 9,631 Elased 0:00:20. Training loss: 3.998266172409058. Learning Rate: 8.01608114108114e-05\n",
      "Batch   100 of 9,631 Elased 0:00:39. Training loss: 3.9597826027870178. Learning Rate: 8.014346764346765e-05\n",
      "Batch   150 of 9,631 Elased 0:00:58. Training loss: 3.9625813086827595. Learning Rate: 8.012612387612389e-05\n",
      "Batch   200 of 9,631 Elased 0:01:17. Training loss: 4.029973777532578. Learning Rate: 8.010878010878012e-05\n",
      "Batch   250 of 9,631 Elased 0:01:36. Training loss: 3.9754755821228027. Learning Rate: 8.009143634143635e-05\n",
      "Batch   300 of 9,631 Elased 0:01:55. Training loss: 3.962911765575409. Learning Rate: 8.007409257409258e-05\n",
      "Batch   350 of 9,631 Elased 0:02:14. Training loss: 3.9543226766586304. Learning Rate: 8.005674880674882e-05\n",
      "Batch   400 of 9,631 Elased 0:02:34. Training loss: 3.973767183423042. Learning Rate: 8.003940503940505e-05\n",
      "Batch   450 of 9,631 Elased 0:02:53. Training loss: 3.968700665367974. Learning Rate: 8.002206127206128e-05\n",
      "Batch   500 of 9,631 Elased 0:03:12. Training loss: 3.97535445356369. Learning Rate: 8.000471750471751e-05\n",
      "Batch   550 of 9,631 Elased 0:03:32. Training loss: 3.963243322805925. Learning Rate: 7.998737373737374e-05\n",
      "Batch   600 of 9,631 Elased 0:03:51. Training loss: 3.9697694190343222. Learning Rate: 7.997002997002997e-05\n",
      "Batch   650 of 9,631 Elased 0:04:10. Training loss: 3.9708480431483344. Learning Rate: 7.995268620268621e-05\n",
      "Batch   700 of 9,631 Elased 0:04:29. Training loss: 3.968568208217621. Learning Rate: 7.993534243534244e-05\n",
      "Batch   750 of 9,631 Elased 0:04:48. Training loss: 3.956099714279175. Learning Rate: 7.991799866799867e-05\n",
      "Batch   800 of 9,631 Elased 0:05:08. Training loss: 3.9556402918696403. Learning Rate: 7.990065490065491e-05\n",
      "Batch   850 of 9,631 Elased 0:05:27. Training loss: 3.946494117905112. Learning Rate: 7.988331113331114e-05\n",
      "Batch   900 of 9,631 Elased 0:05:46. Training loss: 3.949011816183726. Learning Rate: 7.986596736596737e-05\n",
      "Batch   950 of 9,631 Elased 0:06:06. Training loss: 3.9454524682697496. Learning Rate: 7.984862359862361e-05\n",
      "Batch 1,000 of 9,631 Elased 0:06:25. Training loss: 3.94548490357399. Learning Rate: 7.983127983127984e-05\n",
      "Batch 1,050 of 9,631 Elased 0:06:44. Training loss: 3.943502853711446. Learning Rate: 7.981393606393607e-05\n",
      "Batch 1,100 of 9,631 Elased 0:07:03. Training loss: 3.9467718529701235. Learning Rate: 7.97965922965923e-05\n",
      "Batch 1,150 of 9,631 Elased 0:07:22. Training loss: 3.9463972887785537. Learning Rate: 7.977924852924853e-05\n",
      "Batch 1,200 of 9,631 Elased 0:07:41. Training loss: 3.948219070037206. Learning Rate: 7.976190476190477e-05\n",
      "Batch 1,250 of 9,631 Elased 0:08:00. Training loss: 3.9506837530136107. Learning Rate: 7.9744560994561e-05\n",
      "Batch 1,300 of 9,631 Elased 0:08:20. Training loss: 3.950816502387707. Learning Rate: 7.972721722721723e-05\n",
      "Batch 1,350 of 9,631 Elased 0:08:39. Training loss: 3.949435097553112. Learning Rate: 7.970987345987346e-05\n",
      "Batch 1,400 of 9,631 Elased 0:08:58. Training loss: 3.9479473403521945. Learning Rate: 7.969252969252969e-05\n",
      "Batch 1,450 of 9,631 Elased 0:09:18. Training loss: 3.94446729281853. Learning Rate: 7.967518592518593e-05\n",
      "Batch 1,500 of 9,631 Elased 0:09:37. Training loss: 3.9406818520228066. Learning Rate: 7.965784215784216e-05\n",
      "Batch 1,550 of 9,631 Elased 0:09:57. Training loss: 3.939303204936366. Learning Rate: 7.96404983904984e-05\n",
      "Batch 1,600 of 9,631 Elased 0:10:16. Training loss: 3.944754206687212. Learning Rate: 7.962315462315463e-05\n",
      "Batch 1,650 of 9,631 Elased 0:10:35. Training loss: 3.9504334033619273. Learning Rate: 7.960581085581086e-05\n",
      "Batch 1,700 of 9,631 Elased 0:10:54. Training loss: 3.947175097185023. Learning Rate: 7.958846708846709e-05\n",
      "Batch 1,750 of 9,631 Elased 0:11:13. Training loss: 3.9472439947128297. Learning Rate: 7.957112332112332e-05\n",
      "Batch 1,800 of 9,631 Elased 0:11:32. Training loss: 3.9456098871760896. Learning Rate: 7.955377955377956e-05\n",
      "Batch 1,850 of 9,631 Elased 0:11:52. Training loss: 3.9479959883561007. Learning Rate: 7.953643578643579e-05\n",
      "Batch 1,900 of 9,631 Elased 0:12:11. Training loss: 3.944800114380686. Learning Rate: 7.951909201909202e-05\n",
      "Batch 1,950 of 9,631 Elased 0:12:30. Training loss: 3.940992781321208. Learning Rate: 7.950174825174825e-05\n",
      "Batch 2,000 of 9,631 Elased 0:12:49. Training loss: 3.936137597203255. Learning Rate: 7.948440448440448e-05\n",
      "Batch 2,050 of 9,631 Elased 0:13:09. Training loss: 3.9365873335628976. Learning Rate: 7.946706071706072e-05\n",
      "Batch 2,100 of 9,631 Elased 0:13:28. Training loss: 3.939505372501555. Learning Rate: 7.944971694971695e-05\n",
      "Batch 2,150 of 9,631 Elased 0:13:47. Training loss: 3.9409974575042725. Learning Rate: 7.943237318237318e-05\n",
      "Batch 2,200 of 9,631 Elased 0:14:07. Training loss: 3.9413779561086133. Learning Rate: 7.941502941502942e-05\n",
      "Batch 2,250 of 9,631 Elased 0:14:26. Training loss: 3.9390173264609443. Learning Rate: 7.939768564768565e-05\n",
      "Batch 2,300 of 9,631 Elased 0:14:46. Training loss: 3.940592982147051. Learning Rate: 7.938034188034188e-05\n",
      "Batch 2,350 of 9,631 Elased 0:15:05. Training loss: 3.937716646498822. Learning Rate: 7.936299811299813e-05\n",
      "Batch 2,400 of 9,631 Elased 0:15:25. Training loss: 3.937944993476073. Learning Rate: 7.934565434565435e-05\n",
      "Batch 2,450 of 9,631 Elased 0:15:44. Training loss: 3.934640640433954. Learning Rate: 7.932831057831058e-05\n",
      "Batch 2,500 of 9,631 Elased 0:16:04. Training loss: 3.931757497215271. Learning Rate: 7.931096681096681e-05\n",
      "Batch 2,550 of 9,631 Elased 0:16:23. Training loss: 3.9376288693558936. Learning Rate: 7.929362304362304e-05\n",
      "Batch 2,600 of 9,631 Elased 0:16:43. Training loss: 3.9397381919163923. Learning Rate: 7.927627927627927e-05\n",
      "Batch 2,650 of 9,631 Elased 0:17:02. Training loss: 3.940984548622707. Learning Rate: 7.925893550893551e-05\n",
      "Batch 2,700 of 9,631 Elased 0:17:21. Training loss: 3.944000894140314. Learning Rate: 7.924159174159174e-05\n",
      "Batch 2,750 of 9,631 Elased 0:17:41. Training loss: 3.940486103664745. Learning Rate: 7.922424797424797e-05\n",
      "Batch 2,800 of 9,631 Elased 0:18:00. Training loss: 3.941664779526847. Learning Rate: 7.92069042069042e-05\n",
      "Batch 2,850 of 9,631 Elased 0:18:19. Training loss: 3.944150062443917. Learning Rate: 7.918956043956045e-05\n",
      "Batch 2,900 of 9,631 Elased 0:18:39. Training loss: 3.944236765647757. Learning Rate: 7.917221667221669e-05\n",
      "Batch 2,950 of 9,631 Elased 0:18:58. Training loss: 3.9433485821546133. Learning Rate: 7.915487290487292e-05\n",
      "Batch 3,000 of 9,631 Elased 0:19:17. Training loss: 3.9462610092163084. Learning Rate: 7.913752913752915e-05\n",
      "Batch 3,050 of 9,631 Elased 0:19:37. Training loss: 3.947112696522572. Learning Rate: 7.912018537018538e-05\n",
      "Batch 3,100 of 9,631 Elased 0:19:57. Training loss: 3.9473916096841135. Learning Rate: 7.91028416028416e-05\n",
      "Batch 3,150 of 9,631 Elased 0:20:16. Training loss: 3.9428531022298903. Learning Rate: 7.908549783549783e-05\n",
      "Batch 3,200 of 9,631 Elased 0:20:35. Training loss: 3.9447224581986666. Learning Rate: 7.906815406815408e-05\n",
      "Batch 3,250 of 9,631 Elased 0:20:54. Training loss: 3.9441541508161104. Learning Rate: 7.90508103008103e-05\n",
      "Batch 3,300 of 9,631 Elased 0:21:14. Training loss: 3.946896630344969. Learning Rate: 7.903346653346654e-05\n",
      "Batch 3,350 of 9,631 Elased 0:21:33. Training loss: 3.946887359974989. Learning Rate: 7.901612276612277e-05\n",
      "Batch 3,400 of 9,631 Elased 0:21:53. Training loss: 3.9472329217546127. Learning Rate: 7.8998778998779e-05\n",
      "Batch 3,450 of 9,631 Elased 0:22:12. Training loss: 3.948150359996851. Learning Rate: 7.898143523143522e-05\n",
      "Batch 3,500 of 9,631 Elased 0:22:32. Training loss: 3.9466039171218874. Learning Rate: 7.896409146409147e-05\n",
      "Batch 3,550 of 9,631 Elased 0:22:51. Training loss: 3.9467909312584033. Learning Rate: 7.894674769674771e-05\n",
      "Batch 3,600 of 9,631 Elased 0:23:10. Training loss: 3.947951425247722. Learning Rate: 7.892940392940394e-05\n",
      "Batch 3,650 of 9,631 Elased 0:23:30. Training loss: 3.9517655038180415. Learning Rate: 7.891206016206017e-05\n",
      "Batch 3,700 of 9,631 Elased 0:23:49. Training loss: 3.950437766088022. Learning Rate: 7.88947163947164e-05\n",
      "Batch 3,750 of 9,631 Elased 0:24:08. Training loss: 3.9484686614990236. Learning Rate: 7.887737262737264e-05\n",
      "Batch 3,800 of 9,631 Elased 0:24:27. Training loss: 3.9463921239501554. Learning Rate: 7.886002886002887e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3,850 of 9,631 Elased 0:24:47. Training loss: 3.943094689443514. Learning Rate: 7.88426850926851e-05\n",
      "Batch 3,900 of 9,631 Elased 0:25:06. Training loss: 3.940560359954834. Learning Rate: 7.882534132534133e-05\n",
      "Batch 3,950 of 9,631 Elased 0:25:25. Training loss: 3.941094601124148. Learning Rate: 7.880799755799756e-05\n",
      "Batch 4,000 of 9,631 Elased 0:25:44. Training loss: 3.941433801949024. Learning Rate: 7.879065379065379e-05\n",
      "Batch 4,050 of 9,631 Elased 0:26:03. Training loss: 3.940648565939915. Learning Rate: 7.877331002331003e-05\n",
      "Batch 4,100 of 9,631 Elased 0:26:22. Training loss: 3.939374561193513. Learning Rate: 7.875596625596626e-05\n",
      "Batch 4,150 of 9,631 Elased 0:26:42. Training loss: 3.938688110615834. Learning Rate: 7.873862248862249e-05\n",
      "Batch 4,200 of 9,631 Elased 0:27:02. Training loss: 3.9385432424431754. Learning Rate: 7.872127872127873e-05\n",
      "Batch 4,250 of 9,631 Elased 0:27:21. Training loss: 3.9383423804675832. Learning Rate: 7.870393495393496e-05\n",
      "Batch 4,300 of 9,631 Elased 0:27:40. Training loss: 3.937700878963914. Learning Rate: 7.868659118659119e-05\n",
      "Batch 4,350 of 9,631 Elased 0:27:59. Training loss: 3.937157424236166. Learning Rate: 7.866924741924743e-05\n",
      "Batch 4,400 of 9,631 Elased 0:28:19. Training loss: 3.9382671011036092. Learning Rate: 7.865190365190366e-05\n",
      "Batch 4,450 of 9,631 Elased 0:28:38. Training loss: 3.9387645148159414. Learning Rate: 7.863455988455989e-05\n",
      "Batch 4,500 of 9,631 Elased 0:28:57. Training loss: 3.939639673921797. Learning Rate: 7.861721611721612e-05\n",
      "Batch 4,550 of 9,631 Elased 0:29:17. Training loss: 3.9373886449520406. Learning Rate: 7.859987234987235e-05\n",
      "Batch 4,600 of 9,631 Elased 0:29:36. Training loss: 3.9390705019494763. Learning Rate: 7.858252858252859e-05\n",
      "Batch 4,650 of 9,631 Elased 0:29:55. Training loss: 3.9373747652833178. Learning Rate: 7.856518481518482e-05\n",
      "Batch 4,700 of 9,631 Elased 0:30:14. Training loss: 3.9377011350875204. Learning Rate: 7.854784104784105e-05\n",
      "Batch 4,750 of 9,631 Elased 0:30:34. Training loss: 3.937346464659038. Learning Rate: 7.853049728049728e-05\n",
      "Batch 4,800 of 9,631 Elased 0:30:53. Training loss: 3.935838304857413. Learning Rate: 7.851315351315351e-05\n",
      "Batch 4,850 of 9,631 Elased 0:31:12. Training loss: 3.9341878754330666. Learning Rate: 7.849580974580975e-05\n",
      "Batch 4,900 of 9,631 Elased 0:31:32. Training loss: 3.9343983152934485. Learning Rate: 7.847846597846598e-05\n",
      "Batch 4,950 of 9,631 Elased 0:31:51. Training loss: 3.934514869102324. Learning Rate: 7.846112221112222e-05\n",
      "Batch 5,000 of 9,631 Elased 0:32:10. Training loss: 3.935728313922882. Learning Rate: 7.844377844377845e-05\n",
      "Batch 5,050 of 9,631 Elased 0:32:29. Training loss: 3.9358588674280903. Learning Rate: 7.842643467643468e-05\n",
      "Batch 5,100 of 9,631 Elased 0:32:49. Training loss: 3.933560282015333. Learning Rate: 7.840909090909091e-05\n",
      "Batch 5,150 of 9,631 Elased 0:33:08. Training loss: 3.9339351791317023. Learning Rate: 7.839174714174714e-05\n",
      "Batch 5,200 of 9,631 Elased 0:33:27. Training loss: 3.933655627438655. Learning Rate: 7.837440337440338e-05\n",
      "Batch 5,250 of 9,631 Elased 0:33:46. Training loss: 3.933417117958977. Learning Rate: 7.835705960705961e-05\n",
      "Batch 5,300 of 9,631 Elased 0:34:06. Training loss: 3.932331192785839. Learning Rate: 7.833971583971584e-05\n",
      "Batch 5,350 of 9,631 Elased 0:34:26. Training loss: 3.9341682786807834. Learning Rate: 7.832237207237207e-05\n",
      "Batch 5,400 of 9,631 Elased 0:34:45. Training loss: 3.9339913979503844. Learning Rate: 7.83050283050283e-05\n",
      "Batch 5,450 of 9,631 Elased 0:35:04. Training loss: 3.9342881725250036. Learning Rate: 7.828768453768454e-05\n",
      "Batch 5,500 of 9,631 Elased 0:35:23. Training loss: 3.934066929101944. Learning Rate: 7.827034077034077e-05\n",
      "Batch 5,550 of 9,631 Elased 0:35:42. Training loss: 3.9346882962106586. Learning Rate: 7.8252997002997e-05\n",
      "Batch 5,600 of 9,631 Elased 0:36:02. Training loss: 3.9336995787067073. Learning Rate: 7.823565323565324e-05\n",
      "Batch 5,650 of 9,631 Elased 0:36:21. Training loss: 3.935623485999825. Learning Rate: 7.821830946830947e-05\n",
      "Batch 5,700 of 9,631 Elased 0:36:41. Training loss: 3.934873996889382. Learning Rate: 7.82009657009657e-05\n",
      "Batch 5,750 of 9,631 Elased 0:37:00. Training loss: 3.9339671770800715. Learning Rate: 7.818362193362195e-05\n",
      "Batch 5,800 of 9,631 Elased 0:37:19. Training loss: 3.933971733656423. Learning Rate: 7.816627816627818e-05\n",
      "Batch 5,850 of 9,631 Elased 0:37:38. Training loss: 3.9340397278671593. Learning Rate: 7.81489343989344e-05\n",
      "Batch 5,900 of 9,631 Elased 0:37:58. Training loss: 3.9334472574419896. Learning Rate: 7.813159063159063e-05\n",
      "Batch 5,950 of 9,631 Elased 0:38:17. Training loss: 3.933032427334986. Learning Rate: 7.811424686424686e-05\n",
      "Batch 6,000 of 9,631 Elased 0:38:36. Training loss: 3.9320104810198147. Learning Rate: 7.809690309690309e-05\n",
      "Batch 6,050 of 9,631 Elased 0:38:55. Training loss: 3.9321061571176386. Learning Rate: 7.807955932955934e-05\n",
      "Batch 6,100 of 9,631 Elased 0:39:15. Training loss: 3.9319721979195954. Learning Rate: 7.806221556221556e-05\n",
      "Batch 6,150 of 9,631 Elased 0:39:34. Training loss: 3.931179559967382. Learning Rate: 7.80448717948718e-05\n",
      "Batch 6,200 of 9,631 Elased 0:39:54. Training loss: 3.928853749786654. Learning Rate: 7.802752802752802e-05\n",
      "Batch 6,250 of 9,631 Elased 0:40:13. Training loss: 3.928044060382843. Learning Rate: 7.801018426018427e-05\n",
      "Batch 6,300 of 9,631 Elased 0:40:32. Training loss: 3.9283588080368346. Learning Rate: 7.799284049284051e-05\n",
      "Batch 6,350 of 9,631 Elased 0:40:52. Training loss: 3.9272845907098666. Learning Rate: 7.797549672549674e-05\n",
      "Batch 6,400 of 9,631 Elased 0:41:11. Training loss: 3.926154033783823. Learning Rate: 7.795815295815297e-05\n",
      "Batch 6,450 of 9,631 Elased 0:41:30. Training loss: 3.927765250963758. Learning Rate: 7.79408091908092e-05\n",
      "Batch 6,500 of 9,631 Elased 0:41:49. Training loss: 3.925609451679083. Learning Rate: 7.792346542346543e-05\n",
      "Batch 6,550 of 9,631 Elased 0:42:09. Training loss: 3.9259316582534147. Learning Rate: 7.790612165612166e-05\n",
      "Batch 6,600 of 9,631 Elased 0:42:28. Training loss: 3.926548585367925. Learning Rate: 7.78887778887779e-05\n",
      "Batch 6,650 of 9,631 Elased 0:42:47. Training loss: 3.926360540407941. Learning Rate: 7.787143412143413e-05\n",
      "Batch 6,700 of 9,631 Elased 0:43:06. Training loss: 3.927092268769421. Learning Rate: 7.785409035409036e-05\n",
      "Batch 6,750 of 9,631 Elased 0:43:26. Training loss: 3.9267950629304957. Learning Rate: 7.783674658674659e-05\n",
      "Batch 6,800 of 9,631 Elased 0:43:45. Training loss: 3.9257972500780047. Learning Rate: 7.781940281940281e-05\n",
      "Batch 6,850 of 9,631 Elased 0:44:04. Training loss: 3.9249090407886644. Learning Rate: 7.780205905205904e-05\n",
      "Batch 6,900 of 9,631 Elased 0:44:24. Training loss: 3.925057958882788. Learning Rate: 7.778471528471529e-05\n",
      "Batch 6,950 of 9,631 Elased 0:44:43. Training loss: 3.924536742563728. Learning Rate: 7.776737151737153e-05\n",
      "Batch 7,000 of 9,631 Elased 0:45:02. Training loss: 3.923790964654514. Learning Rate: 7.775002775002776e-05\n",
      "Batch 7,050 of 9,631 Elased 0:45:21. Training loss: 3.922590741417932. Learning Rate: 7.773268398268399e-05\n",
      "Batch 7,100 of 9,631 Elased 0:45:41. Training loss: 3.922444998865396. Learning Rate: 7.771534021534022e-05\n",
      "Batch 7,150 of 9,631 Elased 0:46:00. Training loss: 3.9219792136779197. Learning Rate: 7.769799644799645e-05\n",
      "Batch 7,200 of 9,631 Elased 0:46:19. Training loss: 3.921888390564256. Learning Rate: 7.768065268065269e-05\n",
      "Batch 7,250 of 9,631 Elased 0:46:39. Training loss: 3.920316952721826. Learning Rate: 7.766330891330892e-05\n",
      "Batch 7,300 of 9,631 Elased 0:46:58. Training loss: 3.920543966766906. Learning Rate: 7.764596514596515e-05\n",
      "Batch 7,350 of 9,631 Elased 0:47:17. Training loss: 3.9196917440939925. Learning Rate: 7.762862137862138e-05\n",
      "Batch 7,400 of 9,631 Elased 0:47:37. Training loss: 3.9181316945842792. Learning Rate: 7.761127761127761e-05\n",
      "Batch 7,450 of 9,631 Elased 0:47:57. Training loss: 3.9176661851262087. Learning Rate: 7.759393384393385e-05\n",
      "Batch 7,500 of 9,631 Elased 0:48:15. Training loss: 3.9161859739462535. Learning Rate: 7.757659007659008e-05\n",
      "Batch 7,550 of 9,631 Elased 0:48:35. Training loss: 3.9158945524929374. Learning Rate: 7.755924630924631e-05\n",
      "Batch 7,600 of 9,631 Elased 0:48:54. Training loss: 3.9164083571967327. Learning Rate: 7.754190254190255e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7,650 of 9,631 Elased 0:49:13. Training loss: 3.9164503886808757. Learning Rate: 7.752455877455878e-05\n",
      "Batch 7,700 of 9,631 Elased 0:49:32. Training loss: 3.9149166592839477. Learning Rate: 7.750721500721501e-05\n",
      "Batch 7,750 of 9,631 Elased 0:49:51. Training loss: 3.9126150303656053. Learning Rate: 7.748987123987125e-05\n",
      "Batch 7,800 of 9,631 Elased 0:50:11. Training loss: 3.9129457180469465. Learning Rate: 7.747252747252748e-05\n",
      "Batch 7,850 of 9,631 Elased 0:50:30. Training loss: 3.9133021818908156. Learning Rate: 7.745518370518371e-05\n",
      "Batch 7,900 of 9,631 Elased 0:50:49. Training loss: 3.911623830236966. Learning Rate: 7.743783993783994e-05\n",
      "Batch 7,950 of 9,631 Elased 0:51:09. Training loss: 3.913009027520066. Learning Rate: 7.742049617049617e-05\n",
      "Batch 8,000 of 9,631 Elased 0:51:28. Training loss: 3.9124039167016744. Learning Rate: 7.74031524031524e-05\n",
      "Batch 8,050 of 9,631 Elased 0:51:47. Training loss: 3.9133655511518444. Learning Rate: 7.738580863580864e-05\n",
      "Batch 8,100 of 9,631 Elased 0:52:06. Training loss: 3.913296215195715. Learning Rate: 7.736846486846487e-05\n",
      "Batch 8,150 of 9,631 Elased 0:52:26. Training loss: 3.9140820446921274. Learning Rate: 7.73511211011211e-05\n",
      "Batch 8,200 of 9,631 Elased 0:52:45. Training loss: 3.9134149940275567. Learning Rate: 7.733377733377733e-05\n",
      "Batch 8,250 of 9,631 Elased 0:53:04. Training loss: 3.913839417443131. Learning Rate: 7.731643356643357e-05\n",
      "Batch 8,300 of 9,631 Elased 0:53:23. Training loss: 3.9138469315867828. Learning Rate: 7.72990897990898e-05\n",
      "Batch 8,350 of 9,631 Elased 0:53:43. Training loss: 3.9137040164085204. Learning Rate: 7.728174603174604e-05\n",
      "Batch 8,400 of 9,631 Elased 0:54:02. Training loss: 3.9129192746395156. Learning Rate: 7.726440226440227e-05\n",
      "Batch 8,450 of 9,631 Elased 0:54:21. Training loss: 3.9120582279933274. Learning Rate: 7.72470584970585e-05\n",
      "Batch 8,500 of 9,631 Elased 0:54:41. Training loss: 3.91119457969946. Learning Rate: 7.722971472971473e-05\n",
      "Batch 8,550 of 9,631 Elased 0:55:00. Training loss: 3.9106491948428905. Learning Rate: 7.721237096237096e-05\n",
      "Batch 8,600 of 9,631 Elased 0:55:19. Training loss: 3.911020963954371. Learning Rate: 7.71950271950272e-05\n",
      "Batch 8,650 of 9,631 Elased 0:55:38. Training loss: 3.909666423397946. Learning Rate: 7.717768342768343e-05\n",
      "Batch 8,700 of 9,631 Elased 0:55:58. Training loss: 3.909554600510104. Learning Rate: 7.716033966033966e-05\n",
      "Batch 8,750 of 9,631 Elased 0:56:18. Training loss: 3.909263191808973. Learning Rate: 7.714299589299589e-05\n",
      "Batch 8,800 of 9,631 Elased 0:56:37. Training loss: 3.9088135951080107. Learning Rate: 7.712565212565212e-05\n",
      "Batch 8,850 of 9,631 Elased 0:56:56. Training loss: 3.908335008015067. Learning Rate: 7.710830835830835e-05\n",
      "Batch 8,900 of 9,631 Elased 0:57:15. Training loss: 3.9088387664382376. Learning Rate: 7.70909645909646e-05\n",
      "Batch 8,950 of 9,631 Elased 0:57:35. Training loss: 3.908204921277542. Learning Rate: 7.707362082362082e-05\n",
      "Batch 9,000 of 9,631 Elased 0:57:54. Training loss: 3.906746831006474. Learning Rate: 7.705627705627707e-05\n",
      "Batch 9,050 of 9,631 Elased 0:58:14. Training loss: 3.9073217053044567. Learning Rate: 7.70389332889333e-05\n",
      "Batch 9,100 of 9,631 Elased 0:58:33. Training loss: 3.906820213781608. Learning Rate: 7.702158952158952e-05\n",
      "Batch 9,150 of 9,631 Elased 0:58:52. Training loss: 3.9067659363329734. Learning Rate: 7.700424575424577e-05\n",
      "Batch 9,200 of 9,631 Elased 0:59:11. Training loss: 3.907669471748497. Learning Rate: 7.6986901986902e-05\n",
      "Batch 9,250 of 9,631 Elased 0:59:31. Training loss: 3.9070796691404808. Learning Rate: 7.696955821955823e-05\n",
      "Batch 9,300 of 9,631 Elased 0:59:50. Training loss: 3.9074213957658377. Learning Rate: 7.695221445221445e-05\n",
      "Batch 9,350 of 9,631 Elased 1:00:09. Training loss: 3.907123931066238. Learning Rate: 7.693487068487068e-05\n",
      "Batch 9,400 of 9,631 Elased 1:00:29. Training loss: 3.9071760837955676. Learning Rate: 7.691752691752691e-05\n",
      "Batch 9,450 of 9,631 Elased 1:00:48. Training loss: 3.90653885219463. Learning Rate: 7.690018315018316e-05\n",
      "Batch 9,500 of 9,631 Elased 1:01:07. Training loss: 3.906014661952069. Learning Rate: 7.688283938283939e-05\n",
      "Batch 9,550 of 9,631 Elased 1:01:26. Training loss: 3.9051938561119957. Learning Rate: 7.686549561549561e-05\n",
      "Batch 9,600 of 9,631 Elased 1:01:46. Training loss: 3.905686418674886. Learning Rate: 7.684815184815184e-05\n",
      "\n",
      "\n",
      "  Average training loss: 3.91\n",
      "  Training epcoh took: 1:01:57\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcba021194b24ba1a4c3e56f6d86d3e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=67.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Validation Loss: 3.85\n",
      "  Validation took: 0:00:39\n",
      "\n",
      "======== Epoch 8 / 30 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a18dc6bc977e4cd985eb76e6539e4a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch    50 of 9,631 Elased 0:00:19. Training loss: 3.8649965476989747. Learning Rate: 7.682005494505495e-05\n",
      "Batch   100 of 9,631 Elased 0:00:39. Training loss: 3.84120619058609. Learning Rate: 7.680271117771118e-05\n",
      "Batch   150 of 9,631 Elased 0:00:58. Training loss: 3.858397068977356. Learning Rate: 7.678536741036742e-05\n",
      "Batch   200 of 9,631 Elased 0:01:17. Training loss: 3.9117539966106416. Learning Rate: 7.676802364302365e-05\n",
      "Batch   250 of 9,631 Elased 0:01:37. Training loss: 3.8540110578536986. Learning Rate: 7.675067987567988e-05\n",
      "Batch   300 of 9,631 Elased 0:01:56. Training loss: 3.839766409397125. Learning Rate: 7.673333610833611e-05\n",
      "Batch   350 of 9,631 Elased 0:02:15. Training loss: 3.8275011185237338. Learning Rate: 7.671599234099235e-05\n",
      "Batch   400 of 9,631 Elased 0:02:34. Training loss: 3.8434873425960543. Learning Rate: 7.669864857364858e-05\n",
      "Batch   450 of 9,631 Elased 0:02:53. Training loss: 3.842275167041355. Learning Rate: 7.668130480630481e-05\n",
      "Batch   500 of 9,631 Elased 0:03:12. Training loss: 3.8476752009391784. Learning Rate: 7.666396103896104e-05\n",
      "Batch   550 of 9,631 Elased 0:03:31. Training loss: 3.8403366309946234. Learning Rate: 7.664661727161727e-05\n",
      "Batch   600 of 9,631 Elased 0:03:50. Training loss: 3.8466961244742075. Learning Rate: 7.662927350427351e-05\n",
      "Batch   650 of 9,631 Elased 0:04:10. Training loss: 3.8475315944965067. Learning Rate: 7.661192973692974e-05\n",
      "Batch   700 of 9,631 Elased 0:04:29. Training loss: 3.842976244858333. Learning Rate: 7.659458596958597e-05\n",
      "Batch   750 of 9,631 Elased 0:04:48. Training loss: 3.8306933771769205. Learning Rate: 7.65772422022422e-05\n",
      "Batch   800 of 9,631 Elased 0:05:07. Training loss: 3.8285098922252656. Learning Rate: 7.655989843489844e-05\n",
      "Batch   850 of 9,631 Elased 0:05:27. Training loss: 3.819927862952737. Learning Rate: 7.654255466755467e-05\n",
      "Batch   900 of 9,631 Elased 0:05:46. Training loss: 3.822728704346551. Learning Rate: 7.652521090021091e-05\n",
      "Batch   950 of 9,631 Elased 0:06:05. Training loss: 3.816825268394069. Learning Rate: 7.650786713286714e-05\n",
      "Batch 1,000 of 9,631 Elased 0:06:25. Training loss: 3.8159890265464784. Learning Rate: 7.649052336552337e-05\n",
      "Batch 1,050 of 9,631 Elased 0:06:44. Training loss: 3.8171414895284745. Learning Rate: 7.64731795981796e-05\n",
      "Batch 1,100 of 9,631 Elased 0:07:03. Training loss: 3.818446709242734. Learning Rate: 7.645583583083583e-05\n",
      "Batch 1,150 of 9,631 Elased 0:07:22. Training loss: 3.8197282013685805. Learning Rate: 7.643849206349206e-05\n",
      "Batch 1,200 of 9,631 Elased 0:07:42. Training loss: 3.8222660722335178. Learning Rate: 7.64211482961483e-05\n",
      "Batch 1,250 of 9,631 Elased 0:08:01. Training loss: 3.8226992555618287. Learning Rate: 7.640380452880453e-05\n",
      "Batch 1,300 of 9,631 Elased 0:08:20. Training loss: 3.8196579946004428. Learning Rate: 7.638646076146076e-05\n",
      "Batch 1,350 of 9,631 Elased 0:08:39. Training loss: 3.8190359150921855. Learning Rate: 7.636911699411699e-05\n",
      "Batch 1,400 of 9,631 Elased 0:08:59. Training loss: 3.8175874713488986. Learning Rate: 7.635177322677323e-05\n",
      "Batch 1,450 of 9,631 Elased 0:09:18. Training loss: 3.8158681749475414. Learning Rate: 7.633442945942946e-05\n",
      "Batch 1,500 of 9,631 Elased 0:09:37. Training loss: 3.8124629697799683. Learning Rate: 7.63170856920857e-05\n",
      "Batch 1,550 of 9,631 Elased 0:09:57. Training loss: 3.813566778398329. Learning Rate: 7.629974192474193e-05\n",
      "Batch 1,600 of 9,631 Elased 0:10:16. Training loss: 3.818921348750591. Learning Rate: 7.628239815739816e-05\n",
      "Batch 1,650 of 9,631 Elased 0:10:35. Training loss: 3.8243534261530097. Learning Rate: 7.626505439005439e-05\n",
      "Batch 1,700 of 9,631 Elased 0:10:54. Training loss: 3.8208297463024365. Learning Rate: 7.624771062271062e-05\n",
      "Batch 1,750 of 9,631 Elased 0:11:14. Training loss: 3.820676664216178. Learning Rate: 7.623036685536687e-05\n",
      "Batch 1,800 of 9,631 Elased 0:11:33. Training loss: 3.818648753431108. Learning Rate: 7.62130230880231e-05\n",
      "Batch 1,850 of 9,631 Elased 0:11:53. Training loss: 3.821961621336035. Learning Rate: 7.619567932067932e-05\n",
      "Batch 1,900 of 9,631 Elased 0:12:13. Training loss: 3.8195273347904806. Learning Rate: 7.617833555333555e-05\n",
      "Batch 1,950 of 9,631 Elased 0:12:32. Training loss: 3.816089419585008. Learning Rate: 7.616099178599178e-05\n",
      "Batch 2,000 of 9,631 Elased 0:12:51. Training loss: 3.811241665840149. Learning Rate: 7.614364801864801e-05\n",
      "Batch 2,050 of 9,631 Elased 0:13:10. Training loss: 3.812399920137917. Learning Rate: 7.612630425130425e-05\n",
      "Batch 2,100 of 9,631 Elased 0:13:30. Training loss: 3.8167160711969648. Learning Rate: 7.610896048396048e-05\n",
      "Batch 2,150 of 9,631 Elased 0:13:49. Training loss: 3.8180361823148505. Learning Rate: 7.609161671661673e-05\n",
      "Batch 2,200 of 9,631 Elased 0:14:08. Training loss: 3.816450913060795. Learning Rate: 7.607427294927296e-05\n",
      "Batch 2,250 of 9,631 Elased 0:14:28. Training loss: 3.8137044507132636. Learning Rate: 7.605692918192919e-05\n",
      "Batch 2,300 of 9,631 Elased 0:14:47. Training loss: 3.8151466051391933. Learning Rate: 7.603958541458543e-05\n",
      "Batch 2,350 of 9,631 Elased 0:15:06. Training loss: 3.8122758531570433. Learning Rate: 7.602224164724166e-05\n",
      "Batch 2,400 of 9,631 Elased 0:15:25. Training loss: 3.812111966709296. Learning Rate: 7.600489787989789e-05\n",
      "Batch 2,450 of 9,631 Elased 0:15:45. Training loss: 3.8087172855649674. Learning Rate: 7.598755411255412e-05\n",
      "Batch 2,500 of 9,631 Elased 0:16:04. Training loss: 3.8071815327644347. Learning Rate: 7.597021034521034e-05\n",
      "Batch 2,550 of 9,631 Elased 0:16:23. Training loss: 3.8125776331097474. Learning Rate: 7.595286657786657e-05\n",
      "Batch 2,600 of 9,631 Elased 0:16:43. Training loss: 3.814854612350464. Learning Rate: 7.593552281052282e-05\n",
      "Batch 2,650 of 9,631 Elased 0:17:02. Training loss: 3.8172298483578664. Learning Rate: 7.591817904317905e-05\n",
      "Batch 2,700 of 9,631 Elased 0:17:22. Training loss: 3.81967478023635. Learning Rate: 7.590083527583528e-05\n",
      "Batch 2,750 of 9,631 Elased 0:17:41. Training loss: 3.8164249971996655. Learning Rate: 7.58834915084915e-05\n",
      "Batch 2,800 of 9,631 Elased 0:18:00. Training loss: 3.817144339978695. Learning Rate: 7.586614774114775e-05\n",
      "Batch 2,850 of 9,631 Elased 0:18:20. Training loss: 3.8195765555114076. Learning Rate: 7.584880397380398e-05\n",
      "Batch 2,900 of 9,631 Elased 0:18:39. Training loss: 3.8193832371563747. Learning Rate: 7.583146020646022e-05\n",
      "Batch 2,950 of 9,631 Elased 0:18:58. Training loss: 3.8192392745664563. Learning Rate: 7.581411643911645e-05\n",
      "Batch 3,000 of 9,631 Elased 0:19:18. Training loss: 3.822262056072553. Learning Rate: 7.579677267177268e-05\n",
      "Batch 3,050 of 9,631 Elased 0:19:37. Training loss: 3.8224977947063135. Learning Rate: 7.577942890442891e-05\n",
      "Batch 3,100 of 9,631 Elased 0:19:56. Training loss: 3.8227509528975334. Learning Rate: 7.576208513708514e-05\n",
      "Batch 3,150 of 9,631 Elased 0:20:15. Training loss: 3.8185178057731144. Learning Rate: 7.574474136974138e-05\n",
      "Batch 3,200 of 9,631 Elased 0:20:35. Training loss: 3.8204425317421555. Learning Rate: 7.572739760239761e-05\n",
      "Batch 3,250 of 9,631 Elased 0:20:54. Training loss: 3.8202165871766898. Learning Rate: 7.571005383505384e-05\n",
      "Batch 3,300 of 9,631 Elased 0:21:14. Training loss: 3.8224742451942326. Learning Rate: 7.569271006771007e-05\n",
      "Batch 3,350 of 9,631 Elased 0:21:33. Training loss: 3.8223104579768963. Learning Rate: 7.56753663003663e-05\n",
      "Batch 3,400 of 9,631 Elased 0:21:53. Training loss: 3.8230921465859695. Learning Rate: 7.565802253302253e-05\n",
      "Batch 3,450 of 9,631 Elased 0:22:12. Training loss: 3.8239507706614506. Learning Rate: 7.564067876567877e-05\n",
      "Batch 3,500 of 9,631 Elased 0:22:31. Training loss: 3.8222113805157796. Learning Rate: 7.5623334998335e-05\n",
      "Batch 3,550 of 9,631 Elased 0:22:51. Training loss: 3.8226703353331124. Learning Rate: 7.560599123099124e-05\n",
      "Batch 3,600 of 9,631 Elased 0:23:10. Training loss: 3.824454097516007. Learning Rate: 7.558864746364747e-05\n",
      "Batch 3,650 of 9,631 Elased 0:23:29. Training loss: 3.8278481100356743. Learning Rate: 7.55713036963037e-05\n",
      "Batch 3,700 of 9,631 Elased 0:23:49. Training loss: 3.826918634562879. Learning Rate: 7.555395992895993e-05\n",
      "Batch 3,750 of 9,631 Elased 0:24:08. Training loss: 3.825040557829539. Learning Rate: 7.553661616161617e-05\n",
      "Batch 3,800 of 9,631 Elased 0:24:27. Training loss: 3.823182538653675. Learning Rate: 7.55192723942724e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3,850 of 9,631 Elased 0:24:47. Training loss: 3.81997220643155. Learning Rate: 7.550192862692863e-05\n",
      "Batch 3,900 of 9,631 Elased 0:25:06. Training loss: 3.817679696663832. Learning Rate: 7.548458485958486e-05\n",
      "Batch 3,950 of 9,631 Elased 0:25:25. Training loss: 3.818809731308418. Learning Rate: 7.546724109224109e-05\n",
      "Batch 4,000 of 9,631 Elased 0:25:44. Training loss: 3.818885140508413. Learning Rate: 7.544989732489733e-05\n",
      "Batch 4,050 of 9,631 Elased 0:26:04. Training loss: 3.817982268480607. Learning Rate: 7.543255355755356e-05\n",
      "Batch 4,100 of 9,631 Elased 0:26:23. Training loss: 3.816952204791511. Learning Rate: 7.541520979020979e-05\n",
      "Batch 4,150 of 9,631 Elased 0:26:42. Training loss: 3.817161174308823. Learning Rate: 7.539786602286603e-05\n",
      "Batch 4,200 of 9,631 Elased 0:27:02. Training loss: 3.817045424921172. Learning Rate: 7.538052225552226e-05\n",
      "Batch 4,250 of 9,631 Elased 0:27:21. Training loss: 3.816927349847906. Learning Rate: 7.536317848817849e-05\n",
      "Batch 4,300 of 9,631 Elased 0:27:40. Training loss: 3.816485469701678. Learning Rate: 7.534583472083473e-05\n",
      "Batch 4,350 of 9,631 Elased 0:28:00. Training loss: 3.8161895497365927. Learning Rate: 7.532849095349096e-05\n",
      "Batch 4,400 of 9,631 Elased 0:28:19. Training loss: 3.817244554676793. Learning Rate: 7.531114718614719e-05\n",
      "Batch 4,450 of 9,631 Elased 0:28:39. Training loss: 3.818183564095015. Learning Rate: 7.529380341880342e-05\n",
      "Batch 4,500 of 9,631 Elased 0:28:58. Training loss: 3.8194651681317224. Learning Rate: 7.527645965145965e-05\n",
      "Batch 4,550 of 9,631 Elased 0:29:17. Training loss: 3.817680272086636. Learning Rate: 7.525911588411588e-05\n",
      "Batch 4,600 of 9,631 Elased 0:29:36. Training loss: 3.8191649890205133. Learning Rate: 7.524177211677212e-05\n",
      "Batch 4,650 of 9,631 Elased 0:29:56. Training loss: 3.817594287292932. Learning Rate: 7.522442834942835e-05\n",
      "Batch 4,700 of 9,631 Elased 0:30:15. Training loss: 3.8184421587751265. Learning Rate: 7.520708458208458e-05\n",
      "Batch 4,750 of 9,631 Elased 0:30:34. Training loss: 3.8183556231197557. Learning Rate: 7.518974081474081e-05\n",
      "Batch 4,800 of 9,631 Elased 0:30:54. Training loss: 3.816798610513409. Learning Rate: 7.517239704739705e-05\n",
      "Batch 4,850 of 9,631 Elased 0:31:13. Training loss: 3.815635543828158. Learning Rate: 7.515505328005328e-05\n",
      "Batch 4,900 of 9,631 Elased 0:31:32. Training loss: 3.8157974525130527. Learning Rate: 7.513770951270953e-05\n",
      "Batch 4,950 of 9,631 Elased 0:31:51. Training loss: 3.815800922639442. Learning Rate: 7.512036574536576e-05\n",
      "Batch 5,000 of 9,631 Elased 0:32:11. Training loss: 3.816731345963478. Learning Rate: 7.510302197802198e-05\n",
      "Batch 5,050 of 9,631 Elased 0:32:30. Training loss: 3.8166157040501587. Learning Rate: 7.508567821067821e-05\n",
      "Batch 5,100 of 9,631 Elased 0:32:49. Training loss: 3.814276206750496. Learning Rate: 7.506833444333444e-05\n",
      "Batch 5,150 of 9,631 Elased 0:33:09. Training loss: 3.815209028628266. Learning Rate: 7.505099067599069e-05\n",
      "Batch 5,200 of 9,631 Elased 0:33:28. Training loss: 3.815017789533505. Learning Rate: 7.503364690864692e-05\n",
      "Batch 5,250 of 9,631 Elased 0:33:47. Training loss: 3.815022999740782. Learning Rate: 7.501630314130314e-05\n",
      "Batch 5,300 of 9,631 Elased 0:34:07. Training loss: 3.814383168557905. Learning Rate: 7.499895937395937e-05\n",
      "Batch 5,350 of 9,631 Elased 0:34:26. Training loss: 3.816212766170502. Learning Rate: 7.49816156066156e-05\n",
      "Batch 5,400 of 9,631 Elased 0:34:46. Training loss: 3.8162545037931865. Learning Rate: 7.496427183927183e-05\n",
      "Batch 5,450 of 9,631 Elased 0:35:05. Training loss: 3.816408829229687. Learning Rate: 7.494692807192808e-05\n",
      "Batch 5,500 of 9,631 Elased 0:35:25. Training loss: 3.816378357041966. Learning Rate: 7.49295843045843e-05\n",
      "Batch 5,550 of 9,631 Elased 0:35:45. Training loss: 3.8167493746516943. Learning Rate: 7.491224053724055e-05\n",
      "Batch 5,600 of 9,631 Elased 0:36:04. Training loss: 3.8157037993626934. Learning Rate: 7.489489676989678e-05\n",
      "Batch 5,650 of 9,631 Elased 0:36:23. Training loss: 3.817876224834307. Learning Rate: 7.4877553002553e-05\n",
      "Batch 5,700 of 9,631 Elased 0:36:43. Training loss: 3.817360658373749. Learning Rate: 7.486020923520925e-05\n",
      "Batch 5,750 of 9,631 Elased 0:37:02. Training loss: 3.8164216653367746. Learning Rate: 7.484286546786548e-05\n",
      "Batch 5,800 of 9,631 Elased 0:37:21. Training loss: 3.8164134091960973. Learning Rate: 7.482552170052171e-05\n",
      "Batch 5,850 of 9,631 Elased 0:37:40. Training loss: 3.8166262072579475. Learning Rate: 7.480817793317794e-05\n",
      "Batch 5,900 of 9,631 Elased 0:38:00. Training loss: 3.8161610526755703. Learning Rate: 7.479083416583417e-05\n",
      "Batch 5,950 of 9,631 Elased 0:38:19. Training loss: 3.816133746159177. Learning Rate: 7.47734903984904e-05\n",
      "Batch 6,000 of 9,631 Elased 0:38:38. Training loss: 3.815223407725493. Learning Rate: 7.475614663114664e-05\n",
      "Batch 6,050 of 9,631 Elased 0:38:58. Training loss: 3.815217637877819. Learning Rate: 7.473880286380287e-05\n",
      "Batch 6,100 of 9,631 Elased 0:39:17. Training loss: 3.815416688586845. Learning Rate: 7.47214590964591e-05\n",
      "Batch 6,150 of 9,631 Elased 0:39:36. Training loss: 3.814893411175022. Learning Rate: 7.470411532911533e-05\n",
      "Batch 6,200 of 9,631 Elased 0:39:55. Training loss: 3.81264690120374. Learning Rate: 7.468677156177157e-05\n",
      "Batch 6,250 of 9,631 Elased 0:40:15. Training loss: 3.8121774335670473. Learning Rate: 7.46694277944278e-05\n",
      "Batch 6,300 of 9,631 Elased 0:40:35. Training loss: 3.8126138041700637. Learning Rate: 7.465208402708404e-05\n",
      "Batch 6,350 of 9,631 Elased 0:40:54. Training loss: 3.812164964319214. Learning Rate: 7.463474025974027e-05\n",
      "Batch 6,400 of 9,631 Elased 0:41:14. Training loss: 3.8109271862916647. Learning Rate: 7.46173964923965e-05\n",
      "Batch 6,450 of 9,631 Elased 0:41:33. Training loss: 3.812396233747172. Learning Rate: 7.460005272505273e-05\n",
      "Batch 6,500 of 9,631 Elased 0:41:52. Training loss: 3.810190967981632. Learning Rate: 7.458270895770896e-05\n",
      "Batch 6,550 of 9,631 Elased 0:42:11. Training loss: 3.810181067208297. Learning Rate: 7.456536519036519e-05\n",
      "Batch 6,600 of 9,631 Elased 0:42:31. Training loss: 3.8107052304708597. Learning Rate: 7.454802142302143e-05\n",
      "Batch 6,650 of 9,631 Elased 0:42:50. Training loss: 3.810482928824604. Learning Rate: 7.453067765567766e-05\n",
      "Batch 6,700 of 9,631 Elased 0:43:09. Training loss: 3.811273368454691. Learning Rate: 7.451333388833389e-05\n",
      "Batch 6,750 of 9,631 Elased 0:43:29. Training loss: 3.811189703040653. Learning Rate: 7.449599012099012e-05\n",
      "Batch 6,800 of 9,631 Elased 0:43:48. Training loss: 3.8104428045714602. Learning Rate: 7.447864635364635e-05\n",
      "Batch 6,850 of 9,631 Elased 0:44:08. Training loss: 3.809761168817534. Learning Rate: 7.446130258630259e-05\n",
      "Batch 6,900 of 9,631 Elased 0:44:27. Training loss: 3.809442974018014. Learning Rate: 7.444395881895883e-05\n",
      "Batch 6,950 of 9,631 Elased 0:44:46. Training loss: 3.8092088481333617. Learning Rate: 7.442661505161506e-05\n",
      "Batch 7,000 of 9,631 Elased 0:45:06. Training loss: 3.8081700765235085. Learning Rate: 7.440927128427129e-05\n",
      "Batch 7,050 of 9,631 Elased 0:45:25. Training loss: 3.807129977425785. Learning Rate: 7.439192751692752e-05\n",
      "Batch 7,100 of 9,631 Elased 0:45:44. Training loss: 3.8070300125907846. Learning Rate: 7.437458374958375e-05\n",
      "Batch 7,150 of 9,631 Elased 0:46:04. Training loss: 3.8064717700097943. Learning Rate: 7.435723998223999e-05\n",
      "Batch 7,200 of 9,631 Elased 0:46:23. Training loss: 3.8064838387072086. Learning Rate: 7.433989621489622e-05\n",
      "Batch 7,250 of 9,631 Elased 0:46:42. Training loss: 3.805372694196372. Learning Rate: 7.432255244755245e-05\n",
      "Batch 7,300 of 9,631 Elased 0:47:02. Training loss: 3.8054523454137046. Learning Rate: 7.430520868020868e-05\n",
      "Batch 7,350 of 9,631 Elased 0:47:21. Training loss: 3.8045515874453955. Learning Rate: 7.428786491286491e-05\n",
      "Batch 7,400 of 9,631 Elased 0:47:40. Training loss: 3.8031646006655047. Learning Rate: 7.427052114552114e-05\n",
      "Batch 7,450 of 9,631 Elased 0:47:59. Training loss: 3.8032132691664984. Learning Rate: 7.425317737817738e-05\n",
      "Batch 7,500 of 9,631 Elased 0:48:19. Training loss: 3.8019929192701976. Learning Rate: 7.423583361083361e-05\n",
      "Batch 7,550 of 9,631 Elased 0:48:38. Training loss: 3.8016483995772354. Learning Rate: 7.421848984348985e-05\n",
      "Batch 7,600 of 9,631 Elased 0:48:57. Training loss: 3.802004798509573. Learning Rate: 7.420114607614608e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7,650 of 9,631 Elased 0:49:17. Training loss: 3.8020544549374797. Learning Rate: 7.418380230880231e-05\n",
      "Batch 7,700 of 9,631 Elased 0:49:37. Training loss: 3.8009117703468767. Learning Rate: 7.416645854145855e-05\n",
      "Batch 7,750 of 9,631 Elased 0:49:56. Training loss: 3.7989222772505977. Learning Rate: 7.414911477411478e-05\n",
      "Batch 7,800 of 9,631 Elased 0:50:15. Training loss: 3.7991968918610843. Learning Rate: 7.413177100677101e-05\n",
      "Batch 7,850 of 9,631 Elased 0:50:35. Training loss: 3.799594318805986. Learning Rate: 7.411442723942724e-05\n",
      "Batch 7,900 of 9,631 Elased 0:50:54. Training loss: 3.7978489659556858. Learning Rate: 7.409708347208347e-05\n",
      "Batch 7,950 of 9,631 Elased 0:51:14. Training loss: 3.799356620791573. Learning Rate: 7.40797397047397e-05\n",
      "Batch 8,000 of 9,631 Elased 0:51:33. Training loss: 3.7987718834728. Learning Rate: 7.406239593739594e-05\n",
      "Batch 8,050 of 9,631 Elased 0:51:52. Training loss: 3.799859949594699. Learning Rate: 7.404505217005217e-05\n",
      "Batch 8,100 of 9,631 Elased 0:52:12. Training loss: 3.800118586766867. Learning Rate: 7.40277084027084e-05\n",
      "Batch 8,150 of 9,631 Elased 0:52:31. Training loss: 3.801115933064303. Learning Rate: 7.401036463536463e-05\n",
      "Batch 8,200 of 9,631 Elased 0:52:50. Training loss: 3.8004474836587905. Learning Rate: 7.399302086802087e-05\n",
      "Batch 8,250 of 9,631 Elased 0:53:10. Training loss: 3.801205934856877. Learning Rate: 7.39756771006771e-05\n",
      "Batch 8,300 of 9,631 Elased 0:53:29. Training loss: 3.8010916203619485. Learning Rate: 7.395833333333335e-05\n",
      "Batch 8,350 of 9,631 Elased 0:53:49. Training loss: 3.8003103776606255. Learning Rate: 7.394098956598958e-05\n",
      "Batch 8,400 of 9,631 Elased 0:54:08. Training loss: 3.7994448503284226. Learning Rate: 7.39236457986458e-05\n",
      "Batch 8,450 of 9,631 Elased 0:54:27. Training loss: 3.798761117627635. Learning Rate: 7.390630203130203e-05\n",
      "Batch 8,500 of 9,631 Elased 0:54:46. Training loss: 3.7981180121337665. Learning Rate: 7.388895826395826e-05\n",
      "Batch 8,550 of 9,631 Elased 0:55:05. Training loss: 3.7978965861755505. Learning Rate: 7.38716144966145e-05\n",
      "Batch 8,600 of 9,631 Elased 0:55:25. Training loss: 3.798397769692332. Learning Rate: 7.385427072927074e-05\n",
      "Batch 8,650 of 9,631 Elased 0:55:44. Training loss: 3.7972626043192914. Learning Rate: 7.383692696192697e-05\n",
      "Batch 8,700 of 9,631 Elased 0:56:03. Training loss: 3.7970899123021926. Learning Rate: 7.38195831945832e-05\n",
      "Batch 8,750 of 9,631 Elased 0:56:25. Training loss: 3.796652048260825. Learning Rate: 7.380223942723942e-05\n",
      "Batch 8,800 of 9,631 Elased 0:56:45. Training loss: 3.7963950786400926. Learning Rate: 7.378489565989565e-05\n",
      "Batch 8,850 of 9,631 Elased 0:57:07. Training loss: 3.796091712140768. Learning Rate: 7.37675518925519e-05\n",
      "Batch 8,900 of 9,631 Elased 0:57:28. Training loss: 3.7964091426468967. Learning Rate: 7.375020812520812e-05\n",
      "Batch 8,950 of 9,631 Elased 0:57:49. Training loss: 3.7958540071055875. Learning Rate: 7.373286435786437e-05\n",
      "Batch 9,000 of 9,631 Elased 0:58:10. Training loss: 3.7945775274038316. Learning Rate: 7.37155205905206e-05\n",
      "Batch 9,050 of 9,631 Elased 0:58:31. Training loss: 3.7953272794623403. Learning Rate: 7.369817682317683e-05\n",
      "Batch 9,100 of 9,631 Elased 0:58:52. Training loss: 3.794962087057449. Learning Rate: 7.368083305583306e-05\n",
      "Batch 9,150 of 9,631 Elased 0:59:13. Training loss: 3.795046769660679. Learning Rate: 7.36634892884893e-05\n",
      "Batch 9,200 of 9,631 Elased 0:59:34. Training loss: 3.7958890053370724. Learning Rate: 7.364614552114553e-05\n",
      "Batch 9,250 of 9,631 Elased 0:59:55. Training loss: 3.7955587294423903. Learning Rate: 7.362880175380176e-05\n",
      "Batch 9,300 of 9,631 Elased 1:00:17. Training loss: 3.7957550073310893. Learning Rate: 7.361145798645799e-05\n",
      "Batch 9,350 of 9,631 Elased 1:00:38. Training loss: 3.7956477166879625. Learning Rate: 7.359411421911422e-05\n",
      "Batch 9,400 of 9,631 Elased 1:00:59. Training loss: 3.7956690628604686. Learning Rate: 7.357677045177046e-05\n",
      "Batch 9,450 of 9,631 Elased 1:01:20. Training loss: 3.79512502747238. Learning Rate: 7.355942668442669e-05\n",
      "Batch 9,500 of 9,631 Elased 1:01:42. Training loss: 3.7946081075040916. Learning Rate: 7.354208291708292e-05\n",
      "Batch 9,550 of 9,631 Elased 1:02:03. Training loss: 3.793701105679517. Learning Rate: 7.352473914973915e-05\n",
      "Batch 9,600 of 9,631 Elased 1:02:24. Training loss: 3.794209422407051. Learning Rate: 7.350739538239539e-05\n",
      "\n",
      "\n",
      "  Average training loss: 3.79\n",
      "  Training epcoh took: 1:02:37\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbeca93a51394edb816c34a06b10b388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=67.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Validation Loss: 3.75\n",
      "  Validation took: 0:00:43\n",
      "\n",
      "======== Epoch 9 / 30 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b2b731938340e0aa99fe8c6f3a6573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch    50 of 9,631 Elased 0:00:21. Training loss: 3.7701961898803713. Learning Rate: 7.347929847929848e-05\n",
      "Batch   100 of 9,631 Elased 0:00:42. Training loss: 3.7480157089233397. Learning Rate: 7.346195471195472e-05\n",
      "Batch   150 of 9,631 Elased 0:01:03. Training loss: 3.7592346747716268. Learning Rate: 7.344461094461095e-05\n",
      "Batch   200 of 9,631 Elased 0:01:24. Training loss: 3.8038906908035277. Learning Rate: 7.342726717726718e-05\n",
      "Batch   250 of 9,631 Elased 0:01:46. Training loss: 3.753797553062439. Learning Rate: 7.340992340992341e-05\n",
      "Batch   300 of 9,631 Elased 0:02:07. Training loss: 3.731330614089966. Learning Rate: 7.339257964257965e-05\n",
      "Batch   350 of 9,631 Elased 0:02:28. Training loss: 3.727983809198652. Learning Rate: 7.337523587523588e-05\n",
      "Batch   400 of 9,631 Elased 0:02:49. Training loss: 3.7460014432668687. Learning Rate: 7.335789210789211e-05\n",
      "Batch   450 of 9,631 Elased 0:03:10. Training loss: 3.742895693778992. Learning Rate: 7.334054834054834e-05\n",
      "Batch   500 of 9,631 Elased 0:03:31. Training loss: 3.7507696590423585. Learning Rate: 7.332320457320457e-05\n",
      "Batch   550 of 9,631 Elased 0:03:52. Training loss: 3.7426800723509355. Learning Rate: 7.33058608058608e-05\n",
      "Batch   600 of 9,631 Elased 0:04:14. Training loss: 3.7520024502277374. Learning Rate: 7.328851703851704e-05\n",
      "Batch   650 of 9,631 Elased 0:04:35. Training loss: 3.7524260597962598. Learning Rate: 7.327117327117327e-05\n",
      "Batch   700 of 9,631 Elased 0:04:56. Training loss: 3.746018762588501. Learning Rate: 7.32538295038295e-05\n",
      "Batch   750 of 9,631 Elased 0:05:17. Training loss: 3.732809562365214. Learning Rate: 7.323648573648574e-05\n",
      "Batch   800 of 9,631 Elased 0:05:39. Training loss: 3.73200462192297. Learning Rate: 7.321914196914197e-05\n",
      "Batch   850 of 9,631 Elased 0:06:00. Training loss: 3.726434898656957. Learning Rate: 7.320179820179822e-05\n",
      "Batch   900 of 9,631 Elased 0:06:21. Training loss: 3.724665382173326. Learning Rate: 7.318445443445445e-05\n",
      "Batch   950 of 9,631 Elased 0:06:42. Training loss: 3.720668652434098. Learning Rate: 7.316711066711067e-05\n",
      "Batch 1,000 of 9,631 Elased 0:07:03. Training loss: 3.7224847292900085. Learning Rate: 7.31497668997669e-05\n",
      "Batch 1,050 of 9,631 Elased 0:07:25. Training loss: 3.724360572724115. Learning Rate: 7.313242313242313e-05\n",
      "Batch 1,100 of 9,631 Elased 0:07:46. Training loss: 3.7258478645844892. Learning Rate: 7.311507936507936e-05\n",
      "Batch 1,150 of 9,631 Elased 0:08:07. Training loss: 3.725710620880127. Learning Rate: 7.30977355977356e-05\n",
      "Batch 1,200 of 9,631 Elased 0:08:27. Training loss: 3.7278199970722197. Learning Rate: 7.308039183039183e-05\n",
      "Batch 1,250 of 9,631 Elased 0:08:49. Training loss: 3.729247290420532. Learning Rate: 7.306304806304806e-05\n",
      "Batch 1,300 of 9,631 Elased 0:09:10. Training loss: 3.726437814969283. Learning Rate: 7.304570429570429e-05\n",
      "Batch 1,350 of 9,631 Elased 0:09:32. Training loss: 3.724806333118015. Learning Rate: 7.302836052836052e-05\n",
      "Batch 1,400 of 9,631 Elased 0:09:54. Training loss: 3.7234150416510445. Learning Rate: 7.301101676101676e-05\n",
      "Batch 1,450 of 9,631 Elased 0:10:15. Training loss: 3.721359887123108. Learning Rate: 7.299367299367301e-05\n",
      "Batch 1,500 of 9,631 Elased 0:10:36. Training loss: 3.7182789600690205. Learning Rate: 7.297632922632924e-05\n",
      "Batch 1,550 of 9,631 Elased 0:10:57. Training loss: 3.7174252846933182. Learning Rate: 7.295898545898547e-05\n",
      "Batch 1,600 of 9,631 Elased 0:11:18. Training loss: 3.7226557724177836. Learning Rate: 7.29416416916417e-05\n",
      "Batch 1,650 of 9,631 Elased 0:11:39. Training loss: 3.727786072239731. Learning Rate: 7.292429792429792e-05\n",
      "Batch 1,700 of 9,631 Elased 0:12:01. Training loss: 3.7246821149657756. Learning Rate: 7.290695415695417e-05\n",
      "Batch 1,750 of 9,631 Elased 0:12:22. Training loss: 3.7252062702178956. Learning Rate: 7.28896103896104e-05\n",
      "Batch 1,800 of 9,631 Elased 0:12:43. Training loss: 3.7240550023979613. Learning Rate: 7.287226662226663e-05\n",
      "Batch 1,850 of 9,631 Elased 0:13:04. Training loss: 3.726486470119373. Learning Rate: 7.285492285492286e-05\n",
      "Batch 1,900 of 9,631 Elased 0:13:25. Training loss: 3.7243974188754434. Learning Rate: 7.283757908757908e-05\n",
      "Batch 1,950 of 9,631 Elased 0:13:47. Training loss: 3.7193144087913708. Learning Rate: 7.282023532023531e-05\n",
      "Batch 2,000 of 9,631 Elased 0:14:08. Training loss: 3.7152862317562105. Learning Rate: 7.280289155289156e-05\n",
      "Batch 2,050 of 9,631 Elased 0:14:29. Training loss: 3.716481084707307. Learning Rate: 7.278554778554779e-05\n",
      "Batch 2,100 of 9,631 Elased 0:14:50. Training loss: 3.719622157868885. Learning Rate: 7.276820401820403e-05\n",
      "Batch 2,150 of 9,631 Elased 0:15:12. Training loss: 3.7213116399631945. Learning Rate: 7.275086025086026e-05\n",
      "Batch 2,200 of 9,631 Elased 0:15:33. Training loss: 3.72082962567156. Learning Rate: 7.273351648351649e-05\n",
      "Batch 2,250 of 9,631 Elased 0:15:54. Training loss: 3.718854660987854. Learning Rate: 7.271617271617272e-05\n",
      "Batch 2,300 of 9,631 Elased 0:16:15. Training loss: 3.7204275850627733. Learning Rate: 7.269882894882896e-05\n",
      "Batch 2,350 of 9,631 Elased 0:16:36. Training loss: 3.7176611739016594. Learning Rate: 7.268148518148519e-05\n",
      "Batch 2,400 of 9,631 Elased 0:16:58. Training loss: 3.717426103154818. Learning Rate: 7.266414141414142e-05\n",
      "Batch 2,450 of 9,631 Elased 0:17:19. Training loss: 3.7139212412736855. Learning Rate: 7.264679764679765e-05\n",
      "Batch 2,500 of 9,631 Elased 0:17:40. Training loss: 3.71148213596344. Learning Rate: 7.262945387945388e-05\n",
      "Batch 2,550 of 9,631 Elased 0:18:01. Training loss: 3.7165163678748936. Learning Rate: 7.261211011211012e-05\n",
      "Batch 2,600 of 9,631 Elased 0:18:22. Training loss: 3.719473002415437. Learning Rate: 7.259476634476635e-05\n",
      "Batch 2,650 of 9,631 Elased 0:18:44. Training loss: 3.721953349563311. Learning Rate: 7.257742257742258e-05\n",
      "Batch 2,700 of 9,631 Elased 0:19:05. Training loss: 3.7247226668287206. Learning Rate: 7.256007881007881e-05\n",
      "Batch 2,750 of 9,631 Elased 0:19:26. Training loss: 3.721993013381958. Learning Rate: 7.254273504273505e-05\n",
      "Batch 2,800 of 9,631 Elased 0:19:48. Training loss: 3.7239368327174867. Learning Rate: 7.252539127539128e-05\n",
      "Batch 2,850 of 9,631 Elased 0:20:09. Training loss: 3.726226719053168. Learning Rate: 7.250804750804752e-05\n",
      "Batch 2,900 of 9,631 Elased 0:20:30. Training loss: 3.7263474272037374. Learning Rate: 7.249070374070375e-05\n",
      "Batch 2,950 of 9,631 Elased 0:20:51. Training loss: 3.7259782141346043. Learning Rate: 7.247335997335998e-05\n",
      "Batch 3,000 of 9,631 Elased 0:21:12. Training loss: 3.7297235767046613. Learning Rate: 7.245601620601621e-05\n",
      "Batch 3,050 of 9,631 Elased 0:21:34. Training loss: 3.7301440382785485. Learning Rate: 7.243867243867244e-05\n",
      "Batch 3,100 of 9,631 Elased 0:21:55. Training loss: 3.730139909021316. Learning Rate: 7.242132867132867e-05\n",
      "Batch 3,150 of 9,631 Elased 0:22:16. Training loss: 3.7253813500631425. Learning Rate: 7.240398490398491e-05\n",
      "Batch 3,200 of 9,631 Elased 0:22:38. Training loss: 3.7272887521982194. Learning Rate: 7.238664113664114e-05\n",
      "Batch 3,250 of 9,631 Elased 0:22:59. Training loss: 3.727157818720891. Learning Rate: 7.236929736929737e-05\n",
      "Batch 3,300 of 9,631 Elased 0:23:20. Training loss: 3.729959160125617. Learning Rate: 7.23519536019536e-05\n",
      "Batch 3,350 of 9,631 Elased 0:23:41. Training loss: 3.7301535945863864. Learning Rate: 7.233460983460983e-05\n",
      "Batch 3,400 of 9,631 Elased 0:24:02. Training loss: 3.7310640483042774. Learning Rate: 7.231726606726607e-05\n",
      "Batch 3,450 of 9,631 Elased 0:24:24. Training loss: 3.731881618430649. Learning Rate: 7.22999222999223e-05\n",
      "Batch 3,500 of 9,631 Elased 0:24:45. Training loss: 3.730120148181915. Learning Rate: 7.228257853257854e-05\n",
      "Batch 3,550 of 9,631 Elased 0:25:06. Training loss: 3.730720301681841. Learning Rate: 7.226523476523477e-05\n",
      "Batch 3,600 of 9,631 Elased 0:25:27. Training loss: 3.732567513121499. Learning Rate: 7.2247890997891e-05\n",
      "Batch 3,650 of 9,631 Elased 0:25:48. Training loss: 3.7366865384742005. Learning Rate: 7.223054723054723e-05\n",
      "Batch 3,700 of 9,631 Elased 0:26:10. Training loss: 3.7357828754347726. Learning Rate: 7.221320346320347e-05\n",
      "Batch 3,750 of 9,631 Elased 0:26:31. Training loss: 3.7342521999994913. Learning Rate: 7.21958596958597e-05\n",
      "Batch 3,800 of 9,631 Elased 0:26:53. Training loss: 3.732744423966659. Learning Rate: 7.217851592851593e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3,850 of 9,631 Elased 0:27:14. Training loss: 3.730097958391363. Learning Rate: 7.216117216117216e-05\n",
      "Batch 3,900 of 9,631 Elased 0:27:35. Training loss: 3.7283571942035967. Learning Rate: 7.214382839382839e-05\n",
      "Batch 3,950 of 9,631 Elased 0:27:56. Training loss: 3.7296148546436165. Learning Rate: 7.212648462648462e-05\n",
      "Batch 4,000 of 9,631 Elased 0:28:17. Training loss: 3.7298251475095747. Learning Rate: 7.210914085914086e-05\n",
      "Batch 4,050 of 9,631 Elased 0:28:39. Training loss: 3.7290654299582964. Learning Rate: 7.209179709179709e-05\n",
      "Batch 4,100 of 9,631 Elased 0:29:00. Training loss: 3.7285727071180577. Learning Rate: 7.207445332445332e-05\n",
      "Batch 4,150 of 9,631 Elased 0:29:21. Training loss: 3.7284146404266356. Learning Rate: 7.205710955710956e-05\n",
      "Batch 4,200 of 9,631 Elased 0:29:42. Training loss: 3.728153241248358. Learning Rate: 7.20397657897658e-05\n",
      "Batch 4,250 of 9,631 Elased 0:30:03. Training loss: 3.7282219057363624. Learning Rate: 7.202242202242204e-05\n",
      "Batch 4,300 of 9,631 Elased 0:30:25. Training loss: 3.7281552872824113. Learning Rate: 7.200507825507827e-05\n",
      "Batch 4,350 of 9,631 Elased 0:30:46. Training loss: 3.72747758670785. Learning Rate: 7.19877344877345e-05\n",
      "Batch 4,400 of 9,631 Elased 0:31:07. Training loss: 3.7288363110748204. Learning Rate: 7.197039072039072e-05\n",
      "Batch 4,450 of 9,631 Elased 0:31:28. Training loss: 3.7292149946394932. Learning Rate: 7.195304695304695e-05\n",
      "Batch 4,500 of 9,631 Elased 0:31:49. Training loss: 3.7301895791424644. Learning Rate: 7.193570318570318e-05\n",
      "Batch 4,550 of 9,631 Elased 0:32:10. Training loss: 3.728750713447948. Learning Rate: 7.191835941835943e-05\n",
      "Batch 4,600 of 9,631 Elased 0:32:32. Training loss: 3.7301713922490243. Learning Rate: 7.190101565101565e-05\n",
      "Batch 4,650 of 9,631 Elased 0:32:53. Training loss: 3.7288139999297356. Learning Rate: 7.188367188367188e-05\n",
      "Batch 4,700 of 9,631 Elased 0:33:14. Training loss: 3.729683024502815. Learning Rate: 7.186632811632811e-05\n",
      "Batch 4,750 of 9,631 Elased 0:33:36. Training loss: 3.7294400027676633. Learning Rate: 7.184898434898436e-05\n",
      "Batch 4,800 of 9,631 Elased 0:33:57. Training loss: 3.7276636398086946. Learning Rate: 7.183164058164059e-05\n",
      "Batch 4,850 of 9,631 Elased 0:34:18. Training loss: 3.7267011576829496. Learning Rate: 7.181429681429683e-05\n",
      "Batch 4,900 of 9,631 Elased 0:34:40. Training loss: 3.7273032004735906. Learning Rate: 7.179695304695306e-05\n",
      "Batch 4,950 of 9,631 Elased 0:35:01. Training loss: 3.7270593321684635. Learning Rate: 7.177960927960929e-05\n",
      "Batch 5,000 of 9,631 Elased 0:35:22. Training loss: 3.7277433701753617. Learning Rate: 7.176226551226552e-05\n",
      "Batch 5,050 of 9,631 Elased 0:35:43. Training loss: 3.727730706729511. Learning Rate: 7.174492174492175e-05\n",
      "Batch 5,100 of 9,631 Elased 0:36:05. Training loss: 3.725472827495313. Learning Rate: 7.172757797757797e-05\n",
      "Batch 5,150 of 9,631 Elased 0:36:26. Training loss: 3.725802221321365. Learning Rate: 7.171023421023422e-05\n",
      "Batch 5,200 of 9,631 Elased 0:36:47. Training loss: 3.7254615095716255. Learning Rate: 7.169289044289045e-05\n",
      "Batch 5,250 of 9,631 Elased 0:37:08. Training loss: 3.725121747629983. Learning Rate: 7.167554667554668e-05\n",
      "Batch 5,300 of 9,631 Elased 0:37:29. Training loss: 3.7245866087472663. Learning Rate: 7.16582029082029e-05\n",
      "Batch 5,350 of 9,631 Elased 0:37:51. Training loss: 3.726952973548497. Learning Rate: 7.164085914085913e-05\n",
      "Batch 5,400 of 9,631 Elased 0:38:12. Training loss: 3.7269823018930577. Learning Rate: 7.162351537351538e-05\n",
      "Batch 5,450 of 9,631 Elased 0:38:33. Training loss: 3.726754592733646. Learning Rate: 7.16061716061716e-05\n",
      "Batch 5,500 of 9,631 Elased 0:38:55. Training loss: 3.7266745592680843. Learning Rate: 7.158882783882785e-05\n",
      "Batch 5,550 of 9,631 Elased 0:39:16. Training loss: 3.7269447478947337. Learning Rate: 7.157148407148408e-05\n",
      "Batch 5,600 of 9,631 Elased 0:39:37. Training loss: 3.725941173072372. Learning Rate: 7.155414030414031e-05\n",
      "Batch 5,650 of 9,631 Elased 0:39:58. Training loss: 3.7279349929041565. Learning Rate: 7.153679653679654e-05\n",
      "Batch 5,700 of 9,631 Elased 0:40:20. Training loss: 3.727876558993992. Learning Rate: 7.151945276945278e-05\n",
      "Batch 5,750 of 9,631 Elased 0:40:41. Training loss: 3.727013191326805. Learning Rate: 7.150210900210901e-05\n",
      "Batch 5,800 of 9,631 Elased 0:41:03. Training loss: 3.726853457545412. Learning Rate: 7.148476523476524e-05\n",
      "Batch 5,850 of 9,631 Elased 0:41:23. Training loss: 3.7271365870166027. Learning Rate: 7.146742146742147e-05\n",
      "Batch 5,900 of 9,631 Elased 0:41:45. Training loss: 3.726580239413148. Learning Rate: 7.14500777000777e-05\n",
      "Batch 5,950 of 9,631 Elased 0:42:06. Training loss: 3.726657049495633. Learning Rate: 7.143273393273393e-05\n",
      "Batch 6,000 of 9,631 Elased 0:42:28. Training loss: 3.726131836195787. Learning Rate: 7.141539016539017e-05\n",
      "Batch 6,050 of 9,631 Elased 0:42:49. Training loss: 3.7264955072757626. Learning Rate: 7.13980463980464e-05\n",
      "Batch 6,100 of 9,631 Elased 0:43:10. Training loss: 3.726774769200653. Learning Rate: 7.138070263070263e-05\n",
      "Batch 6,150 of 9,631 Elased 0:43:32. Training loss: 3.726280322908386. Learning Rate: 7.136335886335887e-05\n",
      "Batch 6,200 of 9,631 Elased 0:43:53. Training loss: 3.7239472738196775. Learning Rate: 7.13460150960151e-05\n",
      "Batch 6,250 of 9,631 Elased 0:44:14. Training loss: 3.723449402484894. Learning Rate: 7.132867132867134e-05\n",
      "Batch 6,300 of 9,631 Elased 0:44:35. Training loss: 3.7235557599862417. Learning Rate: 7.131132756132757e-05\n",
      "Batch 6,350 of 9,631 Elased 0:44:57. Training loss: 3.7231481104009734. Learning Rate: 7.12939837939838e-05\n",
      "Batch 6,400 of 9,631 Elased 0:45:18. Training loss: 3.722217318955809. Learning Rate: 7.127664002664003e-05\n",
      "Batch 6,450 of 9,631 Elased 0:45:39. Training loss: 3.7234379458057787. Learning Rate: 7.125929625929626e-05\n",
      "Batch 6,500 of 9,631 Elased 0:46:00. Training loss: 3.7214256112208735. Learning Rate: 7.124195249195249e-05\n",
      "Batch 6,550 of 9,631 Elased 0:46:22. Training loss: 3.7218179121636252. Learning Rate: 7.122460872460873e-05\n",
      "Batch 6,600 of 9,631 Elased 0:46:43. Training loss: 3.722071667602568. Learning Rate: 7.120726495726496e-05\n",
      "Batch 6,650 of 9,631 Elased 0:47:04. Training loss: 3.7218720371024054. Learning Rate: 7.118992118992119e-05\n",
      "Batch 6,700 of 9,631 Elased 0:47:26. Training loss: 3.7231107930460974. Learning Rate: 7.117257742257742e-05\n",
      "Batch 6,750 of 9,631 Elased 0:47:47. Training loss: 3.7232693069422687. Learning Rate: 7.115523365523365e-05\n",
      "Batch 6,800 of 9,631 Elased 0:48:09. Training loss: 3.7225806423671104. Learning Rate: 7.113788988788989e-05\n",
      "Batch 6,850 of 9,631 Elased 0:48:30. Training loss: 3.7219100183118. Learning Rate: 7.112054612054612e-05\n",
      "Batch 6,900 of 9,631 Elased 0:48:51. Training loss: 3.721558208240979. Learning Rate: 7.110320235320236e-05\n",
      "Batch 6,950 of 9,631 Elased 0:49:12. Training loss: 3.7214133597277907. Learning Rate: 7.108585858585859e-05\n",
      "Batch 7,000 of 9,631 Elased 0:49:33. Training loss: 3.720816042338099. Learning Rate: 7.106851481851482e-05\n",
      "Batch 7,050 of 9,631 Elased 0:49:55. Training loss: 3.719978163377613. Learning Rate: 7.105117105117105e-05\n",
      "Batch 7,100 of 9,631 Elased 0:50:16. Training loss: 3.7198548353054153. Learning Rate: 7.10338272838273e-05\n",
      "Batch 7,150 of 9,631 Elased 0:50:37. Training loss: 3.7195948415536146. Learning Rate: 7.101648351648352e-05\n",
      "Batch 7,200 of 9,631 Elased 0:50:58. Training loss: 3.7198520347310438. Learning Rate: 7.099913974913975e-05\n",
      "Batch 7,250 of 9,631 Elased 0:51:20. Training loss: 3.718499697142634. Learning Rate: 7.098179598179598e-05\n",
      "Batch 7,300 of 9,631 Elased 0:51:41. Training loss: 3.718958691587187. Learning Rate: 7.096445221445221e-05\n",
      "Batch 7,350 of 9,631 Elased 0:52:03. Training loss: 3.7180779890300464. Learning Rate: 7.094710844710844e-05\n",
      "Batch 7,400 of 9,631 Elased 0:52:25. Training loss: 3.7165955216014708. Learning Rate: 7.092976467976468e-05\n",
      "Batch 7,450 of 9,631 Elased 0:52:47. Training loss: 3.716410768800134. Learning Rate: 7.091242091242091e-05\n",
      "Batch 7,500 of 9,631 Elased 0:53:09. Training loss: 3.7151458538214364. Learning Rate: 7.089507714507714e-05\n",
      "Batch 7,550 of 9,631 Elased 0:53:31. Training loss: 3.7149695953628084. Learning Rate: 7.087773337773338e-05\n",
      "Batch 7,600 of 9,631 Elased 0:53:53. Training loss: 3.71535260672632. Learning Rate: 7.086038961038961e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7,650 of 9,631 Elased 0:54:15. Training loss: 3.7153032063347062. Learning Rate: 7.084304584304584e-05\n",
      "Batch 7,700 of 9,631 Elased 0:54:37. Training loss: 3.7139472557817186. Learning Rate: 7.082570207570209e-05\n",
      "Batch 7,750 of 9,631 Elased 0:54:59. Training loss: 3.712175620525114. Learning Rate: 7.080835830835832e-05\n",
      "Batch 7,800 of 9,631 Elased 0:55:20. Training loss: 3.712551409541032. Learning Rate: 7.079101454101454e-05\n",
      "Batch 7,850 of 9,631 Elased 0:55:41. Training loss: 3.71296097067511. Learning Rate: 7.077367077367077e-05\n",
      "Batch 7,900 of 9,631 Elased 0:56:03. Training loss: 3.711317867704585. Learning Rate: 7.0756327006327e-05\n",
      "Batch 7,950 of 9,631 Elased 0:56:24. Training loss: 3.712574719468003. Learning Rate: 7.073898323898325e-05\n",
      "Batch 8,000 of 9,631 Elased 0:56:45. Training loss: 3.7123409313708544. Learning Rate: 7.072163947163948e-05\n",
      "Batch 8,050 of 9,631 Elased 0:57:07. Training loss: 3.7140782362037563. Learning Rate: 7.07042957042957e-05\n",
      "Batch 8,100 of 9,631 Elased 0:57:28. Training loss: 3.7141154793015234. Learning Rate: 7.068695193695193e-05\n",
      "Batch 8,150 of 9,631 Elased 0:57:49. Training loss: 3.7150414491577384. Learning Rate: 7.066960816960818e-05\n",
      "Batch 8,200 of 9,631 Elased 0:58:10. Training loss: 3.714692117397378. Learning Rate: 7.06522644022644e-05\n",
      "Batch 8,250 of 9,631 Elased 0:58:32. Training loss: 3.7154259269743255. Learning Rate: 7.063492063492065e-05\n",
      "Batch 8,300 of 9,631 Elased 0:58:52. Training loss: 3.7154859387874604. Learning Rate: 7.061757686757688e-05\n",
      "Batch 8,350 of 9,631 Elased 0:59:13. Training loss: 3.7152357055898197. Learning Rate: 7.060023310023311e-05\n",
      "Batch 8,400 of 9,631 Elased 0:59:34. Training loss: 3.7146837464826445. Learning Rate: 7.058288933288934e-05\n",
      "Batch 8,450 of 9,631 Elased 0:59:56. Training loss: 3.71391163629893. Learning Rate: 7.056554556554557e-05\n",
      "Batch 8,500 of 9,631 Elased 1:00:16. Training loss: 3.7134156135531033. Learning Rate: 7.05482017982018e-05\n",
      "Batch 8,550 of 9,631 Elased 1:00:38. Training loss: 3.7129690058328952. Learning Rate: 7.053085803085804e-05\n",
      "Batch 8,600 of 9,631 Elased 1:00:59. Training loss: 3.713665361168773. Learning Rate: 7.051351426351427e-05\n",
      "Batch 8,650 of 9,631 Elased 1:01:20. Training loss: 3.7124692563652304. Learning Rate: 7.04961704961705e-05\n",
      "Batch 8,700 of 9,631 Elased 1:01:41. Training loss: 3.7121496297984287. Learning Rate: 7.047882672882673e-05\n",
      "Batch 8,750 of 9,631 Elased 1:02:01. Training loss: 3.7119752736364093. Learning Rate: 7.046148296148296e-05\n",
      "Batch 8,800 of 9,631 Elased 1:02:23. Training loss: 3.7116928477043456. Learning Rate: 7.04441391941392e-05\n",
      "Batch 8,850 of 9,631 Elased 1:02:44. Training loss: 3.7113931869113514. Learning Rate: 7.042679542679543e-05\n",
      "Batch 8,900 of 9,631 Elased 1:03:06. Training loss: 3.7114811683906597. Learning Rate: 7.040945165945167e-05\n",
      "Batch 8,950 of 9,631 Elased 1:03:27. Training loss: 3.7111184907492314. Learning Rate: 7.03921078921079e-05\n",
      "Batch 9,000 of 9,631 Elased 1:03:48. Training loss: 3.7098405257463454. Learning Rate: 7.037476412476413e-05\n",
      "Batch 9,050 of 9,631 Elased 1:04:09. Training loss: 3.7105668395263716. Learning Rate: 7.035742035742036e-05\n",
      "Batch 9,100 of 9,631 Elased 1:04:31. Training loss: 3.7102792122730843. Learning Rate: 7.03400765900766e-05\n",
      "Batch 9,150 of 9,631 Elased 1:04:52. Training loss: 3.7104165989844526. Learning Rate: 7.032273282273283e-05\n",
      "Batch 9,200 of 9,631 Elased 1:05:13. Training loss: 3.7110679998216423. Learning Rate: 7.030538905538906e-05\n",
      "Batch 9,250 of 9,631 Elased 1:05:34. Training loss: 3.7107119711154217. Learning Rate: 7.028804528804529e-05\n",
      "Batch 9,300 of 9,631 Elased 1:05:56. Training loss: 3.7111794616970966. Learning Rate: 7.027070152070152e-05\n",
      "Batch 9,350 of 9,631 Elased 1:06:17. Training loss: 3.711229924811399. Learning Rate: 7.025335775335775e-05\n",
      "Batch 9,400 of 9,631 Elased 1:06:38. Training loss: 3.711175044559418. Learning Rate: 7.023601398601399e-05\n",
      "Batch 9,450 of 9,631 Elased 1:06:59. Training loss: 3.710699228748443. Learning Rate: 7.021867021867022e-05\n",
      "Batch 9,500 of 9,631 Elased 1:07:20. Training loss: 3.7100705033477985. Learning Rate: 7.020132645132645e-05\n",
      "Batch 9,550 of 9,631 Elased 1:07:41. Training loss: 3.709559582927464. Learning Rate: 7.018398268398269e-05\n",
      "Batch 9,600 of 9,631 Elased 1:08:02. Training loss: 3.7098296756421525. Learning Rate: 7.016663891663892e-05\n",
      "\n",
      "\n",
      "  Average training loss: 3.71\n",
      "  Training epcoh took: 1:08:16\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ac82a4401f40eab2489ca0efef96d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=67.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Validation Loss: 3.69\n",
      "  Validation took: 0:00:43\n",
      "\n",
      "======== Epoch 10 / 30 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb226feee9c7431db992b0b3e6ed0e7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch    50 of 9,631 Elased 0:00:22. Training loss: 3.723969554901123. Learning Rate: 7.013854201354202e-05\n",
      "Batch   100 of 9,631 Elased 0:00:42. Training loss: 3.6807829427719114. Learning Rate: 7.012119824619825e-05\n",
      "Batch   150 of 9,631 Elased 0:01:03. Training loss: 3.681699775060018. Learning Rate: 7.010385447885448e-05\n",
      "Batch   200 of 9,631 Elased 0:01:24. Training loss: 3.7191347682476046. Learning Rate: 7.008651071151071e-05\n",
      "Batch   250 of 9,631 Elased 0:01:45. Training loss: 3.668359393119812. Learning Rate: 7.006916694416696e-05\n",
      "Batch   300 of 9,631 Elased 0:02:06. Training loss: 3.6476801935831706. Learning Rate: 7.005182317682318e-05\n",
      "Batch   350 of 9,631 Elased 0:02:27. Training loss: 3.6393625048228673. Learning Rate: 7.003447940947941e-05\n",
      "Batch   400 of 9,631 Elased 0:02:48. Training loss: 3.65860845208168. Learning Rate: 7.001713564213564e-05\n",
      "Batch   450 of 9,631 Elased 0:03:10. Training loss: 3.6595590103997124. Learning Rate: 6.999979187479187e-05\n",
      "Batch   500 of 9,631 Elased 0:03:31. Training loss: 3.666398437976837. Learning Rate: 6.99824481074481e-05\n",
      "Batch   550 of 9,631 Elased 0:03:52. Training loss: 3.65392025644129. Learning Rate: 6.996510434010434e-05\n",
      "Batch   600 of 9,631 Elased 0:04:13. Training loss: 3.667211074034373. Learning Rate: 6.994776057276057e-05\n",
      "Batch   650 of 9,631 Elased 0:04:34. Training loss: 3.6671642380494336. Learning Rate: 6.99304168054168e-05\n",
      "Batch   700 of 9,631 Elased 0:04:56. Training loss: 3.6642525948796956. Learning Rate: 6.991307303807305e-05\n",
      "Batch   750 of 9,631 Elased 0:05:17. Training loss: 3.6511095355351766. Learning Rate: 6.989572927072928e-05\n",
      "Batch   800 of 9,631 Elased 0:05:38. Training loss: 3.651086269021034. Learning Rate: 6.98783855033855e-05\n",
      "Batch   850 of 9,631 Elased 0:05:59. Training loss: 3.643042158239028. Learning Rate: 6.986104173604175e-05\n",
      "Batch   900 of 9,631 Elased 0:06:21. Training loss: 3.6420897470580207. Learning Rate: 6.984369796869798e-05\n",
      "Batch   950 of 9,631 Elased 0:06:42. Training loss: 3.6392484383834036. Learning Rate: 6.98263542013542e-05\n",
      "Batch 1,000 of 9,631 Elased 0:07:03. Training loss: 3.6401682889461515. Learning Rate: 6.980901043401044e-05\n",
      "Batch 1,050 of 9,631 Elased 0:07:24. Training loss: 3.642657880783081. Learning Rate: 6.979166666666666e-05\n",
      "Batch 1,100 of 9,631 Elased 0:07:45. Training loss: 3.6440692069313743. Learning Rate: 6.977432289932291e-05\n",
      "Batch 1,150 of 9,631 Elased 0:08:07. Training loss: 3.643906727251799. Learning Rate: 6.975697913197914e-05\n",
      "Batch 1,200 of 9,631 Elased 0:08:28. Training loss: 3.646703997850418. Learning Rate: 6.973963536463537e-05\n",
      "Batch 1,250 of 9,631 Elased 0:08:50. Training loss: 3.648413774108887. Learning Rate: 6.97222915972916e-05\n",
      "Batch 1,300 of 9,631 Elased 0:09:11. Training loss: 3.644634840305035. Learning Rate: 6.970494782994782e-05\n",
      "Batch 1,350 of 9,631 Elased 0:09:32. Training loss: 3.643351863136998. Learning Rate: 6.968760406260407e-05\n",
      "Batch 1,400 of 9,631 Elased 0:09:54. Training loss: 3.6419596328905652. Learning Rate: 6.967026029526031e-05\n",
      "Batch 1,450 of 9,631 Elased 0:10:15. Training loss: 3.6394829726219178. Learning Rate: 6.965291652791654e-05\n",
      "Batch 1,500 of 9,631 Elased 0:10:36. Training loss: 3.6393478666146595. Learning Rate: 6.963557276057277e-05\n",
      "Batch 1,550 of 9,631 Elased 0:10:57. Training loss: 3.6392576936752565. Learning Rate: 6.9618228993229e-05\n",
      "Batch 1,600 of 9,631 Elased 0:11:18. Training loss: 3.6444477731734515. Learning Rate: 6.960088522588523e-05\n",
      "Batch 1,650 of 9,631 Elased 0:11:40. Training loss: 3.648075080423644. Learning Rate: 6.958354145854146e-05\n",
      "Batch 1,700 of 9,631 Elased 0:12:01. Training loss: 3.6456483772922965. Learning Rate: 6.95661976911977e-05\n",
      "Batch 1,750 of 9,631 Elased 0:12:22. Training loss: 3.6465930341311865. Learning Rate: 6.954885392385393e-05\n",
      "Batch 1,800 of 9,631 Elased 0:12:43. Training loss: 3.6460879733165106. Learning Rate: 6.953151015651016e-05\n",
      "Batch 1,850 of 9,631 Elased 0:13:05. Training loss: 3.6492868389310065. Learning Rate: 6.951416638916639e-05\n",
      "Batch 1,900 of 9,631 Elased 0:13:26. Training loss: 3.648128595038464. Learning Rate: 6.949682262182262e-05\n",
      "Batch 1,950 of 9,631 Elased 0:13:47. Training loss: 3.6442457254727683. Learning Rate: 6.947947885447886e-05\n",
      "Batch 2,000 of 9,631 Elased 0:14:08. Training loss: 3.639722173035145. Learning Rate: 6.946213508713509e-05\n",
      "Batch 2,050 of 9,631 Elased 0:14:29. Training loss: 3.640497772577332. Learning Rate: 6.944479131979133e-05\n",
      "Batch 2,100 of 9,631 Elased 0:14:50. Training loss: 3.6454500323817842. Learning Rate: 6.942744755244756e-05\n",
      "Batch 2,150 of 9,631 Elased 0:15:12. Training loss: 3.645306555670361. Learning Rate: 6.941010378510379e-05\n",
      "Batch 2,200 of 9,631 Elased 0:15:33. Training loss: 3.645870682272044. Learning Rate: 6.939276001776002e-05\n",
      "Batch 2,250 of 9,631 Elased 0:15:55. Training loss: 3.6437987528377107. Learning Rate: 6.937541625041626e-05\n",
      "Batch 2,300 of 9,631 Elased 0:16:16. Training loss: 3.6449699353653453. Learning Rate: 6.935807248307249e-05\n",
      "Batch 2,350 of 9,631 Elased 0:16:37. Training loss: 3.6423140512121486. Learning Rate: 6.934072871572872e-05\n",
      "Batch 2,400 of 9,631 Elased 0:16:58. Training loss: 3.6426043817897638. Learning Rate: 6.932338494838495e-05\n",
      "Batch 2,450 of 9,631 Elased 0:17:19. Training loss: 3.6393520201955525. Learning Rate: 6.930604118104118e-05\n",
      "Batch 2,500 of 9,631 Elased 0:17:41. Training loss: 3.6369104747295378. Learning Rate: 6.928869741369741e-05\n",
      "Batch 2,550 of 9,631 Elased 0:18:02. Training loss: 3.642485722981247. Learning Rate: 6.927135364635365e-05\n",
      "Batch 2,600 of 9,631 Elased 0:18:23. Training loss: 3.645778405712201. Learning Rate: 6.925400987900988e-05\n",
      "Batch 2,650 of 9,631 Elased 0:18:44. Training loss: 3.6475789990964924. Learning Rate: 6.923666611166611e-05\n",
      "Batch 2,700 of 9,631 Elased 0:19:06. Training loss: 3.6502077243504703. Learning Rate: 6.921932234432235e-05\n",
      "Batch 2,750 of 9,631 Elased 0:19:27. Training loss: 3.6471454569643194. Learning Rate: 6.920197857697858e-05\n",
      "Batch 2,800 of 9,631 Elased 0:19:48. Training loss: 3.6488004131402287. Learning Rate: 6.918463480963482e-05\n",
      "Batch 2,850 of 9,631 Elased 0:20:09. Training loss: 3.6513848933420685. Learning Rate: 6.916729104229105e-05\n",
      "Batch 2,900 of 9,631 Elased 0:20:31. Training loss: 3.651251858226184. Learning Rate: 6.914994727494728e-05\n",
      "Batch 2,950 of 9,631 Elased 0:20:52. Training loss: 3.6520069575309755. Learning Rate: 6.913260350760351e-05\n",
      "Batch 3,000 of 9,631 Elased 0:21:13. Training loss: 3.655040345231692. Learning Rate: 6.911525974025974e-05\n",
      "Batch 3,050 of 9,631 Elased 0:21:34. Training loss: 3.6545004961529717. Learning Rate: 6.909791597291597e-05\n",
      "Batch 3,100 of 9,631 Elased 0:21:55. Training loss: 3.6545904592929346. Learning Rate: 6.908057220557221e-05\n",
      "Batch 3,150 of 9,631 Elased 0:22:17. Training loss: 3.6504296031830803. Learning Rate: 6.906322843822844e-05\n",
      "Batch 3,200 of 9,631 Elased 0:22:39. Training loss: 3.6526101602613927. Learning Rate: 6.904588467088467e-05\n",
      "Batch 3,250 of 9,631 Elased 0:23:00. Training loss: 3.652565090399522. Learning Rate: 6.90285409035409e-05\n",
      "Batch 3,300 of 9,631 Elased 0:23:22. Training loss: 3.655044149846742. Learning Rate: 6.901119713619713e-05\n",
      "Batch 3,350 of 9,631 Elased 0:23:43. Training loss: 3.6555551199414835. Learning Rate: 6.899385336885337e-05\n",
      "Batch 3,400 of 9,631 Elased 0:24:04. Training loss: 3.6562539491933936. Learning Rate: 6.89765096015096e-05\n",
      "Batch 3,450 of 9,631 Elased 0:24:26. Training loss: 3.657099251885345. Learning Rate: 6.895916583416585e-05\n",
      "Batch 3,500 of 9,631 Elased 0:24:47. Training loss: 3.6559631690979004. Learning Rate: 6.894182206682207e-05\n",
      "Batch 3,550 of 9,631 Elased 0:25:08. Training loss: 3.656938329683223. Learning Rate: 6.89244782994783e-05\n",
      "Batch 3,600 of 9,631 Elased 0:25:29. Training loss: 3.658673994474941. Learning Rate: 6.890713453213453e-05\n",
      "Batch 3,650 of 9,631 Elased 0:25:51. Training loss: 3.6624315604771653. Learning Rate: 6.888979076479076e-05\n",
      "Batch 3,700 of 9,631 Elased 0:26:13. Training loss: 3.6608320876069973. Learning Rate: 6.8872446997447e-05\n",
      "Batch 3,750 of 9,631 Elased 0:26:34. Training loss: 3.6595624510447182. Learning Rate: 6.885510323010323e-05\n",
      "Batch 3,800 of 9,631 Elased 0:26:55. Training loss: 3.6582834876211066. Learning Rate: 6.883775946275946e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3,850 of 9,631 Elased 0:27:17. Training loss: 3.6554484690629043. Learning Rate: 6.88204156954157e-05\n",
      "Batch 3,900 of 9,631 Elased 0:27:38. Training loss: 3.6537560194577927. Learning Rate: 6.880307192807192e-05\n",
      "Batch 3,950 of 9,631 Elased 0:27:59. Training loss: 3.6549437799333018. Learning Rate: 6.878572816072817e-05\n",
      "Batch 4,000 of 9,631 Elased 0:28:20. Training loss: 3.65541433429718. Learning Rate: 6.87683843933844e-05\n",
      "Batch 4,050 of 9,631 Elased 0:28:41. Training loss: 3.6546654303279924. Learning Rate: 6.875104062604062e-05\n",
      "Batch 4,100 of 9,631 Elased 0:29:02. Training loss: 3.653849712232264. Learning Rate: 6.873369685869687e-05\n",
      "Batch 4,150 of 9,631 Elased 0:29:23. Training loss: 3.6537888469466244. Learning Rate: 6.87163530913531e-05\n",
      "Batch 4,200 of 9,631 Elased 0:29:45. Training loss: 3.653860879341761. Learning Rate: 6.869900932400933e-05\n",
      "Batch 4,250 of 9,631 Elased 0:30:06. Training loss: 3.6543457195057587. Learning Rate: 6.868166555666557e-05\n",
      "Batch 4,300 of 9,631 Elased 0:30:28. Training loss: 3.654078482361727. Learning Rate: 6.86643217893218e-05\n",
      "Batch 4,350 of 9,631 Elased 0:30:49. Training loss: 3.653561381964848. Learning Rate: 6.864697802197803e-05\n",
      "Batch 4,400 of 9,631 Elased 0:31:10. Training loss: 3.6547281337868083. Learning Rate: 6.862963425463426e-05\n",
      "Batch 4,450 of 9,631 Elased 0:31:31. Training loss: 3.655369691580869. Learning Rate: 6.861229048729049e-05\n",
      "Batch 4,500 of 9,631 Elased 0:31:52. Training loss: 3.656286703851488. Learning Rate: 6.859494671994671e-05\n",
      "Batch 4,550 of 9,631 Elased 0:32:14. Training loss: 3.654509737255809. Learning Rate: 6.857760295260296e-05\n",
      "Batch 4,600 of 9,631 Elased 0:32:35. Training loss: 3.65616980402366. Learning Rate: 6.856025918525919e-05\n",
      "Batch 4,650 of 9,631 Elased 0:32:56. Training loss: 3.6547495569721344. Learning Rate: 6.854291541791542e-05\n",
      "Batch 4,700 of 9,631 Elased 0:33:17. Training loss: 3.6552621453366383. Learning Rate: 6.852557165057165e-05\n",
      "Batch 4,750 of 9,631 Elased 0:33:39. Training loss: 3.655282052391454. Learning Rate: 6.850822788322789e-05\n",
      "Batch 4,800 of 9,631 Elased 0:34:00. Training loss: 3.65343061765035. Learning Rate: 6.849088411588413e-05\n",
      "Batch 4,850 of 9,631 Elased 0:34:21. Training loss: 3.6520448786942. Learning Rate: 6.847354034854036e-05\n",
      "Batch 4,900 of 9,631 Elased 0:34:43. Training loss: 3.6524693076464594. Learning Rate: 6.845619658119659e-05\n",
      "Batch 4,950 of 9,631 Elased 0:35:04. Training loss: 3.652525762403854. Learning Rate: 6.843885281385282e-05\n",
      "Batch 5,000 of 9,631 Elased 0:35:25. Training loss: 3.6537171921253204. Learning Rate: 6.842150904650905e-05\n",
      "Batch 5,050 of 9,631 Elased 0:35:46. Training loss: 3.6536286827597286. Learning Rate: 6.840416527916528e-05\n",
      "Batch 5,100 of 9,631 Elased 0:36:07. Training loss: 3.6515921487060248. Learning Rate: 6.838682151182152e-05\n",
      "Batch 5,150 of 9,631 Elased 0:36:29. Training loss: 3.6525658401933687. Learning Rate: 6.836947774447775e-05\n",
      "Batch 5,200 of 9,631 Elased 0:36:50. Training loss: 3.652511881681589. Learning Rate: 6.835213397713398e-05\n",
      "Batch 5,250 of 9,631 Elased 0:37:11. Training loss: 3.6524604771023705. Learning Rate: 6.833479020979021e-05\n",
      "Batch 5,300 of 9,631 Elased 0:37:32. Training loss: 3.6516616875720476. Learning Rate: 6.831744644244644e-05\n",
      "Batch 5,350 of 9,631 Elased 0:37:54. Training loss: 3.6537248020974276. Learning Rate: 6.830010267510267e-05\n",
      "Batch 5,400 of 9,631 Elased 0:38:15. Training loss: 3.6537428095605637. Learning Rate: 6.828275890775891e-05\n",
      "Batch 5,450 of 9,631 Elased 0:38:36. Training loss: 3.653696040538473. Learning Rate: 6.826541514041515e-05\n",
      "Batch 5,500 of 9,631 Elased 0:38:57. Training loss: 3.6537226705984636. Learning Rate: 6.824807137307138e-05\n",
      "Batch 5,550 of 9,631 Elased 0:39:18. Training loss: 3.6547282753334387. Learning Rate: 6.823072760572761e-05\n",
      "Batch 5,600 of 9,631 Elased 0:39:39. Training loss: 3.653898678932871. Learning Rate: 6.821338383838384e-05\n",
      "Batch 5,650 of 9,631 Elased 0:40:00. Training loss: 3.6560071597057107. Learning Rate: 6.819604007104008e-05\n",
      "Batch 5,700 of 9,631 Elased 0:40:21. Training loss: 3.6559249045556053. Learning Rate: 6.817869630369631e-05\n",
      "Batch 5,750 of 9,631 Elased 0:40:42. Training loss: 3.6551385729001917. Learning Rate: 6.816135253635254e-05\n",
      "Batch 5,800 of 9,631 Elased 0:41:03. Training loss: 3.65515444463697. Learning Rate: 6.814400876900877e-05\n",
      "Batch 5,850 of 9,631 Elased 0:41:25. Training loss: 3.6554547097947863. Learning Rate: 6.8126665001665e-05\n",
      "Batch 5,900 of 9,631 Elased 0:41:46. Training loss: 3.6550320331524992. Learning Rate: 6.810932123432123e-05\n",
      "Batch 5,950 of 9,631 Elased 0:42:07. Training loss: 3.655342204510665. Learning Rate: 6.809197746697747e-05\n",
      "Batch 6,000 of 9,631 Elased 0:42:29. Training loss: 3.6548004550536475. Learning Rate: 6.80746336996337e-05\n",
      "Batch 6,050 of 9,631 Elased 0:42:50. Training loss: 3.6547701119588427. Learning Rate: 6.805728993228993e-05\n",
      "Batch 6,100 of 9,631 Elased 0:43:11. Training loss: 3.6549071675832154. Learning Rate: 6.803994616494617e-05\n",
      "Batch 6,150 of 9,631 Elased 0:43:32. Training loss: 3.6543139906627378. Learning Rate: 6.80226023976024e-05\n",
      "Batch 6,200 of 9,631 Elased 0:43:54. Training loss: 3.652282610131848. Learning Rate: 6.800525863025863e-05\n",
      "Batch 6,250 of 9,631 Elased 0:44:14. Training loss: 3.6517157527923585. Learning Rate: 6.798791486291487e-05\n",
      "Batch 6,300 of 9,631 Elased 0:44:36. Training loss: 3.6519083347396246. Learning Rate: 6.79705710955711e-05\n",
      "Batch 6,350 of 9,631 Elased 0:44:57. Training loss: 3.651393736216027. Learning Rate: 6.795322732822733e-05\n",
      "Batch 6,400 of 9,631 Elased 0:45:19. Training loss: 3.6505836168304087. Learning Rate: 6.793588356088356e-05\n",
      "Batch 6,450 of 9,631 Elased 0:45:40. Training loss: 3.6519860200364462. Learning Rate: 6.791853979353979e-05\n",
      "Batch 6,500 of 9,631 Elased 0:46:01. Training loss: 3.6500824746351976. Learning Rate: 6.790119602619603e-05\n",
      "Batch 6,550 of 9,631 Elased 0:46:22. Training loss: 3.6503968412457533. Learning Rate: 6.788385225885226e-05\n",
      "Batch 6,600 of 9,631 Elased 0:46:43. Training loss: 3.651157100056157. Learning Rate: 6.786650849150849e-05\n",
      "Batch 6,650 of 9,631 Elased 0:47:04. Training loss: 3.651423431984464. Learning Rate: 6.784916472416472e-05\n",
      "Batch 6,700 of 9,631 Elased 0:47:26. Training loss: 3.652737395087285. Learning Rate: 6.783182095682095e-05\n",
      "Batch 6,750 of 9,631 Elased 0:47:47. Training loss: 3.652389457737958. Learning Rate: 6.78144771894772e-05\n",
      "Batch 6,800 of 9,631 Elased 0:48:08. Training loss: 3.651943962188328. Learning Rate: 6.779713342213342e-05\n",
      "Batch 6,850 of 9,631 Elased 0:48:29. Training loss: 3.651056314141211. Learning Rate: 6.777978965478967e-05\n",
      "Batch 6,900 of 9,631 Elased 0:48:51. Training loss: 3.6510481946710227. Learning Rate: 6.77624458874459e-05\n",
      "Batch 6,950 of 9,631 Elased 0:49:12. Training loss: 3.650773002157966. Learning Rate: 6.774510212010212e-05\n",
      "Batch 7,000 of 9,631 Elased 0:49:33. Training loss: 3.649902070726667. Learning Rate: 6.772775835275835e-05\n",
      "Batch 7,050 of 9,631 Elased 0:49:54. Training loss: 3.649274413467299. Learning Rate: 6.771041458541458e-05\n",
      "Batch 7,100 of 9,631 Elased 0:50:16. Training loss: 3.6492702321939063. Learning Rate: 6.769307081807083e-05\n",
      "Batch 7,150 of 9,631 Elased 0:50:37. Training loss: 3.648898065640376. Learning Rate: 6.767572705072706e-05\n",
      "Batch 7,200 of 9,631 Elased 0:50:58. Training loss: 3.649189147684309. Learning Rate: 6.765838328338328e-05\n",
      "Batch 7,250 of 9,631 Elased 0:51:19. Training loss: 3.6478985358928813. Learning Rate: 6.764103951603951e-05\n",
      "Batch 7,300 of 9,631 Elased 0:51:41. Training loss: 3.6481801252822352. Learning Rate: 6.762369574869574e-05\n",
      "Batch 7,350 of 9,631 Elased 0:52:02. Training loss: 3.647548114718223. Learning Rate: 6.760635198135199e-05\n",
      "Batch 7,400 of 9,631 Elased 0:52:23. Training loss: 3.6461380159210512. Learning Rate: 6.758900821400822e-05\n",
      "Batch 7,450 of 9,631 Elased 0:52:44. Training loss: 3.6459935454234182. Learning Rate: 6.757166444666444e-05\n",
      "Batch 7,500 of 9,631 Elased 0:53:06. Training loss: 3.6446830670992534. Learning Rate: 6.755432067932069e-05\n",
      "Batch 7,550 of 9,631 Elased 0:53:27. Training loss: 3.6446188823118906. Learning Rate: 6.753697691197692e-05\n",
      "Batch 7,600 of 9,631 Elased 0:53:48. Training loss: 3.6449793525118577. Learning Rate: 6.751963314463315e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7,650 of 9,631 Elased 0:54:09. Training loss: 3.6449026274213603. Learning Rate: 6.750228937728939e-05\n",
      "Batch 7,700 of 9,631 Elased 0:54:30. Training loss: 3.643865506772871. Learning Rate: 6.748494560994562e-05\n",
      "Batch 7,750 of 9,631 Elased 0:54:52. Training loss: 3.642090999049525. Learning Rate: 6.746760184260185e-05\n",
      "Batch 7,800 of 9,631 Elased 0:55:13. Training loss: 3.642450307424252. Learning Rate: 6.745025807525808e-05\n",
      "Batch 7,850 of 9,631 Elased 0:55:34. Training loss: 3.6426258465894468. Learning Rate: 6.74329143079143e-05\n",
      "Batch 7,900 of 9,631 Elased 0:55:55. Training loss: 3.6411602893962134. Learning Rate: 6.741557054057054e-05\n",
      "Batch 7,950 of 9,631 Elased 0:56:16. Training loss: 3.6423743544284655. Learning Rate: 6.739822677322678e-05\n",
      "Batch 8,000 of 9,631 Elased 0:56:38. Training loss: 3.6419720200002192. Learning Rate: 6.738088300588301e-05\n",
      "Batch 8,050 of 9,631 Elased 0:56:59. Training loss: 3.643152409251432. Learning Rate: 6.736353923853924e-05\n",
      "Batch 8,100 of 9,631 Elased 0:57:20. Training loss: 3.643181557861375. Learning Rate: 6.734619547119547e-05\n",
      "Batch 8,150 of 9,631 Elased 0:57:41. Training loss: 3.6440123319918394. Learning Rate: 6.732885170385171e-05\n",
      "Batch 8,200 of 9,631 Elased 0:58:03. Training loss: 3.6434585996371944. Learning Rate: 6.731150793650795e-05\n",
      "Batch 8,250 of 9,631 Elased 0:58:23. Training loss: 3.6443300215114247. Learning Rate: 6.729416416916418e-05\n",
      "Batch 8,300 of 9,631 Elased 0:58:44. Training loss: 3.6445872248224465. Learning Rate: 6.727682040182041e-05\n",
      "Batch 8,350 of 9,631 Elased 0:59:05. Training loss: 3.6441160268840678. Learning Rate: 6.725947663447664e-05\n",
      "Batch 8,400 of 9,631 Elased 0:59:26. Training loss: 3.643538424003692. Learning Rate: 6.724213286713287e-05\n",
      "Batch 8,450 of 9,631 Elased 0:59:47. Training loss: 3.6427988685799773. Learning Rate: 6.72247890997891e-05\n",
      "Batch 8,500 of 9,631 Elased 1:00:09. Training loss: 3.642113602217506. Learning Rate: 6.720744533244534e-05\n",
      "Batch 8,550 of 9,631 Elased 1:00:30. Training loss: 3.641759407311155. Learning Rate: 6.719010156510157e-05\n",
      "Batch 8,600 of 9,631 Elased 1:00:51. Training loss: 3.642488054286602. Learning Rate: 6.71727577977578e-05\n",
      "Batch 8,650 of 9,631 Elased 1:01:12. Training loss: 3.6411741917946436. Learning Rate: 6.715541403041403e-05\n",
      "Batch 8,700 of 9,631 Elased 1:01:34. Training loss: 3.640901398932797. Learning Rate: 6.713807026307026e-05\n",
      "Batch 8,750 of 9,631 Elased 1:01:55. Training loss: 3.640759957449777. Learning Rate: 6.71207264957265e-05\n",
      "Batch 8,800 of 9,631 Elased 1:02:16. Training loss: 3.640484809279442. Learning Rate: 6.710338272838273e-05\n",
      "Batch 8,850 of 9,631 Elased 1:02:37. Training loss: 3.6401761390125684. Learning Rate: 6.708603896103897e-05\n",
      "Batch 8,900 of 9,631 Elased 1:02:59. Training loss: 3.6403567410586923. Learning Rate: 6.70686951936952e-05\n",
      "Batch 8,950 of 9,631 Elased 1:03:20. Training loss: 3.6398954288653154. Learning Rate: 6.705135142635143e-05\n",
      "Batch 9,000 of 9,631 Elased 1:03:41. Training loss: 3.638778151035309. Learning Rate: 6.703400765900766e-05\n",
      "Batch 9,050 of 9,631 Elased 1:04:03. Training loss: 3.6395447896989013. Learning Rate: 6.70166638916639e-05\n",
      "Batch 9,100 of 9,631 Elased 1:04:24. Training loss: 3.6393627917111573. Learning Rate: 6.699932012432013e-05\n",
      "Batch 9,150 of 9,631 Elased 1:04:45. Training loss: 3.6394577520662317. Learning Rate: 6.698197635697636e-05\n",
      "Batch 9,200 of 9,631 Elased 1:05:06. Training loss: 3.6399145676001257. Learning Rate: 6.696463258963259e-05\n",
      "Batch 9,250 of 9,631 Elased 1:05:28. Training loss: 3.639704584456779. Learning Rate: 6.694728882228882e-05\n",
      "Batch 9,300 of 9,631 Elased 1:05:49. Training loss: 3.6400185522212776. Learning Rate: 6.692994505494505e-05\n",
      "Batch 9,350 of 9,631 Elased 1:06:11. Training loss: 3.6399973547522397. Learning Rate: 6.691260128760129e-05\n",
      "Batch 9,400 of 9,631 Elased 1:06:33. Training loss: 3.639937102021055. Learning Rate: 6.689525752025752e-05\n",
      "Batch 9,450 of 9,631 Elased 1:06:55. Training loss: 3.6393596166151543. Learning Rate: 6.687791375291375e-05\n",
      "Batch 9,500 of 9,631 Elased 1:07:17. Training loss: 3.6389423595353176. Learning Rate: 6.686056998557e-05\n",
      "Batch 9,550 of 9,631 Elased 1:07:38. Training loss: 3.638158087468272. Learning Rate: 6.684322621822622e-05\n",
      "Batch 9,600 of 9,631 Elased 1:08:01. Training loss: 3.6387802150721353. Learning Rate: 6.682588245088245e-05\n",
      "\n",
      "\n",
      "  Average training loss: 3.64\n",
      "  Training epcoh took: 1:08:14\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94be022c57214f5e82552504a119aeb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=67.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Validation Loss: 3.64\n",
      "  Validation took: 0:00:44\n",
      "\n",
      "======== Epoch 11 / 30 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2191d43ce144009b98fa349400df04f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch    50 of 9,631 Elased 0:00:21. Training loss: 3.6506263875961302. Learning Rate: 6.679778554778556e-05\n",
      "Batch   100 of 9,631 Elased 0:00:43. Training loss: 3.5978845715522767. Learning Rate: 6.678044178044179e-05\n",
      "Batch   150 of 9,631 Elased 0:01:04. Training loss: 3.6080456527074176. Learning Rate: 6.676309801309802e-05\n",
      "Batch   200 of 9,631 Elased 0:01:25. Training loss: 3.6597904992103576. Learning Rate: 6.674575424575424e-05\n",
      "Batch   250 of 9,631 Elased 0:01:46. Training loss: 3.610936902999878. Learning Rate: 6.672841047841049e-05\n",
      "Batch   300 of 9,631 Elased 0:02:08. Training loss: 3.5953888114293417. Learning Rate: 6.671106671106672e-05\n",
      "Batch   350 of 9,631 Elased 0:02:29. Training loss: 3.590721881730216. Learning Rate: 6.669372294372295e-05\n",
      "Batch   400 of 9,631 Elased 0:02:51. Training loss: 3.6063717150688173. Learning Rate: 6.667637917637917e-05\n",
      "Batch   450 of 9,631 Elased 0:03:11. Training loss: 3.606941447787815. Learning Rate: 6.66590354090354e-05\n",
      "Batch   500 of 9,631 Elased 0:03:32. Training loss: 3.6163661608695983. Learning Rate: 6.664169164169165e-05\n",
      "Batch   550 of 9,631 Elased 0:03:53. Training loss: 3.6059230852127073. Learning Rate: 6.662434787434788e-05\n",
      "Batch   600 of 9,631 Elased 0:04:14. Training loss: 3.618706039985021. Learning Rate: 6.66070041070041e-05\n",
      "Batch   650 of 9,631 Elased 0:04:36. Training loss: 3.61947546848884. Learning Rate: 6.658966033966035e-05\n",
      "Batch   700 of 9,631 Elased 0:04:57. Training loss: 3.6114537644386293. Learning Rate: 6.657231657231658e-05\n",
      "Batch   750 of 9,631 Elased 0:05:18. Training loss: 3.599810846010844. Learning Rate: 6.655497280497281e-05\n",
      "Batch   800 of 9,631 Elased 0:05:39. Training loss: 3.598026094734669. Learning Rate: 6.653762903762905e-05\n",
      "Batch   850 of 9,631 Elased 0:06:00. Training loss: 3.589654990645016. Learning Rate: 6.652028527028528e-05\n",
      "Batch   900 of 9,631 Elased 0:06:21. Training loss: 3.5866995713445875. Learning Rate: 6.650294150294151e-05\n",
      "Batch   950 of 9,631 Elased 0:06:43. Training loss: 3.5830276551999543. Learning Rate: 6.648559773559774e-05\n",
      "Batch 1,000 of 9,631 Elased 0:07:04. Training loss: 3.583913442850113. Learning Rate: 6.646825396825397e-05\n",
      "Batch 1,050 of 9,631 Elased 0:07:25. Training loss: 3.5861899880000525. Learning Rate: 6.64509102009102e-05\n",
      "Batch 1,100 of 9,631 Elased 0:07:46. Training loss: 3.5881438480723986. Learning Rate: 6.643356643356644e-05\n",
      "Batch 1,150 of 9,631 Elased 0:08:08. Training loss: 3.5882708481083747. Learning Rate: 6.641622266622267e-05\n",
      "Batch 1,200 of 9,631 Elased 0:08:29. Training loss: 3.5899830812215807. Learning Rate: 6.63988788988789e-05\n",
      "Batch 1,250 of 9,631 Elased 0:08:50. Training loss: 3.5911040397644043. Learning Rate: 6.638153513153513e-05\n",
      "Batch 1,300 of 9,631 Elased 0:09:10. Training loss: 3.5900826558699976. Learning Rate: 6.636419136419137e-05\n",
      "Batch 1,350 of 9,631 Elased 0:09:31. Training loss: 3.5891556653270014. Learning Rate: 6.63468475968476e-05\n",
      "Batch 1,400 of 9,631 Elased 0:09:52. Training loss: 3.586934561218534. Learning Rate: 6.632950382950384e-05\n",
      "Batch 1,450 of 9,631 Elased 0:10:13. Training loss: 3.5851301363418844. Learning Rate: 6.631216006216007e-05\n",
      "Batch 1,500 of 9,631 Elased 0:10:35. Training loss: 3.5833334755102793. Learning Rate: 6.62948162948163e-05\n",
      "Batch 1,550 of 9,631 Elased 0:10:56. Training loss: 3.5836751380274374. Learning Rate: 6.627747252747253e-05\n",
      "Batch 1,600 of 9,631 Elased 0:11:18. Training loss: 3.589333166107535. Learning Rate: 6.626012876012876e-05\n",
      "Batch 1,650 of 9,631 Elased 0:11:39. Training loss: 3.593261190399979. Learning Rate: 6.6242784992785e-05\n",
      "Batch 1,700 of 9,631 Elased 0:12:00. Training loss: 3.5898872106916766. Learning Rate: 6.622544122544123e-05\n",
      "Batch 1,750 of 9,631 Elased 0:12:22. Training loss: 3.590374501296452. Learning Rate: 6.620809745809746e-05\n",
      "Batch 1,800 of 9,631 Elased 0:12:43. Training loss: 3.588969806763861. Learning Rate: 6.619075369075369e-05\n",
      "Batch 1,850 of 9,631 Elased 0:13:05. Training loss: 3.592143364145949. Learning Rate: 6.617340992340992e-05\n",
      "Batch 1,900 of 9,631 Elased 0:13:26. Training loss: 3.5908165967464445. Learning Rate: 6.615606615606615e-05\n",
      "Batch 1,950 of 9,631 Elased 0:13:47. Training loss: 3.586614572512798. Learning Rate: 6.613872238872239e-05\n",
      "Batch 2,000 of 9,631 Elased 0:14:08. Training loss: 3.5820136728882788. Learning Rate: 6.612137862137863e-05\n",
      "Batch 2,050 of 9,631 Elased 0:14:29. Training loss: 3.5834164973584617. Learning Rate: 6.610403485403486e-05\n",
      "Batch 2,100 of 9,631 Elased 0:14:51. Training loss: 3.586879335074198. Learning Rate: 6.608669108669109e-05\n",
      "Batch 2,150 of 9,631 Elased 0:15:12. Training loss: 3.5877579700669577. Learning Rate: 6.606934731934732e-05\n",
      "Batch 2,200 of 9,631 Elased 0:15:33. Training loss: 3.587169138680805. Learning Rate: 6.605200355200356e-05\n",
      "Batch 2,250 of 9,631 Elased 0:15:55. Training loss: 3.585175560368432. Learning Rate: 6.60346597846598e-05\n",
      "Batch 2,300 of 9,631 Elased 0:16:16. Training loss: 3.587245938518773. Learning Rate: 6.601731601731602e-05\n",
      "Batch 2,350 of 9,631 Elased 0:16:37. Training loss: 3.58487940194759. Learning Rate: 6.599997224997225e-05\n",
      "Batch 2,400 of 9,631 Elased 0:16:59. Training loss: 3.5857906179130077. Learning Rate: 6.598262848262848e-05\n",
      "Batch 2,450 of 9,631 Elased 0:17:20. Training loss: 3.582678341816883. Learning Rate: 6.596528471528471e-05\n",
      "Batch 2,500 of 9,631 Elased 0:17:41. Training loss: 3.5808059574604036. Learning Rate: 6.594794094794095e-05\n",
      "Batch 2,550 of 9,631 Elased 0:18:02. Training loss: 3.585697291832344. Learning Rate: 6.593059718059718e-05\n",
      "Batch 2,600 of 9,631 Elased 0:18:23. Training loss: 3.5890145029929967. Learning Rate: 6.591325341325341e-05\n",
      "Batch 2,650 of 9,631 Elased 0:18:45. Training loss: 3.590370381418264. Learning Rate: 6.589590964590965e-05\n",
      "Batch 2,700 of 9,631 Elased 0:19:06. Training loss: 3.592885189100548. Learning Rate: 6.587856587856588e-05\n",
      "Batch 2,750 of 9,631 Elased 0:19:27. Training loss: 3.59053366578709. Learning Rate: 6.586122211122211e-05\n",
      "Batch 2,800 of 9,631 Elased 0:19:48. Training loss: 3.59169971955674. Learning Rate: 6.584387834387836e-05\n",
      "Batch 2,850 of 9,631 Elased 0:20:10. Training loss: 3.5943989789276793. Learning Rate: 6.582653457653459e-05\n",
      "Batch 2,900 of 9,631 Elased 0:20:31. Training loss: 3.5950229269471663. Learning Rate: 6.580919080919081e-05\n",
      "Batch 2,950 of 9,631 Elased 0:20:52. Training loss: 3.594773039292481. Learning Rate: 6.579184704184704e-05\n",
      "Batch 3,000 of 9,631 Elased 0:21:13. Training loss: 3.5972711502313612. Learning Rate: 6.577450327450327e-05\n",
      "Batch 3,050 of 9,631 Elased 0:21:35. Training loss: 3.597182858459285. Learning Rate: 6.57571595071595e-05\n",
      "Batch 3,100 of 9,631 Elased 0:21:56. Training loss: 3.597425283731953. Learning Rate: 6.573981573981575e-05\n",
      "Batch 3,150 of 9,631 Elased 0:22:17. Training loss: 3.5932801428673757. Learning Rate: 6.572247197247197e-05\n",
      "Batch 3,200 of 9,631 Elased 0:22:39. Training loss: 3.5955758525058625. Learning Rate: 6.57051282051282e-05\n",
      "Batch 3,250 of 9,631 Elased 0:23:00. Training loss: 3.5955546791736896. Learning Rate: 6.568778443778443e-05\n",
      "Batch 3,300 of 9,631 Elased 0:23:21. Training loss: 3.597669238646825. Learning Rate: 6.567044067044068e-05\n",
      "Batch 3,350 of 9,631 Elased 0:23:42. Training loss: 3.597641906773866. Learning Rate: 6.56530969030969e-05\n",
      "Batch 3,400 of 9,631 Elased 0:24:04. Training loss: 3.59808000469909. Learning Rate: 6.563575313575315e-05\n",
      "Batch 3,450 of 9,631 Elased 0:24:25. Training loss: 3.598631334201149. Learning Rate: 6.561840936840938e-05\n",
      "Batch 3,500 of 9,631 Elased 0:24:46. Training loss: 3.5976030458382198. Learning Rate: 6.56010656010656e-05\n",
      "Batch 3,550 of 9,631 Elased 0:25:07. Training loss: 3.5983533128886154. Learning Rate: 6.558372183372184e-05\n",
      "Batch 3,600 of 9,631 Elased 0:25:28. Training loss: 3.599827695588271. Learning Rate: 6.556637806637806e-05\n",
      "Batch 3,650 of 9,631 Elased 0:25:49. Training loss: 3.6033726111176896. Learning Rate: 6.554903429903431e-05\n",
      "Batch 3,700 of 9,631 Elased 0:26:11. Training loss: 3.6024586527089815. Learning Rate: 6.553169053169054e-05\n",
      "Batch 3,750 of 9,631 Elased 0:26:32. Training loss: 3.6013070886294045. Learning Rate: 6.551434676434677e-05\n",
      "Batch 3,800 of 9,631 Elased 0:26:53. Training loss: 3.6000349948594446. Learning Rate: 6.5497002997003e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3,850 of 9,631 Elased 0:27:15. Training loss: 3.5973708097346417. Learning Rate: 6.547965922965922e-05\n",
      "Batch 3,900 of 9,631 Elased 0:27:36. Training loss: 3.5953402654635602. Learning Rate: 6.546231546231545e-05\n",
      "Batch 3,950 of 9,631 Elased 0:27:57. Training loss: 3.5966814837576466. Learning Rate: 6.54449716949717e-05\n",
      "Batch 4,000 of 9,631 Elased 0:28:18. Training loss: 3.5967947419583797. Learning Rate: 6.542762792762793e-05\n",
      "Batch 4,050 of 9,631 Elased 0:28:40. Training loss: 3.5957188985671524. Learning Rate: 6.541028416028417e-05\n",
      "Batch 4,100 of 9,631 Elased 0:29:01. Training loss: 3.5950222082545116. Learning Rate: 6.53929403929404e-05\n",
      "Batch 4,150 of 9,631 Elased 0:29:22. Training loss: 3.594305455742112. Learning Rate: 6.537559662559663e-05\n",
      "Batch 4,200 of 9,631 Elased 0:29:43. Training loss: 3.5943965073142734. Learning Rate: 6.535825285825287e-05\n",
      "Batch 4,250 of 9,631 Elased 0:30:04. Training loss: 3.594446836976444. Learning Rate: 6.53409090909091e-05\n",
      "Batch 4,300 of 9,631 Elased 0:30:26. Training loss: 3.594107712146848. Learning Rate: 6.532356532356533e-05\n",
      "Batch 4,350 of 9,631 Elased 0:30:47. Training loss: 3.594195628933523. Learning Rate: 6.530622155622156e-05\n",
      "Batch 4,400 of 9,631 Elased 0:31:08. Training loss: 3.5950178382071583. Learning Rate: 6.528887778887779e-05\n",
      "Batch 4,450 of 9,631 Elased 0:31:29. Training loss: 3.5952780217802927. Learning Rate: 6.527153402153402e-05\n",
      "Batch 4,500 of 9,631 Elased 0:31:50. Training loss: 3.5958965560595195. Learning Rate: 6.525419025419026e-05\n",
      "Batch 4,550 of 9,631 Elased 0:32:12. Training loss: 3.5937422515533783. Learning Rate: 6.523684648684649e-05\n",
      "Batch 4,600 of 9,631 Elased 0:32:33. Training loss: 3.5951106411477793. Learning Rate: 6.521950271950272e-05\n",
      "Batch 4,650 of 9,631 Elased 0:32:54. Training loss: 3.5942195191947364. Learning Rate: 6.520215895215895e-05\n",
      "Batch 4,700 of 9,631 Elased 0:33:15. Training loss: 3.594788048216637. Learning Rate: 6.518481518481519e-05\n",
      "Batch 4,750 of 9,631 Elased 0:33:36. Training loss: 3.5944420900344847. Learning Rate: 6.516747141747142e-05\n",
      "Batch 4,800 of 9,631 Elased 0:33:57. Training loss: 3.592685997337103. Learning Rate: 6.515012765012766e-05\n",
      "Batch 4,850 of 9,631 Elased 0:34:19. Training loss: 3.591990995849531. Learning Rate: 6.513278388278389e-05\n",
      "Batch 4,900 of 9,631 Elased 0:34:40. Training loss: 3.5923668839493574. Learning Rate: 6.511544011544012e-05\n",
      "Batch 4,950 of 9,631 Elased 0:35:01. Training loss: 3.5925530617646495. Learning Rate: 6.509809634809635e-05\n",
      "Batch 5,000 of 9,631 Elased 0:35:23. Training loss: 3.59340910615921. Learning Rate: 6.508075258075258e-05\n",
      "Batch 5,050 of 9,631 Elased 0:35:44. Training loss: 3.593222182576019. Learning Rate: 6.506340881340882e-05\n",
      "Batch 5,100 of 9,631 Elased 0:36:05. Training loss: 3.5911160472795074. Learning Rate: 6.504606504606505e-05\n",
      "Batch 5,150 of 9,631 Elased 0:36:26. Training loss: 3.591472915167947. Learning Rate: 6.502872127872128e-05\n",
      "Batch 5,200 of 9,631 Elased 0:36:48. Training loss: 3.591675281524658. Learning Rate: 6.501137751137751e-05\n",
      "Batch 5,250 of 9,631 Elased 0:37:09. Training loss: 3.591576092856271. Learning Rate: 6.499403374403374e-05\n",
      "Batch 5,300 of 9,631 Elased 0:37:30. Training loss: 3.5909564172546817. Learning Rate: 6.497668997668997e-05\n",
      "Batch 5,350 of 9,631 Elased 0:37:52. Training loss: 3.592825438330107. Learning Rate: 6.495934620934621e-05\n",
      "Batch 5,400 of 9,631 Elased 0:38:13. Training loss: 3.592689630852805. Learning Rate: 6.494200244200245e-05\n",
      "Batch 5,450 of 9,631 Elased 0:38:34. Training loss: 3.5931041642722734. Learning Rate: 6.492465867465868e-05\n",
      "Batch 5,500 of 9,631 Elased 0:38:55. Training loss: 3.5936034283638. Learning Rate: 6.490731490731491e-05\n",
      "Batch 5,550 of 9,631 Elased 0:39:17. Training loss: 3.5940686883582726. Learning Rate: 6.488997113997114e-05\n",
      "Batch 5,600 of 9,631 Elased 0:39:38. Training loss: 3.5935131237762317. Learning Rate: 6.487262737262737e-05\n",
      "Batch 5,650 of 9,631 Elased 0:39:59. Training loss: 3.5952927582664826. Learning Rate: 6.485528360528361e-05\n",
      "Batch 5,700 of 9,631 Elased 0:40:21. Training loss: 3.5951671772672418. Learning Rate: 6.483793983793984e-05\n",
      "Batch 5,750 of 9,631 Elased 0:40:42. Training loss: 3.594507419752038. Learning Rate: 6.482059607059607e-05\n",
      "Batch 5,800 of 9,631 Elased 0:41:03. Training loss: 3.594601721208671. Learning Rate: 6.48032523032523e-05\n",
      "Batch 5,850 of 9,631 Elased 0:41:24. Training loss: 3.5948615455831217. Learning Rate: 6.478590853590853e-05\n",
      "Batch 5,900 of 9,631 Elased 0:41:46. Training loss: 3.5947504997859565. Learning Rate: 6.476856476856477e-05\n",
      "Batch 5,950 of 9,631 Elased 0:42:07. Training loss: 3.5946512029351307. Learning Rate: 6.4751221001221e-05\n",
      "Batch 6,000 of 9,631 Elased 0:42:28. Training loss: 3.594073405086994. Learning Rate: 6.473387723387723e-05\n",
      "Batch 6,050 of 9,631 Elased 0:42:50. Training loss: 3.5939910238636426. Learning Rate: 6.471653346653348e-05\n",
      "Batch 6,100 of 9,631 Elased 0:43:11. Training loss: 3.5943855549077517. Learning Rate: 6.46991896991897e-05\n",
      "Batch 6,150 of 9,631 Elased 0:43:32. Training loss: 3.593924138797977. Learning Rate: 6.468184593184593e-05\n",
      "Batch 6,200 of 9,631 Elased 0:43:53. Training loss: 3.5918082928080715. Learning Rate: 6.466450216450218e-05\n",
      "Batch 6,250 of 9,631 Elased 0:44:15. Training loss: 3.5915204005241392. Learning Rate: 6.46471583971584e-05\n",
      "Batch 6,300 of 9,631 Elased 0:44:36. Training loss: 3.591698079582245. Learning Rate: 6.462981462981464e-05\n",
      "Batch 6,350 of 9,631 Elased 0:44:57. Training loss: 3.5911832847182206. Learning Rate: 6.461247086247086e-05\n",
      "Batch 6,400 of 9,631 Elased 0:45:18. Training loss: 3.5902988691069186. Learning Rate: 6.45951270951271e-05\n",
      "Batch 6,450 of 9,631 Elased 0:45:40. Training loss: 3.591595856515012. Learning Rate: 6.457778332778332e-05\n",
      "Batch 6,500 of 9,631 Elased 0:46:01. Training loss: 3.589795264849296. Learning Rate: 6.456043956043957e-05\n",
      "Batch 6,550 of 9,631 Elased 0:46:22. Training loss: 3.5900810750750183. Learning Rate: 6.45430957930958e-05\n",
      "Batch 6,600 of 9,631 Elased 0:46:43. Training loss: 3.590774321032293. Learning Rate: 6.452575202575202e-05\n",
      "Batch 6,650 of 9,631 Elased 0:47:04. Training loss: 3.59078534834367. Learning Rate: 6.450840825840825e-05\n",
      "Batch 6,700 of 9,631 Elased 0:47:26. Training loss: 3.5919383614276774. Learning Rate: 6.44910644910645e-05\n",
      "Batch 6,750 of 9,631 Elased 0:47:47. Training loss: 3.5918531939012035. Learning Rate: 6.447372072372073e-05\n",
      "Batch 6,800 of 9,631 Elased 0:48:08. Training loss: 3.591601462486912. Learning Rate: 6.445637695637697e-05\n",
      "Batch 6,850 of 9,631 Elased 0:48:29. Training loss: 3.590883847240114. Learning Rate: 6.44390331890332e-05\n",
      "Batch 6,900 of 9,631 Elased 0:48:50. Training loss: 3.590808933109477. Learning Rate: 6.442168942168943e-05\n",
      "Batch 6,950 of 9,631 Elased 0:49:11. Training loss: 3.5906060923946845. Learning Rate: 6.440434565434566e-05\n",
      "Batch 7,000 of 9,631 Elased 0:49:33. Training loss: 3.589921503015927. Learning Rate: 6.438700188700189e-05\n",
      "Batch 7,050 of 9,631 Elased 0:49:54. Training loss: 3.5892598312628183. Learning Rate: 6.436965811965813e-05\n",
      "Batch 7,100 of 9,631 Elased 0:50:15. Training loss: 3.589227814489687. Learning Rate: 6.435231435231436e-05\n",
      "Batch 7,150 of 9,631 Elased 0:50:37. Training loss: 3.588928858800368. Learning Rate: 6.433497058497059e-05\n",
      "Batch 7,200 of 9,631 Elased 0:50:58. Training loss: 3.589307542923424. Learning Rate: 6.431762681762682e-05\n",
      "Batch 7,250 of 9,631 Elased 0:51:18. Training loss: 3.588065456341053. Learning Rate: 6.430028305028305e-05\n",
      "Batch 7,300 of 9,631 Elased 0:51:40. Training loss: 3.5883645918761213. Learning Rate: 6.428293928293927e-05\n",
      "Batch 7,350 of 9,631 Elased 0:52:01. Training loss: 3.587747893382092. Learning Rate: 6.426559551559552e-05\n",
      "Batch 7,400 of 9,631 Elased 0:52:22. Training loss: 3.586396973181415. Learning Rate: 6.424825174825175e-05\n",
      "Batch 7,450 of 9,631 Elased 0:52:43. Training loss: 3.5864047635961698. Learning Rate: 6.423090798090799e-05\n",
      "Batch 7,500 of 9,631 Elased 0:53:04. Training loss: 3.5853519834677376. Learning Rate: 6.421356421356422e-05\n",
      "Batch 7,550 of 9,631 Elased 0:53:25. Training loss: 3.5851764943110234. Learning Rate: 6.419622044622045e-05\n",
      "Batch 7,600 of 9,631 Elased 0:53:46. Training loss: 3.585665388436694. Learning Rate: 6.417887667887669e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7,650 of 9,631 Elased 0:54:07. Training loss: 3.585840228227229. Learning Rate: 6.416153291153292e-05\n",
      "Batch 7,700 of 9,631 Elased 0:54:29. Training loss: 3.584837836274853. Learning Rate: 6.414418914418915e-05\n",
      "Batch 7,750 of 9,631 Elased 0:54:50. Training loss: 3.5830547247702076. Learning Rate: 6.412684537684538e-05\n",
      "Batch 7,800 of 9,631 Elased 0:55:11. Training loss: 3.583680027952561. Learning Rate: 6.410950160950161e-05\n",
      "Batch 7,850 of 9,631 Elased 0:55:33. Training loss: 3.5839342389896416. Learning Rate: 6.409215784215784e-05\n",
      "Batch 7,900 of 9,631 Elased 0:55:54. Training loss: 3.5822885955587216. Learning Rate: 6.407481407481408e-05\n",
      "Batch 7,950 of 9,631 Elased 0:56:15. Training loss: 3.583593216317255. Learning Rate: 6.405747030747031e-05\n",
      "Batch 8,000 of 9,631 Elased 0:56:37. Training loss: 3.5832687516361474. Learning Rate: 6.404012654012654e-05\n",
      "Batch 8,050 of 9,631 Elased 0:56:58. Training loss: 3.5844913787871415. Learning Rate: 6.402278277278277e-05\n",
      "Batch 8,100 of 9,631 Elased 0:57:19. Training loss: 3.584671127192768. Learning Rate: 6.400543900543901e-05\n",
      "Batch 8,150 of 9,631 Elased 0:57:40. Training loss: 3.5854866995694445. Learning Rate: 6.398809523809524e-05\n",
      "Batch 8,200 of 9,631 Elased 0:58:02. Training loss: 3.585086865323346. Learning Rate: 6.397075147075148e-05\n",
      "Batch 8,250 of 9,631 Elased 0:58:23. Training loss: 3.5859429715618942. Learning Rate: 6.395340770340771e-05\n",
      "Batch 8,300 of 9,631 Elased 0:58:45. Training loss: 3.5857889251105757. Learning Rate: 6.393606393606394e-05\n",
      "Batch 8,350 of 9,631 Elased 0:59:06. Training loss: 3.585152593272889. Learning Rate: 6.391872016872017e-05\n",
      "Batch 8,400 of 9,631 Elased 0:59:27. Training loss: 3.5844909384279022. Learning Rate: 6.39013764013764e-05\n",
      "Batch 8,450 of 9,631 Elased 0:59:48. Training loss: 3.583825452158437. Learning Rate: 6.388403263403264e-05\n",
      "Batch 8,500 of 9,631 Elased 1:00:10. Training loss: 3.583379760868409. Learning Rate: 6.386668886668887e-05\n",
      "Batch 8,550 of 9,631 Elased 1:00:31. Training loss: 3.583015379501365. Learning Rate: 6.38493450993451e-05\n",
      "Batch 8,600 of 9,631 Elased 1:00:52. Training loss: 3.5839200218195137. Learning Rate: 6.383200133200133e-05\n",
      "Batch 8,650 of 9,631 Elased 1:01:13. Training loss: 3.5830377650260927. Learning Rate: 6.381465756465756e-05\n",
      "Batch 8,700 of 9,631 Elased 1:01:35. Training loss: 3.5828035988615845. Learning Rate: 6.379731379731379e-05\n",
      "Batch 8,750 of 9,631 Elased 1:01:56. Training loss: 3.5826293271609715. Learning Rate: 6.377997002997003e-05\n",
      "Batch 8,800 of 9,631 Elased 1:02:17. Training loss: 3.58262323172255. Learning Rate: 6.376262626262627e-05\n",
      "Batch 8,850 of 9,631 Elased 1:02:39. Training loss: 3.5824138079794112. Learning Rate: 6.37452824952825e-05\n",
      "Batch 8,900 of 9,631 Elased 1:03:00. Training loss: 3.5828110728237066. Learning Rate: 6.372793872793873e-05\n",
      "Batch 8,950 of 9,631 Elased 1:03:21. Training loss: 3.582375404541719. Learning Rate: 6.371059496059496e-05\n",
      "Batch 9,000 of 9,631 Elased 1:03:42. Training loss: 3.5812196278174717. Learning Rate: 6.369325119325119e-05\n",
      "Batch 9,050 of 9,631 Elased 1:04:03. Training loss: 3.582073500472538. Learning Rate: 6.367590742590743e-05\n",
      "Batch 9,100 of 9,631 Elased 1:04:25. Training loss: 3.5817102759618025. Learning Rate: 6.365856365856366e-05\n",
      "Batch 9,150 of 9,631 Elased 1:04:46. Training loss: 3.5816870835439754. Learning Rate: 6.364121989121989e-05\n",
      "Batch 9,200 of 9,631 Elased 1:05:07. Training loss: 3.582150939469752. Learning Rate: 6.362387612387612e-05\n",
      "Batch 9,250 of 9,631 Elased 1:05:28. Training loss: 3.5819007778683223. Learning Rate: 6.360653235653235e-05\n",
      "Batch 9,300 of 9,631 Elased 1:05:49. Training loss: 3.58242262576216. Learning Rate: 6.35891885891886e-05\n",
      "Batch 9,350 of 9,631 Elased 1:06:11. Training loss: 3.5823564222407214. Learning Rate: 6.357184482184482e-05\n",
      "Batch 9,400 of 9,631 Elased 1:06:31. Training loss: 3.582226788122603. Learning Rate: 6.355450105450105e-05\n",
      "Batch 9,450 of 9,631 Elased 1:06:52. Training loss: 3.5819746752769226. Learning Rate: 6.35371572871573e-05\n",
      "Batch 9,500 of 9,631 Elased 1:07:14. Training loss: 3.5815271990299227. Learning Rate: 6.351981351981353e-05\n",
      "Batch 9,550 of 9,631 Elased 1:07:35. Training loss: 3.580719991142213. Learning Rate: 6.350246975246975e-05\n",
      "Batch 9,600 of 9,631 Elased 1:07:57. Training loss: 3.5811890173827607. Learning Rate: 6.3485125985126e-05\n",
      "\n",
      "\n",
      "  Average training loss: 3.58\n",
      "  Training epcoh took: 1:08:10\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9b4a9686374319a2c60e2a602aec18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=67.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Validation Loss: 3.60\n",
      "  Validation took: 0:00:43\n",
      "\n",
      "======== Epoch 12 / 30 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6706736477214d3abb341ae434252421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch    50 of 9,631 Elased 0:00:21. Training loss: 3.65471333026886. Learning Rate: 6.345702908202909e-05\n",
      "Batch   100 of 9,631 Elased 0:00:43. Training loss: 3.6101762771606447. Learning Rate: 6.343968531468532e-05\n",
      "Batch   150 of 9,631 Elased 0:01:03. Training loss: 3.6042906975746156. Learning Rate: 6.342234154734155e-05\n",
      "Batch   200 of 9,631 Elased 0:01:25. Training loss: 3.627894384264946. Learning Rate: 6.340499777999779e-05\n",
      "Batch   250 of 9,631 Elased 0:01:46. Training loss: 3.571517388820648. Learning Rate: 6.338765401265402e-05\n",
      "Batch   300 of 9,631 Elased 0:02:07. Training loss: 3.550074144601822. Learning Rate: 6.337031024531025e-05\n",
      "Batch   350 of 9,631 Elased 0:02:28. Training loss: 3.5424978048460822. Learning Rate: 6.335296647796648e-05\n",
      "Batch   400 of 9,631 Elased 0:02:50. Training loss: 3.5637733426690104. Learning Rate: 6.33356227106227e-05\n",
      "Batch   450 of 9,631 Elased 0:03:10. Training loss: 3.5663156824641757. Learning Rate: 6.331827894327894e-05\n",
      "Batch   500 of 9,631 Elased 0:03:32. Training loss: 3.5731037590503694. Learning Rate: 6.330093517593518e-05\n",
      "Batch   550 of 9,631 Elased 0:03:53. Training loss: 3.5586884942921726. Learning Rate: 6.328359140859141e-05\n",
      "Batch   600 of 9,631 Elased 0:04:14. Training loss: 3.571001385251681. Learning Rate: 6.326624764124765e-05\n",
      "Batch   650 of 9,631 Elased 0:04:36. Training loss: 3.5734717359909642. Learning Rate: 6.324890387390388e-05\n",
      "Batch   700 of 9,631 Elased 0:04:57. Training loss: 3.569342899492809. Learning Rate: 6.323156010656011e-05\n",
      "Batch   750 of 9,631 Elased 0:05:18. Training loss: 3.5572585841814677. Learning Rate: 6.321421633921635e-05\n",
      "Batch   800 of 9,631 Elased 0:05:40. Training loss: 3.5566139639914036. Learning Rate: 6.319687257187258e-05\n",
      "Batch   850 of 9,631 Elased 0:06:01. Training loss: 3.5482586713398203. Learning Rate: 6.317952880452881e-05\n",
      "Batch   900 of 9,631 Elased 0:06:22. Training loss: 3.5459124415450627. Learning Rate: 6.316218503718504e-05\n",
      "Batch   950 of 9,631 Elased 0:06:43. Training loss: 3.5417248039496574. Learning Rate: 6.314484126984127e-05\n",
      "Batch 1,000 of 9,631 Elased 0:07:04. Training loss: 3.543123688340187. Learning Rate: 6.31274975024975e-05\n",
      "Batch 1,050 of 9,631 Elased 0:07:26. Training loss: 3.5453214501199266. Learning Rate: 6.311015373515374e-05\n",
      "Batch 1,100 of 9,631 Elased 0:07:47. Training loss: 3.546295024806803. Learning Rate: 6.309280996780997e-05\n",
      "Batch 1,150 of 9,631 Elased 0:08:08. Training loss: 3.546147784875787. Learning Rate: 6.30754662004662e-05\n",
      "Batch 1,200 of 9,631 Elased 0:08:29. Training loss: 3.550359642803669. Learning Rate: 6.305812243312243e-05\n",
      "Batch 1,250 of 9,631 Elased 0:08:51. Training loss: 3.5514840121269224. Learning Rate: 6.304077866577867e-05\n",
      "Batch 1,300 of 9,631 Elased 0:09:12. Training loss: 3.5468282783031464. Learning Rate: 6.30234348984349e-05\n",
      "Batch 1,350 of 9,631 Elased 0:09:33. Training loss: 3.5479609666930303. Learning Rate: 6.300609113109114e-05\n",
      "Batch 1,400 of 9,631 Elased 0:09:55. Training loss: 3.5458073392084666. Learning Rate: 6.298874736374737e-05\n",
      "Batch 1,450 of 9,631 Elased 0:10:16. Training loss: 3.5432198444728193. Learning Rate: 6.29714035964036e-05\n",
      "Batch 1,500 of 9,631 Elased 0:10:37. Training loss: 3.5400011853377022. Learning Rate: 6.295405982905983e-05\n",
      "Batch 1,550 of 9,631 Elased 0:10:59. Training loss: 3.5396158591393503. Learning Rate: 6.293671606171606e-05\n",
      "Batch 1,600 of 9,631 Elased 0:11:22. Training loss: 3.5445417965203525. Learning Rate: 6.291937229437229e-05\n",
      "Batch 1,650 of 9,631 Elased 0:11:44. Training loss: 3.5467261823740874. Learning Rate: 6.290202852702853e-05\n",
      "Batch 1,700 of 9,631 Elased 0:12:06. Training loss: 3.5428429435982425. Learning Rate: 6.288468475968476e-05\n",
      "Batch 1,750 of 9,631 Elased 0:12:27. Training loss: 3.5417663813318523. Learning Rate: 6.286734099234099e-05\n",
      "Batch 1,800 of 9,631 Elased 0:12:50. Training loss: 3.5410213440656664. Learning Rate: 6.284999722499722e-05\n",
      "Batch 1,850 of 9,631 Elased 0:13:12. Training loss: 3.543898672477619. Learning Rate: 6.283265345765345e-05\n",
      "Batch 1,900 of 9,631 Elased 0:13:34. Training loss: 3.541834154944671. Learning Rate: 6.281530969030969e-05\n",
      "Batch 1,950 of 9,631 Elased 0:13:55. Training loss: 3.537893713437594. Learning Rate: 6.279796592296592e-05\n",
      "Batch 2,000 of 9,631 Elased 0:14:16. Training loss: 3.533440307378769. Learning Rate: 6.278062215562217e-05\n",
      "Batch 2,050 of 9,631 Elased 0:14:37. Training loss: 3.5350153869535865. Learning Rate: 6.27632783882784e-05\n",
      "Batch 2,100 of 9,631 Elased 0:14:58. Training loss: 3.5378493160293214. Learning Rate: 6.274593462093462e-05\n",
      "Batch 2,150 of 9,631 Elased 0:15:20. Training loss: 3.5385889777471853. Learning Rate: 6.272859085359085e-05\n",
      "Batch 2,200 of 9,631 Elased 0:15:41. Training loss: 3.538879748366096. Learning Rate: 6.27112470862471e-05\n",
      "Batch 2,250 of 9,631 Elased 0:16:02. Training loss: 3.537993646144867. Learning Rate: 6.269390331890333e-05\n",
      "Batch 2,300 of 9,631 Elased 0:16:24. Training loss: 3.540148196168568. Learning Rate: 6.267655955155955e-05\n",
      "Batch 2,350 of 9,631 Elased 0:16:45. Training loss: 3.5373119956381776. Learning Rate: 6.265921578421578e-05\n",
      "Batch 2,400 of 9,631 Elased 0:17:07. Training loss: 3.53721606567502. Learning Rate: 6.264187201687201e-05\n",
      "Batch 2,450 of 9,631 Elased 0:17:28. Training loss: 3.533552394253867. Learning Rate: 6.262452824952824e-05\n",
      "Batch 2,500 of 9,631 Elased 0:17:48. Training loss: 3.5323460047245026. Learning Rate: 6.260718448218448e-05\n",
      "Batch 2,550 of 9,631 Elased 0:18:09. Training loss: 3.536393456973282. Learning Rate: 6.258984071484071e-05\n",
      "Batch 2,600 of 9,631 Elased 0:18:30. Training loss: 3.5396191790929206. Learning Rate: 6.257249694749696e-05\n",
      "Batch 2,650 of 9,631 Elased 0:18:51. Training loss: 3.5414877123652766. Learning Rate: 6.255515318015319e-05\n",
      "Batch 2,700 of 9,631 Elased 0:19:13. Training loss: 3.5433905867735547. Learning Rate: 6.253780941280942e-05\n",
      "Batch 2,750 of 9,631 Elased 0:19:34. Training loss: 3.5405378657687794. Learning Rate: 6.252046564546566e-05\n",
      "Batch 2,800 of 9,631 Elased 0:19:55. Training loss: 3.5412514028804645. Learning Rate: 6.250312187812189e-05\n",
      "Batch 2,850 of 9,631 Elased 0:20:17. Training loss: 3.5440969322857105. Learning Rate: 6.248577811077812e-05\n",
      "Batch 2,900 of 9,631 Elased 0:20:38. Training loss: 3.5444395930602632. Learning Rate: 6.246843434343435e-05\n",
      "Batch 2,950 of 9,631 Elased 0:20:59. Training loss: 3.54531017752017. Learning Rate: 6.245109057609058e-05\n",
      "Batch 3,000 of 9,631 Elased 0:21:20. Training loss: 3.5476572255690892. Learning Rate: 6.24337468087468e-05\n",
      "Batch 3,050 of 9,631 Elased 0:21:42. Training loss: 3.547438033017956. Learning Rate: 6.241640304140305e-05\n",
      "Batch 3,100 of 9,631 Elased 0:22:02. Training loss: 3.5473761980379783. Learning Rate: 6.239905927405928e-05\n",
      "Batch 3,150 of 9,631 Elased 0:22:24. Training loss: 3.5431609104928516. Learning Rate: 6.23817155067155e-05\n",
      "Batch 3,200 of 9,631 Elased 0:22:45. Training loss: 3.5450712351128457. Learning Rate: 6.236437173937174e-05\n",
      "Batch 3,250 of 9,631 Elased 0:23:06. Training loss: 3.5448341447023246. Learning Rate: 6.234702797202798e-05\n",
      "Batch 3,300 of 9,631 Elased 0:23:28. Training loss: 3.547274729085691. Learning Rate: 6.232968420468421e-05\n",
      "Batch 3,350 of 9,631 Elased 0:23:49. Training loss: 3.5478479007820587. Learning Rate: 6.231234043734045e-05\n",
      "Batch 3,650 of 9,631 Elased 0:25:56. Training loss: 3.5552903443493253. Learning Rate: 6.220827783327784e-05\n",
      "Batch 3,700 of 9,631 Elased 0:26:17. Training loss: 3.5542964949156786. Learning Rate: 6.219093406593407e-05\n",
      "Batch 3,750 of 9,631 Elased 0:26:39. Training loss: 3.5533400279680887. Learning Rate: 6.21735902985903e-05\n",
      "Batch 3,800 of 9,631 Elased 0:27:00. Training loss: 3.552007211289908. Learning Rate: 6.215624653124653e-05\n",
      "Batch 3,850 of 9,631 Elased 0:27:21. Training loss: 3.549362006032622. Learning Rate: 6.213890276390276e-05\n",
      "Batch 3,900 of 9,631 Elased 0:27:42. Training loss: 3.5476653650173775. Learning Rate: 6.2121558996559e-05\n",
      "Batch 3,950 of 9,631 Elased 0:28:04. Training loss: 3.548581613498398. Learning Rate: 6.210421522921523e-05\n",
      "Batch 4,000 of 9,631 Elased 0:28:25. Training loss: 3.5491424546539783. Learning Rate: 6.208687146187147e-05\n",
      "Batch 4,050 of 9,631 Elased 0:28:46. Training loss: 3.5482423459453347. Learning Rate: 6.20695276945277e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4,100 of 9,631 Elased 0:29:07. Training loss: 3.547335392643766. Learning Rate: 6.205218392718393e-05\n",
      "Batch 4,150 of 9,631 Elased 0:29:28. Training loss: 3.5467084094415227. Learning Rate: 6.203484015984016e-05\n",
      "Batch 4,200 of 9,631 Elased 0:29:50. Training loss: 3.546709568755967. Learning Rate: 6.20174963924964e-05\n",
      "Batch 4,250 of 9,631 Elased 0:30:11. Training loss: 3.547186553702635. Learning Rate: 6.200015262515263e-05\n",
      "Batch 4,300 of 9,631 Elased 0:30:33. Training loss: 3.546641405987185. Learning Rate: 6.198280885780886e-05\n",
      "Batch 4,350 of 9,631 Elased 0:30:53. Training loss: 3.5466957047067837. Learning Rate: 6.196546509046509e-05\n",
      "Batch 4,400 of 9,631 Elased 0:31:14. Training loss: 3.5476058399948207. Learning Rate: 6.194812132312132e-05\n",
      "Batch 4,450 of 9,631 Elased 0:31:35. Training loss: 3.548051263643115. Learning Rate: 6.193077755577756e-05\n",
      "Batch 4,500 of 9,631 Elased 0:31:57. Training loss: 3.549632072210312. Learning Rate: 6.191343378843379e-05\n",
      "Batch 4,550 of 9,631 Elased 0:32:18. Training loss: 3.5477501356470715. Learning Rate: 6.189609002109002e-05\n",
      "Batch 4,600 of 9,631 Elased 0:32:39. Training loss: 3.5489356503538465. Learning Rate: 6.187874625374625e-05\n",
      "Batch 4,650 of 9,631 Elased 0:33:00. Training loss: 3.547635260064115. Learning Rate: 6.186140248640249e-05\n",
      "Batch 4,700 of 9,631 Elased 0:33:21. Training loss: 3.5480894373071954. Learning Rate: 6.184405871905872e-05\n",
      "Batch 4,750 of 9,631 Elased 0:33:43. Training loss: 3.5480158137522246. Learning Rate: 6.182671495171496e-05\n",
      "Batch 4,800 of 9,631 Elased 0:34:04. Training loss: 3.5460614446550607. Learning Rate: 6.18093711843712e-05\n",
      "Batch 4,850 of 9,631 Elased 0:34:25. Training loss: 3.545208509558255. Learning Rate: 6.179202741702742e-05\n",
      "Batch 4,900 of 9,631 Elased 0:34:47. Training loss: 3.545696107538379. Learning Rate: 6.177468364968365e-05\n",
      "Batch 4,950 of 9,631 Elased 0:35:08. Training loss: 3.5461031531083465. Learning Rate: 6.175733988233988e-05\n",
      "Batch 5,000 of 9,631 Elased 0:35:29. Training loss: 3.546795762991905. Learning Rate: 6.173999611499611e-05\n",
      "Batch 5,050 of 9,631 Elased 0:35:50. Training loss: 3.546582506930474. Learning Rate: 6.172265234765235e-05\n",
      "Batch 5,100 of 9,631 Elased 0:36:11. Training loss: 3.5447273501929115. Learning Rate: 6.170530858030858e-05\n",
      "Batch 5,150 of 9,631 Elased 0:36:32. Training loss: 3.5454226339210586. Learning Rate: 6.168796481296481e-05\n",
      "Batch 5,200 of 9,631 Elased 0:36:53. Training loss: 3.544780861230997. Learning Rate: 6.167062104562104e-05\n",
      "Batch 5,250 of 9,631 Elased 0:37:14. Training loss: 3.544778244336446. Learning Rate: 6.165327727827727e-05\n",
      "Batch 5,300 of 9,631 Elased 0:37:35. Training loss: 3.5441522668892484. Learning Rate: 6.163593351093351e-05\n",
      "Batch 5,350 of 9,631 Elased 0:37:57. Training loss: 3.5459868203813785. Learning Rate: 6.161858974358974e-05\n",
      "Batch 5,400 of 9,631 Elased 0:38:18. Training loss: 3.5460212784784813. Learning Rate: 6.160124597624599e-05\n",
      "Batch 5,450 of 9,631 Elased 0:38:40. Training loss: 3.5462853667495446. Learning Rate: 6.158390220890222e-05\n",
      "Batch 5,500 of 9,631 Elased 0:39:01. Training loss: 3.5466748008728026. Learning Rate: 6.156655844155844e-05\n",
      "Batch 5,550 of 9,631 Elased 0:39:22. Training loss: 3.547350450721947. Learning Rate: 6.154921467421467e-05\n",
      "Batch 5,600 of 9,631 Elased 0:39:44. Training loss: 3.546650991524969. Learning Rate: 6.153187090687092e-05\n",
      "Batch 5,650 of 9,631 Elased 0:40:05. Training loss: 3.5482929577025693. Learning Rate: 6.151452713952715e-05\n",
      "Batch 5,700 of 9,631 Elased 0:40:26. Training loss: 3.548156690221084. Learning Rate: 6.149718337218337e-05\n",
      "Batch 5,750 of 9,631 Elased 0:40:47. Training loss: 3.5476208651584127. Learning Rate: 6.14798396048396e-05\n",
      "Batch 5,800 of 9,631 Elased 0:41:08. Training loss: 3.5477700438581663. Learning Rate: 6.146249583749583e-05\n",
      "Batch 5,850 of 9,631 Elased 0:41:29. Training loss: 3.548458242538648. Learning Rate: 6.144515207015206e-05\n",
      "Batch 5,900 of 9,631 Elased 0:41:51. Training loss: 3.548104269544957. Learning Rate: 6.14278083028083e-05\n",
      "Batch 5,950 of 9,631 Elased 0:42:12. Training loss: 3.5486915297868875. Learning Rate: 6.141046453546453e-05\n",
      "Batch 6,000 of 9,631 Elased 0:42:33. Training loss: 3.5480704607566196. Learning Rate: 6.139312076812078e-05\n",
      "Batch 6,050 of 9,631 Elased 0:42:54. Training loss: 3.5478638381012213. Learning Rate: 6.137577700077701e-05\n",
      "Batch 6,100 of 9,631 Elased 0:43:16. Training loss: 3.5479304151456863. Learning Rate: 6.135843323343324e-05\n",
      "Batch 6,150 of 9,631 Elased 0:43:37. Training loss: 3.5477061757808777. Learning Rate: 6.134108946608948e-05\n",
      "Batch 6,200 of 9,631 Elased 0:43:58. Training loss: 3.5454791468766427. Learning Rate: 6.132374569874571e-05\n",
      "Batch 6,250 of 9,631 Elased 0:44:19. Training loss: 3.54487726190567. Learning Rate: 6.130640193140194e-05\n",
      "Batch 6,300 of 9,631 Elased 0:44:41. Training loss: 3.5448311335132234. Learning Rate: 6.128905816405817e-05\n",
      "Batch 6,350 of 9,631 Elased 0:45:02. Training loss: 3.5445759731390343. Learning Rate: 6.12717143967144e-05\n",
      "Batch 6,400 of 9,631 Elased 0:45:23. Training loss: 3.543729791287333. Learning Rate: 6.125437062937063e-05\n",
      "Batch 6,450 of 9,631 Elased 0:45:44. Training loss: 3.544946109472319. Learning Rate: 6.123702686202687e-05\n",
      "Batch 6,500 of 9,631 Elased 0:46:06. Training loss: 3.5431446247651026. Learning Rate: 6.12196830946831e-05\n",
      "Batch 6,550 of 9,631 Elased 0:46:27. Training loss: 3.5432439529440787. Learning Rate: 6.120233932733933e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4,100 of 9,631 Elased 0:29:07. Training loss: 3.5062374622065846. Learning Rate: 5.871142746142746e-05\n",
      "Batch 4,150 of 9,631 Elased 0:29:28. Training loss: 3.5056003927322754. Learning Rate: 5.8694083694083704e-05\n",
      "Batch 4,200 of 9,631 Elased 0:29:49. Training loss: 3.5059047590550922. Learning Rate: 5.8676739926739934e-05\n",
      "Batch 4,250 of 9,631 Elased 0:30:10. Training loss: 3.505993204313166. Learning Rate: 5.865939615939616e-05\n",
      "Batch 4,300 of 9,631 Elased 0:30:32. Training loss: 3.5061853049244993. Learning Rate: 5.864205239205239e-05\n",
      "Batch 4,350 of 9,631 Elased 0:30:53. Training loss: 3.5055334823707054. Learning Rate: 5.862470862470862e-05\n",
      "Batch 4,400 of 9,631 Elased 0:31:14. Training loss: 3.506493333551017. Learning Rate: 5.860736485736486e-05\n",
      "Batch 4,450 of 9,631 Elased 0:31:36. Training loss: 3.5071377627501326. Learning Rate: 5.8590021090021093e-05\n",
      "Batch 4,500 of 9,631 Elased 0:31:57. Training loss: 3.5082596291701. Learning Rate: 5.857267732267733e-05\n",
      "Batch 4,550 of 9,631 Elased 0:32:18. Training loss: 3.5063142682955815. Learning Rate: 5.855533355533356e-05\n",
      "Batch 4,600 of 9,631 Elased 0:32:39. Training loss: 3.507442917175915. Learning Rate: 5.853798978798979e-05\n",
      "Batch 4,650 of 9,631 Elased 0:33:00. Training loss: 3.5062802712378964. Learning Rate: 5.852064602064602e-05\n",
      "Batch 4,700 of 9,631 Elased 0:33:22. Training loss: 3.506815726833141. Learning Rate: 5.850330225330226e-05\n",
      "Batch 4,750 of 9,631 Elased 0:33:43. Training loss: 3.5068652101064983. Learning Rate: 5.848595848595849e-05\n",
      "Batch 4,800 of 9,631 Elased 0:34:05. Training loss: 3.5049346428364516. Learning Rate: 5.8468614718614725e-05\n",
      "Batch 4,850 of 9,631 Elased 0:34:26. Training loss: 3.5038412008826265. Learning Rate: 5.8451270951270955e-05\n",
      "Batch 4,900 of 9,631 Elased 0:34:47. Training loss: 3.504293907296901. Learning Rate: 5.8433927183927184e-05\n",
      "Batch 4,950 of 9,631 Elased 0:35:09. Training loss: 3.5046998133081377. Learning Rate: 5.841658341658341e-05\n",
      "Batch 5,000 of 9,631 Elased 0:35:30. Training loss: 3.506160187602043. Learning Rate: 5.8399239649239656e-05\n",
      "Batch 5,050 of 9,631 Elased 0:35:51. Training loss: 3.5059143296090682. Learning Rate: 5.8381895881895885e-05\n",
      "Batch 5,100 of 9,631 Elased 0:36:13. Training loss: 3.5040598402070064. Learning Rate: 5.8364552114552115e-05\n",
      "Batch 5,150 of 9,631 Elased 0:36:34. Training loss: 3.504687355212795. Learning Rate: 5.834720834720835e-05\n",
      "Batch 5,200 of 9,631 Elased 0:36:56. Training loss: 3.5047374110267713. Learning Rate: 5.832986457986458e-05\n",
      "Batch 5,250 of 9,631 Elased 0:37:17. Training loss: 3.5049575956208368. Learning Rate: 5.831252081252081e-05\n",
      "Batch 5,300 of 9,631 Elased 0:37:38. Training loss: 3.504354443077771. Learning Rate: 5.829517704517705e-05\n",
      "Batch 5,350 of 9,631 Elased 0:37:59. Training loss: 3.506487287917984. Learning Rate: 5.827783327783328e-05\n",
      "Batch 5,400 of 9,631 Elased 0:38:21. Training loss: 3.5066711493112424. Learning Rate: 5.826048951048951e-05\n",
      "Batch 5,450 of 9,631 Elased 0:38:42. Training loss: 3.5068880623414977. Learning Rate: 5.824314574314575e-05\n",
      "Batch 5,500 of 9,631 Elased 0:39:03. Training loss: 3.5069674832820894. Learning Rate: 5.8225801975801976e-05\n",
      "Batch 5,550 of 9,631 Elased 0:39:24. Training loss: 3.5074024302035838. Learning Rate: 5.820845820845822e-05\n",
      "Batch 5,600 of 9,631 Elased 0:39:46. Training loss: 3.5069787135507378. Learning Rate: 5.819111444111445e-05\n",
      "Batch 5,650 of 9,631 Elased 0:40:07. Training loss: 3.508698675379289. Learning Rate: 5.817377067377068e-05\n",
      "Batch 5,700 of 9,631 Elased 0:40:28. Training loss: 3.50865641855357. Learning Rate: 5.8156426906426907e-05\n",
      "Batch 5,750 of 9,631 Elased 0:40:49. Training loss: 3.508188993267391. Learning Rate: 5.813908313908314e-05\n",
      "Batch 5,800 of 9,631 Elased 0:41:10. Training loss: 3.508159349971804. Learning Rate: 5.812173937173937e-05\n",
      "Batch 5,850 of 9,631 Elased 0:41:32. Training loss: 3.5084114052495385. Learning Rate: 5.8104395604395615e-05\n",
      "Batch 5,900 of 9,631 Elased 0:41:53. Training loss: 3.5082060517092883. Learning Rate: 5.8087051837051844e-05\n",
      "Batch 5,950 of 9,631 Elased 0:42:14. Training loss: 3.508307395762756. Learning Rate: 5.806970806970807e-05\n",
      "Batch 6,000 of 9,631 Elased 0:42:36. Training loss: 3.5077006988724073. Learning Rate: 5.80523643023643e-05\n",
      "Batch 6,050 of 9,631 Elased 0:42:57. Training loss: 3.5078009073990435. Learning Rate: 5.803502053502053e-05\n",
      "Batch 6,100 of 9,631 Elased 0:43:19. Training loss: 3.50762537076825. Learning Rate: 5.801767676767677e-05\n",
      "Batch 6,150 of 9,631 Elased 0:43:40. Training loss: 3.5076039504229537. Learning Rate: 5.8000333000333004e-05\n",
      "Batch 6,200 of 9,631 Elased 0:44:01. Training loss: 3.5055015698555976. Learning Rate: 5.798298923298924e-05\n",
      "Batch 6,250 of 9,631 Elased 0:44:23. Training loss: 3.5051748904800415. Learning Rate: 5.796564546564547e-05\n",
      "Batch 6,300 of 9,631 Elased 0:44:44. Training loss: 3.5050405220758347. Learning Rate: 5.79483016983017e-05\n",
      "Batch 6,350 of 9,631 Elased 0:45:05. Training loss: 3.5045242216455654. Learning Rate: 5.793095793095793e-05\n",
      "Batch 6,400 of 9,631 Elased 0:45:26. Training loss: 3.503829000741243. Learning Rate: 5.791361416361417e-05\n",
      "Batch 6,450 of 9,631 Elased 0:45:47. Training loss: 3.505040081896523. Learning Rate: 5.78962703962704e-05\n",
      "Batch 6,500 of 9,631 Elased 0:46:08. Training loss: 3.5033539566626914. Learning Rate: 5.7878926628926636e-05\n",
      "Batch 6,550 of 9,631 Elased 0:46:30. Training loss: 3.5036060271008322. Learning Rate: 5.7861582861582865e-05\n",
      "Batch 6,600 of 9,631 Elased 0:46:51. Training loss: 3.5041956479260414. Learning Rate: 5.7844239094239094e-05\n",
      "Batch 6,650 of 9,631 Elased 0:47:12. Training loss: 3.5044907747354723. Learning Rate: 5.7826895326895324e-05\n",
      "Batch 6,700 of 9,631 Elased 0:47:33. Training loss: 3.5054406979902466. Learning Rate: 5.7809551559551566e-05\n",
      "Batch 6,750 of 9,631 Elased 0:47:55. Training loss: 3.505262705732275. Learning Rate: 5.7792207792207796e-05\n",
      "Batch 6,800 of 9,631 Elased 0:48:16. Training loss: 3.5050271622924245. Learning Rate: 5.7774864024864025e-05\n",
      "Batch 6,850 of 9,631 Elased 0:48:37. Training loss: 3.504180960133128. Learning Rate: 5.775752025752026e-05\n",
      "Batch 6,900 of 9,631 Elased 0:48:59. Training loss: 3.504067096468331. Learning Rate: 5.774017649017649e-05\n",
      "Batch 6,950 of 9,631 Elased 0:49:20. Training loss: 3.503844398148626. Learning Rate: 5.772283272283272e-05\n",
      "Batch 7,000 of 9,631 Elased 0:49:41. Training loss: 3.50296186879703. Learning Rate: 5.770548895548896e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4,700 of 9,631 Elased 0:33:19. Training loss: 3.4677971250452893. Learning Rate: 5.516254578754579e-05\n",
      "Batch 4,750 of 9,631 Elased 0:33:40. Training loss: 3.467873672786512. Learning Rate: 5.514520202020203e-05\n",
      "Batch 4,800 of 9,631 Elased 0:34:01. Training loss: 3.4663593503832817. Learning Rate: 5.512785825285826e-05\n",
      "Batch 4,850 of 9,631 Elased 0:34:22. Training loss: 3.465189896908003. Learning Rate: 5.5110514485514486e-05\n",
      "Batch 4,900 of 9,631 Elased 0:34:43. Training loss: 3.4658797560419354. Learning Rate: 5.5093170718170716e-05\n",
      "Batch 4,950 of 9,631 Elased 0:35:04. Training loss: 3.4662452748327546. Learning Rate: 5.507582695082696e-05\n",
      "Batch 5,000 of 9,631 Elased 0:35:25. Training loss: 3.467271652030945. Learning Rate: 5.505848318348319e-05\n",
      "Batch 5,050 of 9,631 Elased 0:35:47. Training loss: 3.4668712978079768. Learning Rate: 5.504113941613942e-05\n",
      "Batch 5,100 of 9,631 Elased 0:36:08. Training loss: 3.4648484677894444. Learning Rate: 5.502379564879565e-05\n",
      "Batch 5,150 of 9,631 Elased 0:36:29. Training loss: 3.464952835018195. Learning Rate: 5.500645188145188e-05\n",
      "Batch 5,200 of 9,631 Elased 0:36:50. Training loss: 3.46447046146943. Learning Rate: 5.498910811410811e-05\n",
      "Batch 5,250 of 9,631 Elased 0:37:12. Training loss: 3.464447600818816. Learning Rate: 5.4971764346764354e-05\n",
      "Batch 5,300 of 9,631 Elased 0:37:33. Training loss: 3.463835461004725. Learning Rate: 5.4954420579420584e-05\n",
      "Batch 5,350 of 9,631 Elased 0:37:54. Training loss: 3.465788006381454. Learning Rate: 5.493707681207681e-05\n",
      "Batch 5,400 of 9,631 Elased 0:38:16. Training loss: 3.4657374996609156. Learning Rate: 5.491973304473305e-05\n",
      "Batch 5,450 of 9,631 Elased 0:38:38. Training loss: 3.4659510836470018. Learning Rate: 5.490238927738928e-05\n",
      "Batch 5,500 of 9,631 Elased 0:39:00. Training loss: 3.466207231955095. Learning Rate: 5.488504551004551e-05\n",
      "Batch 5,550 of 9,631 Elased 0:39:22. Training loss: 3.466594251297616. Learning Rate: 5.486770174270175e-05\n",
      "Batch 5,600 of 9,631 Elased 0:39:44. Training loss: 3.4655862396103996. Learning Rate: 5.485035797535798e-05\n",
      "Batch 5,650 of 9,631 Elased 0:40:06. Training loss: 3.4672891007482476. Learning Rate: 5.483301420801421e-05\n",
      "Batch 5,700 of 9,631 Elased 0:40:28. Training loss: 3.4672358342848324. Learning Rate: 5.481567044067044e-05\n",
      "Batch 5,750 of 9,631 Elased 0:40:50. Training loss: 3.466852612308834. Learning Rate: 5.4798326673326674e-05\n",
      "Batch 5,800 of 9,631 Elased 0:41:12. Training loss: 3.466941336824976. Learning Rate: 5.478098290598292e-05\n",
      "Batch 5,850 of 9,631 Elased 0:41:33. Training loss: 3.4674153974117377. Learning Rate: 5.4763639138639146e-05\n",
      "Batch 5,900 of 9,631 Elased 0:41:54. Training loss: 3.467252338437711. Learning Rate: 5.4746295371295376e-05\n",
      "Batch 5,950 of 9,631 Elased 0:42:15. Training loss: 3.4677852333493595. Learning Rate: 5.4728951603951605e-05\n",
      "Batch 6,000 of 9,631 Elased 0:42:37. Training loss: 3.4675578259825706. Learning Rate: 5.4711607836607834e-05\n",
      "Batch 6,050 of 9,631 Elased 0:42:58. Training loss: 3.467626870663698. Learning Rate: 5.469426406926407e-05\n",
      "Batch 6,100 of 9,631 Elased 0:43:19. Training loss: 3.467884186115421. Learning Rate: 5.4676920301920306e-05\n",
      "Batch 6,150 of 9,631 Elased 0:43:40. Training loss: 3.4677245700456263. Learning Rate: 5.465957653457654e-05\n",
      "Batch 6,200 of 9,631 Elased 0:44:01. Training loss: 3.4656807944274717. Learning Rate: 5.464223276723277e-05\n",
      "Batch 6,250 of 9,631 Elased 0:44:22. Training loss: 3.465372175579071. Learning Rate: 5.4624888999889e-05\n",
      "Batch 6,300 of 9,631 Elased 0:44:43. Training loss: 3.465134326333091. Learning Rate: 5.460754523254523e-05\n",
      "Batch 6,350 of 9,631 Elased 0:45:05. Training loss: 3.4646622891313448. Learning Rate: 5.459020146520146e-05\n",
      "Batch 6,400 of 9,631 Elased 0:45:26. Training loss: 3.4643821600824594. Learning Rate: 5.45728576978577e-05\n",
      "Batch 6,450 of 9,631 Elased 0:45:47. Training loss: 3.4657926825959553. Learning Rate: 5.455551393051394e-05\n",
      "Batch 6,500 of 9,631 Elased 0:46:08. Training loss: 3.463988695511451. Learning Rate: 5.453817016317017e-05\n",
      "Batch 6,550 of 9,631 Elased 0:46:30. Training loss: 3.464091335835348. Learning Rate: 5.45208263958264e-05\n",
      "Batch 6,600 of 9,631 Elased 0:46:51. Training loss: 3.4648032185886843. Learning Rate: 5.4503482628482626e-05\n",
      "Batch 6,650 of 9,631 Elased 0:47:12. Training loss: 3.4649224230042077. Learning Rate: 5.448613886113887e-05\n",
      "Batch 6,700 of 9,631 Elased 0:47:33. Training loss: 3.4659988197639806. Learning Rate: 5.44687950937951e-05\n",
      "Batch 6,750 of 9,631 Elased 0:47:54. Training loss: 3.4659038460342972. Learning Rate: 5.445145132645133e-05\n",
      "Batch 6,800 of 9,631 Elased 0:48:16. Training loss: 3.4655224829210955. Learning Rate: 5.443410755910756e-05\n",
      "Batch 6,850 of 9,631 Elased 0:48:37. Training loss: 3.4648057751585966. Learning Rate: 5.441676379176379e-05\n",
      "Batch 6,900 of 9,631 Elased 0:48:58. Training loss: 3.4646258328444715. Learning Rate: 5.439942002442002e-05\n",
      "Batch 6,950 of 9,631 Elased 0:49:20. Training loss: 3.464418532265176. Learning Rate: 5.4382076257076265e-05\n",
      "Batch 7,000 of 9,631 Elased 0:49:41. Training loss: 3.4636964425529753. Learning Rate: 5.4364732489732494e-05\n",
      "Batch 7,050 of 9,631 Elased 0:50:02. Training loss: 3.463066365482114. Learning Rate: 5.434738872238872e-05\n",
      "Batch 7,100 of 9,631 Elased 0:50:23. Training loss: 3.463156035970634. Learning Rate: 5.433004495504496e-05\n",
      "Batch 7,150 of 9,631 Elased 0:50:45. Training loss: 3.4628692730490145. Learning Rate: 5.431270118770119e-05\n",
      "Batch 7,200 of 9,631 Elased 0:51:06. Training loss: 3.463445838044087. Learning Rate: 5.429535742035742e-05\n",
      "Batch 7,250 of 9,631 Elased 0:51:27. Training loss: 3.4621814056429367. Learning Rate: 5.427801365301366e-05\n",
      "Batch 7,300 of 9,631 Elased 0:51:48. Training loss: 3.462591185945354. Learning Rate: 5.426066988566989e-05\n",
      "Batch 7,350 of 9,631 Elased 0:52:10. Training loss: 3.4622462132025738. Learning Rate: 5.424332611832612e-05\n",
      "Batch 7,400 of 9,631 Elased 0:52:31. Training loss: 3.460911795819128. Learning Rate: 5.422598235098235e-05\n",
      "Batch 7,450 of 9,631 Elased 0:52:53. Training loss: 3.4608357461986925. Learning Rate: 5.4208638583638584e-05\n",
      "Batch 7,500 of 9,631 Elased 0:53:14. Training loss: 3.4596625680128734. Learning Rate: 5.419129481629483e-05\n",
      "Batch 7,550 of 9,631 Elased 0:53:35. Training loss: 3.4595928845973996. Learning Rate: 5.4173951048951057e-05\n",
      "Batch 7,600 of 9,631 Elased 0:53:57. Training loss: 3.4599908960022425. Learning Rate: 5.4156607281607286e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4,600 of 9,631 Elased 0:32:35. Training loss: 3.4304363300489342. Learning Rate: 5.185647685647685e-05\n",
      "Batch 4,650 of 9,631 Elased 0:32:55. Training loss: 3.429594017203136. Learning Rate: 5.1839133089133094e-05\n",
      "Batch 4,700 of 9,631 Elased 0:33:17. Training loss: 3.429761177428225. Learning Rate: 5.182178932178933e-05\n",
      "Batch 4,750 of 9,631 Elased 0:33:38. Training loss: 3.430469931100544. Learning Rate: 5.180444555444556e-05\n",
      "Batch 4,800 of 9,631 Elased 0:33:59. Training loss: 3.428769943366448. Learning Rate: 5.178710178710179e-05\n",
      "Batch 4,850 of 9,631 Elased 0:34:20. Training loss: 3.42782984846646. Learning Rate: 5.176975801975802e-05\n",
      "Batch 4,900 of 9,631 Elased 0:34:42. Training loss: 3.4287430692205625. Learning Rate: 5.175241425241425e-05\n",
      "Batch 4,950 of 9,631 Elased 0:35:03. Training loss: 3.429219219901345. Learning Rate: 5.173507048507049e-05\n",
      "Batch 5,000 of 9,631 Elased 0:35:24. Training loss: 3.430415325689316. Learning Rate: 5.171772671772672e-05\n",
      "Batch 5,050 of 9,631 Elased 0:35:45. Training loss: 3.4303361412558226. Learning Rate: 5.1700382950382955e-05\n",
      "Batch 5,100 of 9,631 Elased 0:36:07. Training loss: 3.42847415933422. Learning Rate: 5.1683039183039185e-05\n",
      "Batch 5,150 of 9,631 Elased 0:36:28. Training loss: 3.428770150420735. Learning Rate: 5.1665695415695414e-05\n",
      "Batch 5,200 of 9,631 Elased 0:36:49. Training loss: 3.4286475460575176. Learning Rate: 5.164835164835166e-05\n",
      "Batch 5,250 of 9,631 Elased 0:37:11. Training loss: 3.4285382815542675. Learning Rate: 5.1631007881007886e-05\n",
      "Batch 5,300 of 9,631 Elased 0:37:32. Training loss: 3.428038095955579. Learning Rate: 5.1613664113664115e-05\n",
      "Batch 5,350 of 9,631 Elased 0:37:53. Training loss: 3.430028885970606. Learning Rate: 5.159632034632035e-05\n",
      "Batch 5,400 of 9,631 Elased 0:38:15. Training loss: 3.430039808154106. Learning Rate: 5.157897657897658e-05\n",
      "Batch 5,450 of 9,631 Elased 0:38:36. Training loss: 3.430278969532853. Learning Rate: 5.156163281163281e-05\n",
      "Batch 5,500 of 9,631 Elased 0:38:57. Training loss: 3.430508471510627. Learning Rate: 5.154428904428905e-05\n",
      "Batch 5,550 of 9,631 Elased 0:39:18. Training loss: 3.4309816038501157. Learning Rate: 5.152694527694528e-05\n",
      "Batch 5,600 of 9,631 Elased 0:39:39. Training loss: 3.4303134243616036. Learning Rate: 5.150960150960151e-05\n",
      "Batch 5,650 of 9,631 Elased 0:40:01. Training loss: 3.432248351405152. Learning Rate: 5.149225774225774e-05\n",
      "Batch 5,700 of 9,631 Elased 0:40:22. Training loss: 3.432143862226553. Learning Rate: 5.1474913974913976e-05\n",
      "Batch 5,750 of 9,631 Elased 0:40:43. Training loss: 3.431732223738795. Learning Rate: 5.1457570207570206e-05\n",
      "Batch 5,800 of 9,631 Elased 0:41:04. Training loss: 3.4317448218320976. Learning Rate: 5.144022644022645e-05\n",
      "Batch 5,850 of 9,631 Elased 0:41:25. Training loss: 3.4322764513839004. Learning Rate: 5.142288267288268e-05\n",
      "Batch 5,900 of 9,631 Elased 0:41:46. Training loss: 3.4321349155902863. Learning Rate: 5.140553890553891e-05\n",
      "Batch 5,950 of 9,631 Elased 0:42:08. Training loss: 3.432624517709267. Learning Rate: 5.1388195138195136e-05\n",
      "Batch 6,000 of 9,631 Elased 0:42:29. Training loss: 3.432266723573208. Learning Rate: 5.137085137085137e-05\n",
      "Batch 6,050 of 9,631 Elased 0:42:50. Training loss: 3.43242730591908. Learning Rate: 5.135350760350761e-05\n",
      "Batch 6,100 of 9,631 Elased 0:43:11. Training loss: 3.432534793068151. Learning Rate: 5.1336163836163845e-05\n",
      "Batch 6,150 of 9,631 Elased 0:43:32. Training loss: 3.4322222299692107. Learning Rate: 5.1318820068820074e-05\n",
      "Batch 6,200 of 9,631 Elased 0:43:53. Training loss: 3.430323614285838. Learning Rate: 5.13014763014763e-05\n",
      "Batch 6,250 of 9,631 Elased 0:44:14. Training loss: 3.429821447544098. Learning Rate: 5.128413253413253e-05\n",
      "Batch 6,300 of 9,631 Elased 0:44:35. Training loss: 3.4297251193864007. Learning Rate: 5.126678876678876e-05\n",
      "Batch 6,350 of 9,631 Elased 0:44:57. Training loss: 3.4293986762039306. Learning Rate: 5.1249444999445004e-05\n",
      "Batch 6,400 of 9,631 Elased 0:45:18. Training loss: 3.428725086078048. Learning Rate: 5.123210123210124e-05\n",
      "Batch 6,450 of 9,631 Elased 0:45:39. Training loss: 3.4301643381562346. Learning Rate: 5.121475746475747e-05\n",
      "Batch 6,500 of 9,631 Elased 0:46:00. Training loss: 3.4283517048909116. Learning Rate: 5.11974136974137e-05\n",
      "Batch 6,550 of 9,631 Elased 0:46:21. Training loss: 3.4287123895237466. Learning Rate: 5.118006993006993e-05\n",
      "Batch 6,600 of 9,631 Elased 0:46:43. Training loss: 3.4294353478966335. Learning Rate: 5.116272616272616e-05\n",
      "Batch 6,650 of 9,631 Elased 0:47:04. Training loss: 3.4295815500101647. Learning Rate: 5.11453823953824e-05\n",
      "Batch 6,700 of 9,631 Elased 0:47:25. Training loss: 3.4309446096064438. Learning Rate: 5.112803862803863e-05\n",
      "Batch 6,750 of 9,631 Elased 0:47:47. Training loss: 3.4310004645100345. Learning Rate: 5.1110694860694866e-05\n",
      "Batch 6,800 of 9,631 Elased 0:48:08. Training loss: 3.4307495953054987. Learning Rate: 5.1093351093351095e-05\n",
      "Batch 6,850 of 9,631 Elased 0:48:29. Training loss: 3.4301894606987053. Learning Rate: 5.1076007326007324e-05\n",
      "Batch 6,900 of 9,631 Elased 0:48:50. Training loss: 3.4298094742885534. Learning Rate: 5.105866355866357e-05\n",
      "Batch 6,950 of 9,631 Elased 0:49:11. Training loss: 3.42977139051012. Learning Rate: 5.1041319791319796e-05\n",
      "Batch 7,000 of 9,631 Elased 0:49:32. Training loss: 3.4292793323653084. Learning Rate: 5.1023976023976026e-05\n",
      "Batch 7,050 of 9,631 Elased 0:49:53. Training loss: 3.428324848405013. Learning Rate: 5.100663225663226e-05\n",
      "Batch 7,100 of 9,631 Elased 0:50:14. Training loss: 3.4285261634705773. Learning Rate: 5.098928848928849e-05\n",
      "Batch 7,150 of 9,631 Elased 0:50:36. Training loss: 3.428557502206389. Learning Rate: 5.097194472194472e-05\n",
      "Batch 7,200 of 9,631 Elased 0:50:57. Training loss: 3.429077846010526. Learning Rate: 5.095460095460096e-05\n",
      "Batch 7,250 of 9,631 Elased 0:51:18. Training loss: 3.4279692605446126. Learning Rate: 5.093725718725719e-05\n",
      "Batch 7,300 of 9,631 Elased 0:51:39. Training loss: 3.4283945858315246. Learning Rate: 5.091991341991342e-05\n",
      "Batch 7,350 of 9,631 Elased 0:52:00. Training loss: 3.4277576928884805. Learning Rate: 5.090256965256965e-05\n",
      "Batch 7,400 of 9,631 Elased 0:52:22. Training loss: 3.426358636198817. Learning Rate: 5.088522588522589e-05\n",
      "Batch 7,450 of 9,631 Elased 0:52:44. Training loss: 3.4261186983121323. Learning Rate: 5.0867882117882116e-05\n",
      "Batch 7,500 of 9,631 Elased 0:53:06. Training loss: 3.4250356479962667. Learning Rate: 5.085053835053836e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5,250 of 9,631 Elased 0:37:14. Training loss: 3.4015482333274116. Learning Rate: 4.829025141525142e-05\n",
      "Batch 5,300 of 9,631 Elased 0:37:35. Training loss: 3.4010868614349725. Learning Rate: 4.8272907647907654e-05\n",
      "Batch 5,350 of 9,631 Elased 0:37:57. Training loss: 3.4032272507988406. Learning Rate: 4.825556388056388e-05\n",
      "Batch 5,400 of 9,631 Elased 0:38:18. Training loss: 3.402980342242453. Learning Rate: 4.823822011322012e-05\n",
      "Batch 5,450 of 9,631 Elased 0:38:39. Training loss: 3.4029814128044547. Learning Rate: 4.822087634587635e-05\n",
      "Batch 5,500 of 9,631 Elased 0:39:00. Training loss: 3.4032308949773964. Learning Rate: 4.820353257853258e-05\n",
      "Batch 5,550 of 9,631 Elased 0:39:21. Training loss: 3.4033247827194835. Learning Rate: 4.8186188811188814e-05\n",
      "Batch 5,600 of 9,631 Elased 0:39:42. Training loss: 3.4026846256639276. Learning Rate: 4.816884504384504e-05\n",
      "Batch 5,650 of 9,631 Elased 0:40:03. Training loss: 3.4041295022247113. Learning Rate: 4.815150127650128e-05\n",
      "Batch 5,700 of 9,631 Elased 0:40:25. Training loss: 3.4041600136798724. Learning Rate: 4.8134157509157515e-05\n",
      "Batch 5,750 of 9,631 Elased 0:40:46. Training loss: 3.4036172675879106. Learning Rate: 4.8116813741813744e-05\n",
      "Batch 5,800 of 9,631 Elased 0:41:07. Training loss: 3.4038007860553674. Learning Rate: 4.809946997446998e-05\n",
      "Batch 5,850 of 9,631 Elased 0:41:29. Training loss: 3.4043873303772036. Learning Rate: 4.808212620712621e-05\n",
      "Batch 5,900 of 9,631 Elased 0:41:50. Training loss: 3.4043249491715835. Learning Rate: 4.806478243978244e-05\n",
      "Batch 5,950 of 9,631 Elased 0:42:11. Training loss: 3.404715114381133. Learning Rate: 4.8047438672438675e-05\n",
      "Batch 6,000 of 9,631 Elased 0:42:33. Training loss: 3.4042717393040656. Learning Rate: 4.803009490509491e-05\n",
      "Batch 6,050 of 9,631 Elased 0:42:54. Training loss: 3.4045007369340943. Learning Rate: 4.801275113775114e-05\n",
      "Batch 6,100 of 9,631 Elased 0:43:16. Training loss: 3.4049425777646363. Learning Rate: 4.7995407370407376e-05\n",
      "Batch 6,150 of 9,631 Elased 0:43:37. Training loss: 3.4047981748542164. Learning Rate: 4.7978063603063605e-05\n",
      "Batch 6,200 of 9,631 Elased 0:43:58. Training loss: 3.4029109061341134. Learning Rate: 4.7960719835719835e-05\n",
      "Batch 6,250 of 9,631 Elased 0:44:19. Training loss: 3.4027615242958067. Learning Rate: 4.794337606837607e-05\n",
      "Batch 6,300 of 9,631 Elased 0:44:40. Training loss: 3.4025563775736187. Learning Rate: 4.79260323010323e-05\n",
      "Batch 6,350 of 9,631 Elased 0:45:01. Training loss: 3.40202950892486. Learning Rate: 4.7908688533688536e-05\n",
      "Batch 6,400 of 9,631 Elased 0:45:23. Training loss: 3.4016985868476333. Learning Rate: 4.789134476634477e-05\n",
      "Batch 6,450 of 9,631 Elased 0:45:45. Training loss: 3.403051911010299. Learning Rate: 4.7874000999001e-05\n",
      "Batch 6,500 of 9,631 Elased 0:46:06. Training loss: 3.401302995590063. Learning Rate: 4.785665723165723e-05\n",
      "Batch 6,550 of 9,631 Elased 0:46:27. Training loss: 3.4016174790149427. Learning Rate: 4.783931346431347e-05\n",
      "Batch 6,600 of 9,631 Elased 0:46:48. Training loss: 3.4024094923156682. Learning Rate: 4.7821969696969696e-05\n",
      "Batch 6,650 of 9,631 Elased 0:47:09. Training loss: 3.402849551304839. Learning Rate: 4.780462592962593e-05\n",
      "Batch 6,700 of 9,631 Elased 0:47:30. Training loss: 3.403919913146033. Learning Rate: 4.778728216228217e-05\n",
      "Batch 6,750 of 9,631 Elased 0:47:52. Training loss: 3.403870368833895. Learning Rate: 4.77699383949384e-05\n",
      "Batch 6,800 of 9,631 Elased 0:48:13. Training loss: 3.403491343207219. Learning Rate: 4.775259462759463e-05\n",
      "Batch 6,850 of 9,631 Elased 0:48:34. Training loss: 3.402840412568002. Learning Rate: 4.773525086025086e-05\n",
      "Batch 6,900 of 9,631 Elased 0:48:55. Training loss: 3.402765813616739. Learning Rate: 4.771790709290709e-05\n",
      "Batch 6,950 of 9,631 Elased 0:49:16. Training loss: 3.402488485566146. Learning Rate: 4.770056332556333e-05\n",
      "Batch 7,000 of 9,631 Elased 0:49:38. Training loss: 3.4017788381406238. Learning Rate: 4.7683219558219564e-05\n",
      "Batch 7,050 of 9,631 Elased 0:49:59. Training loss: 3.4010944215118464. Learning Rate: 4.766587579087579e-05\n",
      "Batch 7,100 of 9,631 Elased 0:50:21. Training loss: 3.4011098732243124. Learning Rate: 4.764853202353203e-05\n",
      "Batch 7,150 of 9,631 Elased 0:50:42. Training loss: 3.400866347643045. Learning Rate: 4.763118825618826e-05\n",
      "Batch 7,200 of 9,631 Elased 0:51:03. Training loss: 3.4011884238157006. Learning Rate: 4.761384448884449e-05\n",
      "Batch 7,250 of 9,631 Elased 0:51:24. Training loss: 3.400016505718231. Learning Rate: 4.7596500721500724e-05\n",
      "Batch 7,300 of 9,631 Elased 0:51:44. Training loss: 3.40025887148021. Learning Rate: 4.757915695415695e-05\n",
      "Batch 7,350 of 9,631 Elased 0:52:06. Training loss: 3.3998726167646396. Learning Rate: 4.756181318681319e-05\n",
      "Batch 7,400 of 9,631 Elased 0:52:27. Training loss: 3.398536666905558. Learning Rate: 4.7544469419469425e-05\n",
      "Batch 7,450 of 9,631 Elased 0:52:49. Training loss: 3.3985504292321687. Learning Rate: 4.7527125652125654e-05\n",
      "Batch 7,500 of 9,631 Elased 0:53:10. Training loss: 3.397620663690567. Learning Rate: 4.7509781884781884e-05\n",
      "Batch 7,550 of 9,631 Elased 0:53:31. Training loss: 3.397685967707476. Learning Rate: 4.749243811743812e-05\n",
      "Batch 7,600 of 9,631 Elased 0:53:52. Training loss: 3.3979930092629633. Learning Rate: 4.747509435009435e-05\n",
      "Batch 7,650 of 9,631 Elased 0:54:13. Training loss: 3.398032928541595. Learning Rate: 4.7457750582750585e-05\n",
      "Batch 7,700 of 9,631 Elased 0:54:35. Training loss: 3.397138142647681. Learning Rate: 4.744040681540682e-05\n",
      "Batch 7,750 of 9,631 Elased 0:54:56. Training loss: 3.3954965616041615. Learning Rate: 4.742306304806305e-05\n",
      "Batch 7,800 of 9,631 Elased 0:55:18. Training loss: 3.3958048021182035. Learning Rate: 4.7405719280719286e-05\n",
      "Batch 7,850 of 9,631 Elased 0:55:39. Training loss: 3.396195659394477. Learning Rate: 4.7388375513375516e-05\n",
      "Batch 7,900 of 9,631 Elased 0:56:00. Training loss: 3.395057646051238. Learning Rate: 4.7371031746031745e-05\n",
      "Batch 7,950 of 9,631 Elased 0:56:22. Training loss: 3.3964440986945195. Learning Rate: 4.735368797868798e-05\n",
      "Batch 8,000 of 9,631 Elased 0:56:43. Training loss: 3.3959651719927786. Learning Rate: 4.733634421134421e-05\n",
      "Batch 8,050 of 9,631 Elased 0:57:04. Training loss: 3.3974253558816376. Learning Rate: 4.7319000444000446e-05\n",
      "Batch 8,100 of 9,631 Elased 0:57:25. Training loss: 3.3976023439713465. Learning Rate: 4.730165667665668e-05\n",
      "Batch 8,150 of 9,631 Elased 0:57:46. Training loss: 3.3984002337719033. Learning Rate: 4.728431290931291e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6,050 of 9,631 Elased 0:42:51. Training loss: 3.3764190963280103. Learning Rate: 4.467199467199467e-05\n",
      "Batch 6,100 of 9,631 Elased 0:43:12. Training loss: 3.376448462361195. Learning Rate: 4.465465090465091e-05\n",
      "Batch 6,150 of 9,631 Elased 0:43:33. Training loss: 3.3763619048420974. Learning Rate: 4.463730713730714e-05\n",
      "Batch 6,200 of 9,631 Elased 0:43:55. Training loss: 3.374411836939473. Learning Rate: 4.461996336996337e-05\n",
      "Batch 6,250 of 9,631 Elased 0:44:16. Training loss: 3.3739477695083617. Learning Rate: 4.46026196026196e-05\n",
      "Batch 6,300 of 9,631 Elased 0:44:37. Training loss: 3.373849691341794. Learning Rate: 4.458527583527584e-05\n",
      "Batch 6,350 of 9,631 Elased 0:44:58. Training loss: 3.3736583964467988. Learning Rate: 4.4567932067932074e-05\n",
      "Batch 6,400 of 9,631 Elased 0:45:20. Training loss: 3.373409980442375. Learning Rate: 4.4550588300588304e-05\n",
      "Batch 6,450 of 9,631 Elased 0:45:41. Training loss: 3.374484322625537. Learning Rate: 4.453324453324453e-05\n",
      "Batch 6,500 of 9,631 Elased 0:46:02. Training loss: 3.372846529355416. Learning Rate: 4.451590076590077e-05\n",
      "Batch 6,550 of 9,631 Elased 0:46:24. Training loss: 3.3732797301452577. Learning Rate: 4.4498556998557e-05\n",
      "Batch 6,600 of 9,631 Elased 0:46:44. Training loss: 3.3737934759349533. Learning Rate: 4.4481213231213234e-05\n",
      "Batch 6,650 of 9,631 Elased 0:47:05. Training loss: 3.374000542414816. Learning Rate: 4.446386946386947e-05\n",
      "Batch 6,700 of 9,631 Elased 0:47:27. Training loss: 3.3751067789810807. Learning Rate: 4.44465256965257e-05\n",
      "Batch 6,750 of 9,631 Elased 0:47:48. Training loss: 3.374823086650283. Learning Rate: 4.442918192918193e-05\n",
      "Batch 6,800 of 9,631 Elased 0:48:09. Training loss: 3.3747143220726183. Learning Rate: 4.4411838161838165e-05\n",
      "Batch 6,850 of 9,631 Elased 0:48:30. Training loss: 3.3738409683304114. Learning Rate: 4.4394494394494394e-05\n",
      "Batch 6,900 of 9,631 Elased 0:48:52. Training loss: 3.3734882686967436. Learning Rate: 4.4377150627150623e-05\n",
      "Batch 6,950 of 9,631 Elased 0:49:13. Training loss: 3.3734135966163743. Learning Rate: 4.435980685980686e-05\n",
      "Batch 7,000 of 9,631 Elased 0:49:34. Training loss: 3.3727715342896327. Learning Rate: 4.4342463092463096e-05\n",
      "Batch 7,050 of 9,631 Elased 0:49:55. Training loss: 3.371869067553933. Learning Rate: 4.432511932511933e-05\n",
      "Batch 7,100 of 9,631 Elased 0:50:17. Training loss: 3.372144255822813. Learning Rate: 4.430777555777556e-05\n",
      "Batch 7,150 of 9,631 Elased 0:50:38. Training loss: 3.371975462169914. Learning Rate: 4.429043179043179e-05\n",
      "Batch 7,200 of 9,631 Elased 0:50:59. Training loss: 3.3717436230348214. Learning Rate: 4.4273088023088026e-05\n",
      "Batch 7,250 of 9,631 Elased 0:51:20. Training loss: 3.3707611578086327. Learning Rate: 4.4255744255744255e-05\n",
      "Batch 7,300 of 9,631 Elased 0:51:42. Training loss: 3.3709517995462024. Learning Rate: 4.423840048840049e-05\n",
      "Batch 7,350 of 9,631 Elased 0:52:03. Training loss: 3.3705260384650457. Learning Rate: 4.422105672105673e-05\n",
      "Batch 7,400 of 9,631 Elased 0:52:24. Training loss: 3.369248262901564. Learning Rate: 4.420371295371296e-05\n",
      "Batch 7,450 of 9,631 Elased 0:52:45. Training loss: 3.368988346029448. Learning Rate: 4.4186369186369186e-05\n",
      "Batch 7,500 of 9,631 Elased 0:53:06. Training loss: 3.368091179529826. Learning Rate: 4.416902541902542e-05\n",
      "Batch 7,550 of 9,631 Elased 0:53:28. Training loss: 3.367770592424254. Learning Rate: 4.415168165168165e-05\n",
      "Batch 7,600 of 9,631 Elased 0:53:49. Training loss: 3.3683059186057043. Learning Rate: 4.413433788433789e-05\n",
      "Batch 7,650 of 9,631 Elased 0:54:11. Training loss: 3.3684515494614646. Learning Rate: 4.4116994116994123e-05\n",
      "Batch 7,700 of 9,631 Elased 0:54:32. Training loss: 3.3677391902192846. Learning Rate: 4.409965034965035e-05\n",
      "Batch 7,750 of 9,631 Elased 0:54:53. Training loss: 3.366232764367134. Learning Rate: 4.408230658230658e-05\n",
      "Batch 7,800 of 9,631 Elased 0:55:15. Training loss: 3.3670234137315016. Learning Rate: 4.406496281496282e-05\n",
      "Batch 7,850 of 9,631 Elased 0:55:35. Training loss: 3.367397988131092. Learning Rate: 4.404761904761905e-05\n",
      "Batch 7,900 of 9,631 Elased 0:55:57. Training loss: 3.366066598484788. Learning Rate: 4.403027528027528e-05\n",
      "Batch 7,950 of 9,631 Elased 0:56:18. Training loss: 3.367472027427745. Learning Rate: 4.401293151293151e-05\n",
      "Batch 8,000 of 9,631 Elased 0:56:39. Training loss: 3.3670676339417698. Learning Rate: 4.399558774558775e-05\n",
      "Batch 8,050 of 9,631 Elased 0:57:00. Training loss: 3.3680756655242874. Learning Rate: 4.3978243978243985e-05\n",
      "Batch 8,100 of 9,631 Elased 0:57:22. Training loss: 3.368397271088612. Learning Rate: 4.3960900210900214e-05\n",
      "Batch 8,150 of 9,631 Elased 0:57:43. Training loss: 3.3691396324619927. Learning Rate: 4.394355644355644e-05\n",
      "Batch 8,200 of 9,631 Elased 0:58:05. Training loss: 3.3688424789469416. Learning Rate: 4.392621267621268e-05\n",
      "Batch 8,250 of 9,631 Elased 0:58:26. Training loss: 3.3695899872490855. Learning Rate: 4.390886890886891e-05\n",
      "Batch 8,300 of 9,631 Elased 0:58:47. Training loss: 3.369609342181539. Learning Rate: 4.3891525141525145e-05\n",
      "Batch 8,350 of 9,631 Elased 0:59:09. Training loss: 3.369203950559308. Learning Rate: 4.387418137418138e-05\n",
      "Batch 8,400 of 9,631 Elased 0:59:29. Training loss: 3.368372365647838. Learning Rate: 4.385683760683761e-05\n",
      "Batch 8,450 of 9,631 Elased 0:59:50. Training loss: 3.3676919148377413. Learning Rate: 4.383949383949384e-05\n",
      "Batch 8,500 of 9,631 Elased 1:00:11. Training loss: 3.3673839041064766. Learning Rate: 4.3822150072150075e-05\n",
      "Batch 8,550 of 9,631 Elased 1:00:32. Training loss: 3.36695990466235. Learning Rate: 4.3804806304806304e-05\n",
      "Batch 8,600 of 9,631 Elased 1:00:54. Training loss: 3.367454890137495. Learning Rate: 4.3787462537462534e-05\n",
      "Batch 8,650 of 9,631 Elased 1:01:15. Training loss: 3.3664621012334877. Learning Rate: 4.377011877011877e-05\n",
      "Batch 8,700 of 9,631 Elased 1:01:37. Training loss: 3.366156477503393. Learning Rate: 4.3752775002775006e-05\n",
      "Batch 8,750 of 9,631 Elased 1:01:58. Training loss: 3.3659464503833227. Learning Rate: 4.373543123543124e-05\n",
      "Batch 8,800 of 9,631 Elased 1:02:19. Training loss: 3.365863537978042. Learning Rate: 4.371808746808747e-05\n",
      "Batch 8,850 of 9,631 Elased 1:02:40. Training loss: 3.3655224125264054. Learning Rate: 4.37007437007437e-05\n",
      "Batch 8,900 of 9,631 Elased 1:03:01. Training loss: 3.365630079923051. Learning Rate: 4.3683399933399936e-05\n",
      "Batch 8,950 of 9,631 Elased 1:03:23. Training loss: 3.3654548489314884. Learning Rate: 4.3666056166056166e-05\n",
      "Batch 9,000 of 9,631 Elased 1:03:44. Training loss: 3.3644676346911324. Learning Rate: 4.36487123987124e-05\n",
      "Batch 9,050 of 9,631 Elased 1:04:05. Training loss: 3.365327088832855. Learning Rate: 4.363136863136864e-05\n",
      "Batch 9,100 of 9,631 Elased 1:04:26. Training loss: 3.3650421257726437. Learning Rate: 4.361402486402487e-05\n",
      "Batch 9,150 of 9,631 Elased 1:04:48. Training loss: 3.3652078503077147. Learning Rate: 4.3596681096681096e-05\n",
      "Batch 9,200 of 9,631 Elased 1:05:09. Training loss: 3.3657484500434087. Learning Rate: 4.357933732933733e-05\n",
      "Batch 9,250 of 9,631 Elased 1:05:30. Training loss: 3.3655389624801844. Learning Rate: 4.356199356199356e-05\n",
      "Batch 9,300 of 9,631 Elased 1:05:51. Training loss: 3.366173298833191. Learning Rate: 4.35446497946498e-05\n",
      "Batch 9,350 of 9,631 Elased 1:06:13. Training loss: 3.366103340253473. Learning Rate: 4.3527306027306034e-05\n",
      "Batch 9,400 of 9,631 Elased 1:06:33. Training loss: 3.365948602810819. Learning Rate: 4.350996225996226e-05\n",
      "Batch 9,450 of 9,631 Elased 1:06:55. Training loss: 3.365606713231909. Learning Rate: 4.349261849261849e-05\n",
      "Batch 9,500 of 9,631 Elased 1:07:16. Training loss: 3.365078516320178. Learning Rate: 4.347527472527473e-05\n",
      "Batch 9,550 of 9,631 Elased 1:07:37. Training loss: 3.364351526043178. Learning Rate: 4.345793095793096e-05\n",
      "Batch 9,600 of 9,631 Elased 1:07:59. Training loss: 3.3647493236139416. Learning Rate: 4.3440587190587194e-05\n",
      "\n",
      "\n",
      "  Average training loss: 3.36\n",
      "  Training epcoh took: 1:08:12\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463e5f1fac5b4ef3a287549b33800141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=67.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Validation Loss: 3.48\n",
      "  Validation took: 0:00:43\n",
      "\n",
      "======== Epoch 18 / 30 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f5962a86efa4a86883f598b06d96d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch    50 of 9,631 Elased 0:00:21. Training loss: 3.461851282119751. Learning Rate: 4.3412490287490285e-05\n",
      "Batch   100 of 9,631 Elased 0:00:42. Training loss: 3.3968516302108767. Learning Rate: 4.339514652014652e-05\n",
      "Batch   150 of 9,631 Elased 0:01:03. Training loss: 3.399154550234477. Learning Rate: 4.337780275280276e-05\n",
      "Batch   200 of 9,631 Elased 0:01:24. Training loss: 3.429931420087814. Learning Rate: 4.3360458985458986e-05\n",
      "Batch   250 of 9,631 Elased 0:01:46. Training loss: 3.37412619304657. Learning Rate: 4.334311521811522e-05\n",
      "Batch   300 of 9,631 Elased 0:02:07. Training loss: 3.357205644448598. Learning Rate: 4.332577145077145e-05\n",
      "Batch   350 of 9,631 Elased 0:02:27. Training loss: 3.3546202748162406. Learning Rate: 4.330842768342769e-05\n",
      "Batch   400 of 9,631 Elased 0:02:48. Training loss: 3.3657863533496855. Learning Rate: 4.3291083916083917e-05\n",
      "Batch   450 of 9,631 Elased 0:03:10. Training loss: 3.3705655638376872. Learning Rate: 4.327374014874015e-05\n",
      "Batch   500 of 9,631 Elased 0:03:31. Training loss: 3.3742575087547304. Learning Rate: 4.325639638139639e-05\n",
      "Batch   550 of 9,631 Elased 0:03:52. Training loss: 3.362160334153609. Learning Rate: 4.323905261405262e-05\n",
      "Batch   600 of 9,631 Elased 0:04:13. Training loss: 3.377086892922719. Learning Rate: 4.322170884670885e-05\n",
      "Batch   650 of 9,631 Elased 0:04:34. Training loss: 3.3778978725580067. Learning Rate: 4.320436507936508e-05\n",
      "Batch   700 of 9,631 Elased 0:04:56. Training loss: 3.370531416279929. Learning Rate: 4.318702131202131e-05\n",
      "Batch   750 of 9,631 Elased 0:05:17. Training loss: 3.357805106163025. Learning Rate: 4.316967754467754e-05\n",
      "Batch   800 of 9,631 Elased 0:05:38. Training loss: 3.3550836300849913. Learning Rate: 4.315233377733378e-05\n",
      "Batch   850 of 9,631 Elased 0:05:59. Training loss: 3.346414318084717. Learning Rate: 4.3134990009990014e-05\n",
      "Batch   900 of 9,631 Elased 0:06:20. Training loss: 3.3441645696428086. Learning Rate: 4.311764624264624e-05\n",
      "Batch   950 of 9,631 Elased 0:06:41. Training loss: 3.3402549194034776. Learning Rate: 4.310030247530248e-05\n",
      "Batch 1,000 of 9,631 Elased 0:07:03. Training loss: 3.340333869457245. Learning Rate: 4.308295870795871e-05\n",
      "Batch 1,050 of 9,631 Elased 0:07:24. Training loss: 3.3429907433191937. Learning Rate: 4.306561494061494e-05\n",
      "Batch 1,100 of 9,631 Elased 0:07:46. Training loss: 3.3434942037409003. Learning Rate: 4.3048271173271174e-05\n",
      "Batch 1,150 of 9,631 Elased 0:08:07. Training loss: 3.342722459046737. Learning Rate: 4.303092740592741e-05\n",
      "Batch 1,200 of 9,631 Elased 0:08:29. Training loss: 3.3441431403160093. Learning Rate: 4.3013583638583646e-05\n",
      "Batch 1,250 of 9,631 Elased 0:08:50. Training loss: 3.345389063167572. Learning Rate: 4.2996239871239875e-05\n",
      "Batch 1,300 of 9,631 Elased 0:09:11. Training loss: 3.3403201476427227. Learning Rate: 4.2978896103896104e-05\n",
      "Batch 1,350 of 9,631 Elased 0:09:32. Training loss: 3.3400685999128554. Learning Rate: 4.296155233655234e-05\n",
      "Batch 1,400 of 9,631 Elased 0:09:54. Training loss: 3.3387326206479755. Learning Rate: 4.294420856920857e-05\n",
      "Batch 1,450 of 9,631 Elased 0:10:15. Training loss: 3.337095539652068. Learning Rate: 4.2926864801864806e-05\n",
      "Batch 1,500 of 9,631 Elased 0:10:37. Training loss: 3.3353015542030335. Learning Rate: 4.290952103452104e-05\n",
      "Batch 1,550 of 9,631 Elased 0:10:59. Training loss: 3.335572412860009. Learning Rate: 4.289217726717727e-05\n",
      "Batch 1,600 of 9,631 Elased 0:11:21. Training loss: 3.339565393179655. Learning Rate: 4.28748334998335e-05\n",
      "Batch 1,650 of 9,631 Elased 0:11:43. Training loss: 3.3418781764579544. Learning Rate: 4.2857489732489736e-05\n",
      "Batch 1,700 of 9,631 Elased 0:12:05. Training loss: 3.3398382888120763. Learning Rate: 4.2840145965145966e-05\n",
      "Batch 1,750 of 9,631 Elased 0:12:27. Training loss: 3.340259955065591. Learning Rate: 4.2822802197802195e-05\n",
      "Batch 1,800 of 9,631 Elased 0:12:49. Training loss: 3.3390696031517453. Learning Rate: 4.280545843045843e-05\n",
      "Batch 1,850 of 9,631 Elased 0:13:11. Training loss: 3.3413194398622257. Learning Rate: 4.278811466311467e-05\n",
      "Batch 1,900 of 9,631 Elased 0:13:33. Training loss: 3.3408756972614087. Learning Rate: 4.2770770895770896e-05\n",
      "Batch 1,950 of 9,631 Elased 0:13:54. Training loss: 3.3372145597751324. Learning Rate: 4.275342712842713e-05\n",
      "Batch 2,000 of 9,631 Elased 0:14:15. Training loss: 3.3330083438158034. Learning Rate: 4.273608336108336e-05\n",
      "Batch 2,050 of 9,631 Elased 0:14:37. Training loss: 3.3342945796687427. Learning Rate: 4.27187395937396e-05\n",
      "Batch 2,100 of 9,631 Elased 0:14:58. Training loss: 3.3392529930387225. Learning Rate: 4.270139582639583e-05\n",
      "Batch 2,150 of 9,631 Elased 0:15:19. Training loss: 3.339784695714019. Learning Rate: 4.268405205905206e-05\n",
      "Batch 2,200 of 9,631 Elased 0:15:40. Training loss: 3.339744407046925. Learning Rate: 4.26667082917083e-05\n",
      "Batch 2,250 of 9,631 Elased 0:16:02. Training loss: 3.3373713342878553. Learning Rate: 4.264936452436453e-05\n",
      "Batch 2,300 of 9,631 Elased 0:16:23. Training loss: 3.338316865692968. Learning Rate: 4.263202075702076e-05\n",
      "Batch 2,350 of 9,631 Elased 0:16:44. Training loss: 3.3360702106800484. Learning Rate: 4.2614676989676994e-05\n",
      "Batch 2,400 of 9,631 Elased 0:17:06. Training loss: 3.3362352550029755. Learning Rate: 4.259733322233322e-05\n",
      "Batch 2,450 of 9,631 Elased 0:17:27. Training loss: 3.332513692427655. Learning Rate: 4.257998945498945e-05\n",
      "Batch 2,500 of 9,631 Elased 0:17:49. Training loss: 3.3311163002967836. Learning Rate: 4.256264568764569e-05\n",
      "Batch 2,550 of 9,631 Elased 0:18:10. Training loss: 3.336052406161439. Learning Rate: 4.2545301920301924e-05\n",
      "Batch 2,600 of 9,631 Elased 0:18:31. Training loss: 3.3390903380283943. Learning Rate: 4.2527958152958153e-05\n",
      "Batch 2,650 of 9,631 Elased 0:18:53. Training loss: 3.3417736680552643. Learning Rate: 4.251061438561439e-05\n",
      "Batch 2,700 of 9,631 Elased 0:19:13. Training loss: 3.3439047688025014. Learning Rate: 4.249327061827062e-05\n",
      "Batch 2,750 of 9,631 Elased 0:19:34. Training loss: 3.3412939277995717. Learning Rate: 4.247592685092685e-05\n",
      "Batch 2,800 of 9,631 Elased 0:19:56. Training loss: 3.342483946340425. Learning Rate: 4.2458583083583084e-05\n",
      "Batch 2,850 of 9,631 Elased 0:20:17. Training loss: 3.3455021086910315. Learning Rate: 4.244123931623932e-05\n",
      "Batch 2,900 of 9,631 Elased 0:20:38. Training loss: 3.3458137173488223. Learning Rate: 4.2423895548895556e-05\n",
      "Batch 2,950 of 9,631 Elased 0:20:59. Training loss: 3.346352738283448. Learning Rate: 4.2406551781551785e-05\n",
      "Batch 3,000 of 9,631 Elased 0:21:21. Training loss: 3.348671569188436. Learning Rate: 4.2389208014208015e-05\n",
      "Batch 3,050 of 9,631 Elased 0:21:42. Training loss: 3.348078053583864. Learning Rate: 4.237186424686425e-05\n",
      "Batch 3,100 of 9,631 Elased 0:22:03. Training loss: 3.3476089402168028. Learning Rate: 4.235452047952048e-05\n",
      "Batch 3,150 of 9,631 Elased 0:22:25. Training loss: 3.343819423630124. Learning Rate: 4.2337176712176716e-05\n",
      "Batch 3,200 of 9,631 Elased 0:22:46. Training loss: 3.345945878177881. Learning Rate: 4.231983294483295e-05\n",
      "Batch 3,250 of 9,631 Elased 0:23:08. Training loss: 3.3458886070251466. Learning Rate: 4.230248917748918e-05\n",
      "Batch 3,300 of 9,631 Elased 0:23:29. Training loss: 3.3477827324289264. Learning Rate: 4.228514541014541e-05\n",
      "Batch 3,350 of 9,631 Elased 0:23:50. Training loss: 3.348421853335936. Learning Rate: 4.226780164280165e-05\n",
      "Batch 3,400 of 9,631 Elased 0:24:11. Training loss: 3.348920187108657. Learning Rate: 4.2250457875457876e-05\n",
      "Batch 3,450 of 9,631 Elased 0:24:32. Training loss: 3.3493580138856087. Learning Rate: 4.2233114108114105e-05\n",
      "Batch 3,500 of 9,631 Elased 0:24:53. Training loss: 3.348120618377413. Learning Rate: 4.221577034077034e-05\n",
      "Batch 3,550 of 9,631 Elased 0:25:15. Training loss: 3.3492548734705214. Learning Rate: 4.219842657342658e-05\n",
      "Batch 3,600 of 9,631 Elased 0:25:36. Training loss: 3.35085687196917. Learning Rate: 4.2181082806082807e-05\n",
      "Batch 3,650 of 9,631 Elased 0:25:57. Training loss: 3.354546435205904. Learning Rate: 4.216373903873904e-05\n",
      "Batch 3,700 of 9,631 Elased 0:26:18. Training loss: 3.3534951999058595. Learning Rate: 4.214639527139527e-05\n",
      "Batch 3,750 of 9,631 Elased 0:26:40. Training loss: 3.3521475938479104. Learning Rate: 4.212905150405151e-05\n",
      "Batch 3,800 of 9,631 Elased 0:27:01. Training loss: 3.3509468311071395. Learning Rate: 4.211170773670774e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3,850 of 9,631 Elased 0:27:22. Training loss: 3.3482820175530073. Learning Rate: 4.209436396936397e-05\n",
      "Batch 3,900 of 9,631 Elased 0:27:43. Training loss: 3.347087758107063. Learning Rate: 4.207702020202021e-05\n",
      "Batch 3,950 of 9,631 Elased 0:28:04. Training loss: 3.348374889862688. Learning Rate: 4.205967643467644e-05\n",
      "Batch 4,000 of 9,631 Elased 0:28:25. Training loss: 3.3487391156852246. Learning Rate: 4.204233266733267e-05\n",
      "Batch 4,050 of 9,631 Elased 0:28:47. Training loss: 3.348185070031955. Learning Rate: 4.2024988899988904e-05\n",
      "Batch 4,100 of 9,631 Elased 0:29:08. Training loss: 3.3472811536963394. Learning Rate: 4.200764513264513e-05\n",
      "Batch 4,150 of 9,631 Elased 0:29:29. Training loss: 3.346770411198398. Learning Rate: 4.199030136530136e-05\n",
      "Batch 4,200 of 9,631 Elased 0:29:50. Training loss: 3.346960761405173. Learning Rate: 4.19729575979576e-05\n",
      "Batch 4,250 of 9,631 Elased 0:30:12. Training loss: 3.3470021679541646. Learning Rate: 4.1955613830613834e-05\n",
      "Batch 4,300 of 9,631 Elased 0:30:33. Training loss: 3.3469942373730417. Learning Rate: 4.1938270063270064e-05\n",
      "Batch 4,350 of 9,631 Elased 0:30:54. Training loss: 3.3466954852520736. Learning Rate: 4.19209262959263e-05\n",
      "Batch 4,400 of 9,631 Elased 0:31:15. Training loss: 3.3480023777213965. Learning Rate: 4.190358252858253e-05\n",
      "Batch 4,450 of 9,631 Elased 0:31:37. Training loss: 3.3487775458378737. Learning Rate: 4.188623876123876e-05\n",
      "Batch 4,500 of 9,631 Elased 0:31:58. Training loss: 3.3500770098103416. Learning Rate: 4.1868894993894994e-05\n",
      "Batch 4,550 of 9,631 Elased 0:32:19. Training loss: 3.34839687198073. Learning Rate: 4.185155122655123e-05\n",
      "Batch 4,600 of 9,631 Elased 0:32:41. Training loss: 3.349263647250507. Learning Rate: 4.1834207459207466e-05\n",
      "Batch 4,650 of 9,631 Elased 0:33:02. Training loss: 3.34820510912967. Learning Rate: 4.1816863691863696e-05\n",
      "Batch 4,700 of 9,631 Elased 0:33:23. Training loss: 3.34820001244545. Learning Rate: 4.1799519924519925e-05\n",
      "Batch 4,750 of 9,631 Elased 0:33:44. Training loss: 3.3483280697621796. Learning Rate: 4.178217615717616e-05\n",
      "Batch 4,800 of 9,631 Elased 0:34:05. Training loss: 3.346509337897102. Learning Rate: 4.176483238983239e-05\n",
      "Batch 4,850 of 9,631 Elased 0:34:27. Training loss: 3.3460275720075234. Learning Rate: 4.1747488622488626e-05\n",
      "Batch 4,900 of 9,631 Elased 0:34:48. Training loss: 3.3467279061249324. Learning Rate: 4.173014485514486e-05\n",
      "Batch 4,950 of 9,631 Elased 0:35:09. Training loss: 3.3472197267503447. Learning Rate: 4.171280108780109e-05\n",
      "Batch 5,000 of 9,631 Elased 0:35:31. Training loss: 3.3482346776008605. Learning Rate: 4.169545732045732e-05\n",
      "Batch 5,050 of 9,631 Elased 0:35:52. Training loss: 3.348031836405839. Learning Rate: 4.167811355311356e-05\n",
      "Batch 5,100 of 9,631 Elased 0:36:13. Training loss: 3.346463719816769. Learning Rate: 4.1660769785769786e-05\n",
      "Batch 5,150 of 9,631 Elased 0:36:35. Training loss: 3.346842320405164. Learning Rate: 4.1643426018426015e-05\n",
      "Batch 5,200 of 9,631 Elased 0:36:56. Training loss: 3.3463392567634584. Learning Rate: 4.162608225108225e-05\n",
      "Batch 5,250 of 9,631 Elased 0:37:17. Training loss: 3.3464108369463967. Learning Rate: 4.160873848373849e-05\n",
      "Batch 5,300 of 9,631 Elased 0:37:38. Training loss: 3.3457991732291457. Learning Rate: 4.159139471639472e-05\n",
      "Batch 5,350 of 9,631 Elased 0:37:59. Training loss: 3.347849816696666. Learning Rate: 4.157405094905095e-05\n",
      "Batch 5,400 of 9,631 Elased 0:38:20. Training loss: 3.3478873082002005. Learning Rate: 4.155670718170718e-05\n",
      "Batch 5,450 of 9,631 Elased 0:38:42. Training loss: 3.348091509276574. Learning Rate: 4.153936341436341e-05\n",
      "Batch 5,500 of 9,631 Elased 0:39:03. Training loss: 3.3484035906791685. Learning Rate: 4.152201964701965e-05\n",
      "Batch 5,550 of 9,631 Elased 0:39:24. Training loss: 3.3487087611035182. Learning Rate: 4.1504675879675884e-05\n",
      "Batch 5,600 of 9,631 Elased 0:39:45. Training loss: 3.3482069380794255. Learning Rate: 4.148733211233212e-05\n",
      "Batch 5,650 of 9,631 Elased 0:40:07. Training loss: 3.3500206873480196. Learning Rate: 4.146998834498835e-05\n",
      "Batch 5,700 of 9,631 Elased 0:40:28. Training loss: 3.349740436328085. Learning Rate: 4.145264457764458e-05\n",
      "Batch 5,750 of 9,631 Elased 0:40:49. Training loss: 3.349382560564124. Learning Rate: 4.1435300810300814e-05\n",
      "Batch 5,800 of 9,631 Elased 0:41:11. Training loss: 3.3494115193547875. Learning Rate: 4.1417957042957043e-05\n",
      "Batch 5,850 of 9,631 Elased 0:41:32. Training loss: 3.3496886828414394. Learning Rate: 4.140061327561327e-05\n",
      "Batch 5,900 of 9,631 Elased 0:41:53. Training loss: 3.3495344692367617. Learning Rate: 4.1383269508269515e-05\n",
      "Batch 5,950 of 9,631 Elased 0:42:14. Training loss: 3.3497452308550604. Learning Rate: 4.1365925740925745e-05\n",
      "Batch 6,000 of 9,631 Elased 0:42:35. Training loss: 3.349095235725244. Learning Rate: 4.1348581973581974e-05\n",
      "Batch 6,050 of 9,631 Elased 0:42:57. Training loss: 3.349380019932739. Learning Rate: 4.133123820623821e-05\n",
      "Batch 6,100 of 9,631 Elased 0:43:18. Training loss: 3.3496089971847223. Learning Rate: 4.131389443889444e-05\n",
      "Batch 6,150 of 9,631 Elased 0:43:39. Training loss: 3.3493117732730338. Learning Rate: 4.129655067155067e-05\n",
      "Batch 6,200 of 9,631 Elased 0:44:00. Training loss: 3.347507556580728. Learning Rate: 4.1279206904206905e-05\n",
      "Batch 6,250 of 9,631 Elased 0:44:22. Training loss: 3.3470108954048157. Learning Rate: 4.126186313686314e-05\n",
      "Batch 6,300 of 9,631 Elased 0:44:43. Training loss: 3.346813761525684. Learning Rate: 4.124451936951937e-05\n",
      "Batch 6,350 of 9,631 Elased 0:45:04. Training loss: 3.3463532059399164. Learning Rate: 4.1227175602175606e-05\n",
      "Batch 6,400 of 9,631 Elased 0:45:26. Training loss: 3.3457888989709317. Learning Rate: 4.1209831834831835e-05\n",
      "Batch 6,450 of 9,631 Elased 0:45:47. Training loss: 3.3468151823494785. Learning Rate: 4.119248806748807e-05\n",
      "Batch 6,500 of 9,631 Elased 0:46:08. Training loss: 3.3452885474608496. Learning Rate: 4.11751443001443e-05\n",
      "Batch 6,550 of 9,631 Elased 0:46:30. Training loss: 3.345639367048977. Learning Rate: 4.115780053280054e-05\n",
      "Batch 6,600 of 9,631 Elased 0:46:51. Training loss: 3.346177880926566. Learning Rate: 4.114045676545677e-05\n",
      "Batch 6,650 of 9,631 Elased 0:47:12. Training loss: 3.3465427931986356. Learning Rate: 4.1123112998113e-05\n",
      "Batch 6,700 of 9,631 Elased 0:47:34. Training loss: 3.3476415661377694. Learning Rate: 4.110576923076923e-05\n",
      "Batch 6,750 of 9,631 Elased 0:47:55. Training loss: 3.3476281764242386. Learning Rate: 4.108842546342547e-05\n",
      "Batch 6,800 of 9,631 Elased 0:48:16. Training loss: 3.3474949903172604. Learning Rate: 4.1071081696081697e-05\n",
      "Batch 6,850 of 9,631 Elased 0:48:37. Training loss: 3.347213347584662. Learning Rate: 4.1053737928737926e-05\n",
      "Batch 6,900 of 9,631 Elased 0:48:59. Training loss: 3.3471399253865948. Learning Rate: 4.103639416139416e-05\n",
      "Batch 6,950 of 9,631 Elased 0:49:20. Training loss: 3.347032050314567. Learning Rate: 4.10190503940504e-05\n",
      "Batch 7,000 of 9,631 Elased 0:49:41. Training loss: 3.3464200601748058. Learning Rate: 4.100170662670663e-05\n",
      "Batch 7,050 of 9,631 Elased 0:50:02. Training loss: 3.345799153730379. Learning Rate: 4.098436285936286e-05\n",
      "Batch 7,100 of 9,631 Elased 0:50:23. Training loss: 3.346105031497042. Learning Rate: 4.096701909201909e-05\n",
      "Batch 7,150 of 9,631 Elased 0:50:45. Training loss: 3.346082898820197. Learning Rate: 4.094967532467532e-05\n",
      "Batch 7,200 of 9,631 Elased 0:51:06. Training loss: 3.3462262603806123. Learning Rate: 4.093233155733156e-05\n",
      "Batch 7,250 of 9,631 Elased 0:51:27. Training loss: 3.345228734526141. Learning Rate: 4.0914987789987794e-05\n",
      "Batch 7,300 of 9,631 Elased 0:51:49. Training loss: 3.345502332220339. Learning Rate: 4.089764402264403e-05\n",
      "Batch 7,350 of 9,631 Elased 0:52:10. Training loss: 3.3448886565286284. Learning Rate: 4.088030025530026e-05\n",
      "Batch 7,400 of 9,631 Elased 0:52:31. Training loss: 3.343926907729458. Learning Rate: 4.086295648795649e-05\n",
      "Batch 7,450 of 9,631 Elased 0:52:52. Training loss: 3.343818043910417. Learning Rate: 4.0845612720612724e-05\n",
      "Batch 7,500 of 9,631 Elased 0:53:14. Training loss: 3.3429820931275684. Learning Rate: 4.0828268953268954e-05\n",
      "Batch 7,550 of 9,631 Elased 0:53:35. Training loss: 3.342535017357757. Learning Rate: 4.081092518592518e-05\n",
      "Batch 7,600 of 9,631 Elased 0:53:57. Training loss: 3.3432148100984724. Learning Rate: 4.0793581418581426e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7,650 of 9,631 Elased 0:54:18. Training loss: 3.343040932658451. Learning Rate: 4.0776237651237655e-05\n",
      "Batch 7,700 of 9,631 Elased 0:54:39. Training loss: 3.342141339515711. Learning Rate: 4.0758893883893884e-05\n",
      "Batch 7,750 of 9,631 Elased 0:55:00. Training loss: 3.340629198828051. Learning Rate: 4.074155011655012e-05\n",
      "Batch 7,800 of 9,631 Elased 0:55:21. Training loss: 3.3412609821252333. Learning Rate: 4.072420634920635e-05\n",
      "Batch 7,850 of 9,631 Elased 0:55:43. Training loss: 3.341520572938737. Learning Rate: 4.070686258186258e-05\n",
      "Batch 7,900 of 9,631 Elased 0:56:04. Training loss: 3.340293490977227. Learning Rate: 4.0689518814518815e-05\n",
      "Batch 7,950 of 9,631 Elased 0:56:25. Training loss: 3.3417323954600207. Learning Rate: 4.067217504717505e-05\n",
      "Batch 8,000 of 9,631 Elased 0:56:47. Training loss: 3.3415316667854786. Learning Rate: 4.065483127983128e-05\n",
      "Batch 8,050 of 9,631 Elased 0:57:08. Training loss: 3.342619332556399. Learning Rate: 4.0637487512487516e-05\n",
      "Batch 8,100 of 9,631 Elased 0:57:29. Training loss: 3.3429585040645833. Learning Rate: 4.0620143745143746e-05\n",
      "Batch 8,150 of 9,631 Elased 0:57:50. Training loss: 3.34361068994721. Learning Rate: 4.060279997779998e-05\n",
      "Batch 8,200 of 9,631 Elased 0:58:12. Training loss: 3.3433956343080937. Learning Rate: 4.058545621045621e-05\n",
      "Batch 8,250 of 9,631 Elased 0:58:33. Training loss: 3.34396058883089. Learning Rate: 4.056811244311245e-05\n",
      "Batch 8,300 of 9,631 Elased 0:58:54. Training loss: 3.344045690140092. Learning Rate: 4.055076867576868e-05\n",
      "Batch 8,350 of 9,631 Elased 0:59:15. Training loss: 3.343826334690619. Learning Rate: 4.053342490842491e-05\n",
      "Batch 8,400 of 9,631 Elased 0:59:36. Training loss: 3.3429057083527245. Learning Rate: 4.051608114108114e-05\n",
      "Batch 8,450 of 9,631 Elased 0:59:58. Training loss: 3.3422090920894103. Learning Rate: 4.049873737373738e-05\n",
      "Batch 8,500 of 9,631 Elased 1:00:19. Training loss: 3.3419383115768433. Learning Rate: 4.048139360639361e-05\n",
      "Batch 8,550 of 9,631 Elased 1:00:40. Training loss: 3.3416105547285917. Learning Rate: 4.0464049839049836e-05\n",
      "Batch 8,600 of 9,631 Elased 1:01:02. Training loss: 3.342385099682697. Learning Rate: 4.044670607170607e-05\n",
      "Batch 8,650 of 9,631 Elased 1:01:23. Training loss: 3.3412785609195685. Learning Rate: 4.042936230436231e-05\n",
      "Batch 8,700 of 9,631 Elased 1:01:44. Training loss: 3.3411136270665573. Learning Rate: 4.041201853701854e-05\n",
      "Batch 8,750 of 9,631 Elased 1:02:05. Training loss: 3.3411353811808997. Learning Rate: 4.0394674769674774e-05\n",
      "Batch 8,800 of 9,631 Elased 1:02:26. Training loss: 3.3409420429847456. Learning Rate: 4.0377331002331e-05\n",
      "Batch 8,850 of 9,631 Elased 1:02:48. Training loss: 3.3406639583770836. Learning Rate: 4.035998723498723e-05\n",
      "Batch 8,900 of 9,631 Elased 1:03:09. Training loss: 3.3407291795162672. Learning Rate: 4.034264346764347e-05\n",
      "Batch 8,950 of 9,631 Elased 1:03:30. Training loss: 3.340639019012451. Learning Rate: 4.0325299700299704e-05\n",
      "Batch 9,000 of 9,631 Elased 1:03:51. Training loss: 3.3396941079298657. Learning Rate: 4.030795593295594e-05\n",
      "Batch 9,050 of 9,631 Elased 1:04:13. Training loss: 3.3406396894560335. Learning Rate: 4.029061216561217e-05\n",
      "Batch 9,100 of 9,631 Elased 1:04:34. Training loss: 3.340448676125034. Learning Rate: 4.02732683982684e-05\n",
      "Batch 9,150 of 9,631 Elased 1:04:55. Training loss: 3.3406576583424554. Learning Rate: 4.0255924630924635e-05\n",
      "Batch 9,200 of 9,631 Elased 1:05:16. Training loss: 3.3411937835942145. Learning Rate: 4.0238580863580864e-05\n",
      "Batch 9,250 of 9,631 Elased 1:05:38. Training loss: 3.3409212638236383. Learning Rate: 4.022123709623709e-05\n",
      "Batch 9,300 of 9,631 Elased 1:05:59. Training loss: 3.3415386572191794. Learning Rate: 4.0203893328893336e-05\n",
      "Batch 9,350 of 9,631 Elased 1:06:21. Training loss: 3.341566990658561. Learning Rate: 4.0186549561549565e-05\n",
      "Batch 9,400 of 9,631 Elased 1:06:42. Training loss: 3.3417748499931172. Learning Rate: 4.0169205794205795e-05\n",
      "Batch 9,450 of 9,631 Elased 1:07:03. Training loss: 3.34160762605213. Learning Rate: 4.015186202686203e-05\n",
      "Batch 9,500 of 9,631 Elased 1:07:25. Training loss: 3.34119818740142. Learning Rate: 4.013451825951826e-05\n",
      "Batch 9,550 of 9,631 Elased 1:07:46. Training loss: 3.3405128896673313. Learning Rate: 4.011717449217449e-05\n",
      "Batch 9,600 of 9,631 Elased 1:08:06. Training loss: 3.3409586872905495. Learning Rate: 4.0099830724830725e-05\n",
      "\n",
      "\n",
      "  Average training loss: 3.34\n",
      "  Training epcoh took: 1:08:20\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0480331108ab43658a08506ff97f71b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=67.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Validation Loss: 3.47\n",
      "  Validation took: 0:00:42\n",
      "\n",
      "======== Epoch 19 / 30 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab695de601942e5b2cb89d7df4f4ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch    50 of 9,631 Elased 0:00:21. Training loss: 3.427773108482361. Learning Rate: 4.007173382173382e-05\n",
      "Batch   100 of 9,631 Elased 0:00:42. Training loss: 3.363998603820801. Learning Rate: 4.005439005439006e-05\n",
      "Batch   150 of 9,631 Elased 0:01:04. Training loss: 3.371913301150004. Learning Rate: 4.003704628704629e-05\n",
      "Batch   200 of 9,631 Elased 0:01:25. Training loss: 3.394621058702469. Learning Rate: 4.0019702519702524e-05\n",
      "Batch   250 of 9,631 Elased 0:01:46. Training loss: 3.3455696783065796. Learning Rate: 4.0002358752358754e-05\n",
      "Batch   300 of 9,631 Elased 0:02:07. Training loss: 3.330793567498525. Learning Rate: 3.998501498501498e-05\n",
      "Batch   350 of 9,631 Elased 0:02:29. Training loss: 3.3233462640217373. Learning Rate: 3.996767121767122e-05\n",
      "Batch   400 of 9,631 Elased 0:02:50. Training loss: 3.334564073085785. Learning Rate: 3.9950327450327455e-05\n",
      "Batch   450 of 9,631 Elased 0:03:11. Training loss: 3.3389199606577553. Learning Rate: 3.9932983682983684e-05\n",
      "Batch   500 of 9,631 Elased 0:03:32. Training loss: 3.346970111846924. Learning Rate: 3.991563991563992e-05\n",
      "Batch   550 of 9,631 Elased 0:03:53. Training loss: 3.332680223638361. Learning Rate: 3.989829614829615e-05\n",
      "Batch   600 of 9,631 Elased 0:04:14. Training loss: 3.3471984736124676. Learning Rate: 3.9880952380952386e-05\n",
      "Batch   650 of 9,631 Elased 0:04:36. Training loss: 3.3486361819047192. Learning Rate: 3.9863608613608615e-05\n",
      "Batch   700 of 9,631 Elased 0:04:57. Training loss: 3.3441540922437394. Learning Rate: 3.9846264846264844e-05\n",
      "Batch   750 of 9,631 Elased 0:05:19. Training loss: 3.3325554828643797. Learning Rate: 3.982892107892108e-05\n",
      "Batch   800 of 9,631 Elased 0:05:40. Training loss: 3.3291314285993576. Learning Rate: 3.9811577311577316e-05\n",
      "Batch   850 of 9,631 Elased 0:06:01. Training loss: 3.3228318573446836. Learning Rate: 3.9794233544233545e-05\n",
      "Batch   900 of 9,631 Elased 0:06:22. Training loss: 3.320887010627323. Learning Rate: 3.977688977688978e-05\n",
      "Batch   950 of 9,631 Elased 0:06:43. Training loss: 3.3153497143795616. Learning Rate: 3.975954600954601e-05\n",
      "Batch 1,000 of 9,631 Elased 0:07:04. Training loss: 3.316990938901901. Learning Rate: 3.974220224220224e-05\n",
      "Batch 1,050 of 9,631 Elased 0:07:25. Training loss: 3.3200547132037936. Learning Rate: 3.9724858474858476e-05\n",
      "Batch 1,100 of 9,631 Elased 0:07:46. Training loss: 3.3225859878279946. Learning Rate: 3.970751470751471e-05\n",
      "Batch 1,150 of 9,631 Elased 0:08:08. Training loss: 3.321239665280218. Learning Rate: 3.969017094017094e-05\n",
      "Batch 1,200 of 9,631 Elased 0:08:29. Training loss: 3.3228598568836847. Learning Rate: 3.967282717282718e-05\n",
      "Batch 1,250 of 9,631 Elased 0:08:50. Training loss: 3.3232783248901367. Learning Rate: 3.965548340548341e-05\n",
      "Batch 1,300 of 9,631 Elased 0:09:11. Training loss: 3.3204805442003105. Learning Rate: 3.9638139638139636e-05\n",
      "Batch 1,350 of 9,631 Elased 0:09:32. Training loss: 3.3198353253470527. Learning Rate: 3.962079587079587e-05\n",
      "Batch 1,400 of 9,631 Elased 0:09:53. Training loss: 3.3174070615427835. Learning Rate: 3.96034521034521e-05\n",
      "Batch 1,450 of 9,631 Elased 0:10:14. Training loss: 3.3151970721935404. Learning Rate: 3.9586108336108344e-05\n",
      "Batch 1,500 of 9,631 Elased 0:10:36. Training loss: 3.313676901022593. Learning Rate: 3.956876456876457e-05\n",
      "Batch 1,550 of 9,631 Elased 0:10:57. Training loss: 3.313547957943332. Learning Rate: 3.95514208014208e-05\n",
      "Batch 1,600 of 9,631 Elased 0:11:18. Training loss: 3.31976341933012. Learning Rate: 3.953407703407704e-05\n",
      "Batch 1,650 of 9,631 Elased 0:11:40. Training loss: 3.321775352593624. Learning Rate: 3.951673326673327e-05\n",
      "Batch 1,700 of 9,631 Elased 0:12:01. Training loss: 3.3188054011849797. Learning Rate: 3.94993894993895e-05\n",
      "Batch 1,750 of 9,631 Elased 0:12:23. Training loss: 3.3185011650494167. Learning Rate: 3.948204573204573e-05\n",
      "Batch 1,800 of 9,631 Elased 0:12:43. Training loss: 3.317217934793896. Learning Rate: 3.946470196470197e-05\n",
      "Batch 1,850 of 9,631 Elased 0:13:05. Training loss: 3.318968427632306. Learning Rate: 3.94473581973582e-05\n",
      "Batch 1,900 of 9,631 Elased 0:13:26. Training loss: 3.3180384190459002. Learning Rate: 3.9430014430014435e-05\n",
      "Batch 1,950 of 9,631 Elased 0:13:48. Training loss: 3.315550775161156. Learning Rate: 3.9412670662670664e-05\n",
      "Batch 2,000 of 9,631 Elased 0:14:09. Training loss: 3.3112930178642275. Learning Rate: 3.939532689532689e-05\n",
      "Batch 2,050 of 9,631 Elased 0:14:31. Training loss: 3.3113295560929834. Learning Rate: 3.937798312798313e-05\n",
      "Batch 2,100 of 9,631 Elased 0:14:51. Training loss: 3.3155377365293957. Learning Rate: 3.9360639360639365e-05\n",
      "Batch 2,150 of 9,631 Elased 0:15:13. Training loss: 3.3162026638208433. Learning Rate: 3.9343295593295594e-05\n",
      "Batch 2,200 of 9,631 Elased 0:15:34. Training loss: 3.3164112440022557. Learning Rate: 3.932595182595183e-05\n",
      "Batch 2,250 of 9,631 Elased 0:15:55. Training loss: 3.314847600724962. Learning Rate: 3.930860805860806e-05\n",
      "Batch 2,300 of 9,631 Elased 0:16:16. Training loss: 3.31661527633667. Learning Rate: 3.9291264291264296e-05\n",
      "Batch 2,350 of 9,631 Elased 0:16:38. Training loss: 3.3144121508395417. Learning Rate: 3.9273920523920525e-05\n",
      "Batch 2,400 of 9,631 Elased 0:16:59. Training loss: 3.3143341564635436. Learning Rate: 3.9256576756576754e-05\n",
      "Batch 2,450 of 9,631 Elased 0:17:20. Training loss: 3.310434484676439. Learning Rate: 3.923923298923299e-05\n",
      "Batch 2,500 of 9,631 Elased 0:17:41. Training loss: 3.308873544216156. Learning Rate: 3.9221889221889226e-05\n",
      "Batch 2,550 of 9,631 Elased 0:18:03. Training loss: 3.3134396381004185. Learning Rate: 3.9204545454545456e-05\n",
      "Batch 2,600 of 9,631 Elased 0:18:24. Training loss: 3.3172186465446765. Learning Rate: 3.918720168720169e-05\n",
      "Batch 2,650 of 9,631 Elased 0:18:45. Training loss: 3.319944467094709. Learning Rate: 3.916985791985792e-05\n",
      "Batch 2,700 of 9,631 Elased 0:19:06. Training loss: 3.3225762166800323. Learning Rate: 3.915251415251415e-05\n",
      "Batch 2,750 of 9,631 Elased 0:19:28. Training loss: 3.3200266772183507. Learning Rate: 3.9135170385170386e-05\n",
      "Batch 2,800 of 9,631 Elased 0:19:49. Training loss: 3.3210394088711057. Learning Rate: 3.911782661782662e-05\n",
      "Batch 2,850 of 9,631 Elased 0:20:10. Training loss: 3.323935984477662. Learning Rate: 3.910048285048285e-05\n",
      "Batch 2,900 of 9,631 Elased 0:20:32. Training loss: 3.324109736639878. Learning Rate: 3.908313908313909e-05\n",
      "Batch 2,950 of 9,631 Elased 0:20:53. Training loss: 3.3247166555210694. Learning Rate: 3.906579531579532e-05\n",
      "Batch 3,000 of 9,631 Elased 0:21:14. Training loss: 3.327520857175191. Learning Rate: 3.9048451548451546e-05\n",
      "Batch 3,050 of 9,631 Elased 0:21:35. Training loss: 3.326834756194568. Learning Rate: 3.903110778110778e-05\n",
      "Batch 3,100 of 9,631 Elased 0:21:56. Training loss: 3.3280586607225477. Learning Rate: 3.901376401376401e-05\n",
      "Batch 3,150 of 9,631 Elased 0:22:18. Training loss: 3.323984549386161. Learning Rate: 3.8996420246420254e-05\n",
      "Batch 3,200 of 9,631 Elased 0:22:39. Training loss: 3.325954101458192. Learning Rate: 3.8979076479076484e-05\n",
      "Batch 3,250 of 9,631 Elased 0:23:00. Training loss: 3.325636006648724. Learning Rate: 3.896173271173271e-05\n",
      "Batch 3,300 of 9,631 Elased 0:23:22. Training loss: 3.327537911227255. Learning Rate: 3.894438894438895e-05\n",
      "Batch 3,350 of 9,631 Elased 0:23:43. Training loss: 3.3286192575141564. Learning Rate: 3.892704517704518e-05\n",
      "Batch 3,400 of 9,631 Elased 0:24:05. Training loss: 3.3288190890059752. Learning Rate: 3.890970140970141e-05\n",
      "Batch 3,450 of 9,631 Elased 0:24:27. Training loss: 3.32917685902637. Learning Rate: 3.8892357642357644e-05\n",
      "Batch 3,500 of 9,631 Elased 0:24:49. Training loss: 3.328261543580464. Learning Rate: 3.887501387501388e-05\n",
      "Batch 3,550 of 9,631 Elased 0:25:11. Training loss: 3.329597522876632. Learning Rate: 3.885767010767011e-05\n",
      "Batch 3,600 of 9,631 Elased 0:25:33. Training loss: 3.331422534088294. Learning Rate: 3.8840326340326345e-05\n",
      "Batch 3,650 of 9,631 Elased 0:25:55. Training loss: 3.3349683891257196. Learning Rate: 3.8822982572982574e-05\n",
      "Batch 3,700 of 9,631 Elased 0:26:17. Training loss: 3.334385002529299. Learning Rate: 3.8805638805638803e-05\n",
      "Batch 3,750 of 9,631 Elased 0:26:39. Training loss: 3.3326624917030334. Learning Rate: 3.878829503829504e-05\n",
      "Batch 3,800 of 9,631 Elased 0:27:00. Training loss: 3.3316813922869533. Learning Rate: 3.8770951270951276e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3,850 of 9,631 Elased 0:27:22. Training loss: 3.329107890098126. Learning Rate: 3.8753607503607505e-05\n",
      "Batch 3,900 of 9,631 Elased 0:27:43. Training loss: 3.3276516626737056. Learning Rate: 3.873626373626374e-05\n",
      "Batch 3,950 of 9,631 Elased 0:28:04. Training loss: 3.3293501455874384. Learning Rate: 3.871891996891997e-05\n",
      "Batch 4,000 of 9,631 Elased 0:28:26. Training loss: 3.329794092267752. Learning Rate: 3.87015762015762e-05\n",
      "Batch 4,050 of 9,631 Elased 0:28:47. Training loss: 3.3294120900719255. Learning Rate: 3.8684232434232435e-05\n",
      "Batch 4,100 of 9,631 Elased 0:29:08. Training loss: 3.328699522977922. Learning Rate: 3.8666888666888665e-05\n",
      "Batch 4,150 of 9,631 Elased 0:29:29. Training loss: 3.3280871753520276. Learning Rate: 3.86495448995449e-05\n",
      "Batch 4,200 of 9,631 Elased 0:29:51. Training loss: 3.328131382777577. Learning Rate: 3.863220113220114e-05\n",
      "Batch 4,250 of 9,631 Elased 0:30:12. Training loss: 3.328456604536842. Learning Rate: 3.8614857364857366e-05\n",
      "Batch 4,300 of 9,631 Elased 0:30:33. Training loss: 3.327952660322189. Learning Rate: 3.85975135975136e-05\n",
      "Batch 4,350 of 9,631 Elased 0:30:55. Training loss: 3.327789839848705. Learning Rate: 3.858016983016983e-05\n",
      "Batch 4,400 of 9,631 Elased 0:31:16. Training loss: 3.328823497539217. Learning Rate: 3.856282606282606e-05\n",
      "Batch 4,450 of 9,631 Elased 0:31:37. Training loss: 3.3290991708937656. Learning Rate: 3.85454822954823e-05\n",
      "Batch 4,500 of 9,631 Elased 0:31:59. Training loss: 3.3305736266507044. Learning Rate: 3.852813852813853e-05\n",
      "Batch 4,550 of 9,631 Elased 0:32:20. Training loss: 3.328751121536716. Learning Rate: 3.851079476079476e-05\n",
      "Batch 4,600 of 9,631 Elased 0:32:41. Training loss: 3.329752790953802. Learning Rate: 3.8493450993451e-05\n",
      "Batch 4,650 of 9,631 Elased 0:33:03. Training loss: 3.328694111377962. Learning Rate: 3.847610722610723e-05\n",
      "Batch 4,700 of 9,631 Elased 0:33:24. Training loss: 3.329263778316214. Learning Rate: 3.8458763458763457e-05\n",
      "Batch 4,750 of 9,631 Elased 0:33:45. Training loss: 3.3291847943506743. Learning Rate: 3.844141969141969e-05\n",
      "Batch 4,800 of 9,631 Elased 0:34:06. Training loss: 3.3271639434744915. Learning Rate: 3.842407592407592e-05\n",
      "Batch 4,850 of 9,631 Elased 0:34:26. Training loss: 3.326495704478824. Learning Rate: 3.840673215673216e-05\n",
      "Batch 4,900 of 9,631 Elased 0:34:48. Training loss: 3.32736014178821. Learning Rate: 3.8389388389388394e-05\n",
      "Batch 4,950 of 9,631 Elased 0:35:09. Training loss: 3.3279710915353564. Learning Rate: 3.837204462204462e-05\n",
      "Batch 5,000 of 9,631 Elased 0:35:30. Training loss: 3.329157227110863. Learning Rate: 3.835470085470086e-05\n",
      "Batch 5,050 of 9,631 Elased 0:35:51. Training loss: 3.329525710403329. Learning Rate: 3.833735708735709e-05\n",
      "Batch 5,100 of 9,631 Elased 0:36:13. Training loss: 3.327860849721759. Learning Rate: 3.832001332001332e-05\n",
      "Batch 5,150 of 9,631 Elased 0:36:34. Training loss: 3.32822261456147. Learning Rate: 3.8302669552669554e-05\n",
      "Batch 5,200 of 9,631 Elased 0:36:55. Training loss: 3.32799199037827. Learning Rate: 3.828532578532579e-05\n",
      "Batch 5,250 of 9,631 Elased 0:37:17. Training loss: 3.3279158528418766. Learning Rate: 3.826798201798202e-05\n",
      "Batch 5,300 of 9,631 Elased 0:37:38. Training loss: 3.3275282232041627. Learning Rate: 3.8250638250638255e-05\n",
      "Batch 5,350 of 9,631 Elased 0:37:59. Training loss: 3.3294096946270666. Learning Rate: 3.8233294483294484e-05\n",
      "Batch 5,400 of 9,631 Elased 0:38:20. Training loss: 3.329321443637212. Learning Rate: 3.8215950715950714e-05\n",
      "Batch 5,450 of 9,631 Elased 0:38:42. Training loss: 3.329498352479497. Learning Rate: 3.819860694860695e-05\n",
      "Batch 5,500 of 9,631 Elased 0:39:03. Training loss: 3.329685642069036. Learning Rate: 3.8181263181263186e-05\n",
      "Batch 5,550 of 9,631 Elased 0:39:24. Training loss: 3.330105810380197. Learning Rate: 3.8163919413919415e-05\n",
      "Batch 5,600 of 9,631 Elased 0:39:46. Training loss: 3.329619550172772. Learning Rate: 3.814657564657565e-05\n",
      "Batch 5,650 of 9,631 Elased 0:40:07. Training loss: 3.3313013874956994. Learning Rate: 3.812923187923188e-05\n",
      "Batch 5,700 of 9,631 Elased 0:40:28. Training loss: 3.3310636853544335. Learning Rate: 3.811188811188811e-05\n",
      "Batch 5,750 of 9,631 Elased 0:40:49. Training loss: 3.33061172085223. Learning Rate: 3.8094544344544346e-05\n",
      "Batch 5,800 of 9,631 Elased 0:41:11. Training loss: 3.330639936122401. Learning Rate: 3.8077200577200575e-05\n",
      "Batch 5,850 of 9,631 Elased 0:41:32. Training loss: 3.330888006238856. Learning Rate: 3.805985680985681e-05\n",
      "Batch 5,900 of 9,631 Elased 0:41:53. Training loss: 3.3307313109858563. Learning Rate: 3.804251304251305e-05\n",
      "Batch 5,950 of 9,631 Elased 0:42:15. Training loss: 3.331102613020344. Learning Rate: 3.8025169275169276e-05\n",
      "Batch 6,000 of 9,631 Elased 0:42:36. Training loss: 3.3305780746738116. Learning Rate: 3.800782550782551e-05\n",
      "Batch 6,050 of 9,631 Elased 0:42:58. Training loss: 3.3306602158231184. Learning Rate: 3.799048174048174e-05\n",
      "Batch 6,100 of 9,631 Elased 0:43:19. Training loss: 3.331344434687349. Learning Rate: 3.797313797313797e-05\n",
      "Batch 6,150 of 9,631 Elased 0:43:40. Training loss: 3.3311380542003044. Learning Rate: 3.795579420579421e-05\n",
      "Batch 6,200 of 9,631 Elased 0:44:01. Training loss: 3.3292435467820014. Learning Rate: 3.793845043845044e-05\n",
      "Batch 6,250 of 9,631 Elased 0:44:22. Training loss: 3.3289524145317078. Learning Rate: 3.792110667110667e-05\n",
      "Batch 6,300 of 9,631 Elased 0:44:43. Training loss: 3.328864220059107. Learning Rate: 3.790376290376291e-05\n",
      "Batch 6,350 of 9,631 Elased 0:45:05. Training loss: 3.3284250657952676. Learning Rate: 3.788641913641914e-05\n",
      "Batch 6,400 of 9,631 Elased 0:45:26. Training loss: 3.3280222622863946. Learning Rate: 3.786907536907537e-05\n",
      "Batch 6,450 of 9,631 Elased 0:45:48. Training loss: 3.3293803322222804. Learning Rate: 3.78517316017316e-05\n",
      "Batch 6,500 of 9,631 Elased 0:46:09. Training loss: 3.327661726273023. Learning Rate: 3.783438783438783e-05\n",
      "Batch 6,550 of 9,631 Elased 0:46:30. Training loss: 3.3279804971745905. Learning Rate: 3.781704406704407e-05\n",
      "Batch 6,600 of 9,631 Elased 0:46:52. Training loss: 3.3287831310973024. Learning Rate: 3.7799700299700304e-05\n",
      "Batch 6,650 of 9,631 Elased 0:47:13. Training loss: 3.329112042072124. Learning Rate: 3.7782356532356534e-05\n",
      "Batch 6,700 of 9,631 Elased 0:47:35. Training loss: 3.3303455312394386. Learning Rate: 3.776501276501277e-05\n",
      "Batch 6,750 of 9,631 Elased 0:47:56. Training loss: 3.330352209603345. Learning Rate: 3.7747668997669e-05\n",
      "Batch 6,800 of 9,631 Elased 0:48:17. Training loss: 3.3299968683894945. Learning Rate: 3.773032523032523e-05\n",
      "Batch 6,850 of 9,631 Elased 0:48:38. Training loss: 3.3295540700515693. Learning Rate: 3.7712981462981464e-05\n",
      "Batch 6,900 of 9,631 Elased 0:48:59. Training loss: 3.329109596048576. Learning Rate: 3.76956376956377e-05\n",
      "Batch 6,950 of 9,631 Elased 0:49:21. Training loss: 3.3289026406343036. Learning Rate: 3.767829392829393e-05\n",
      "Batch 7,000 of 9,631 Elased 0:49:42. Training loss: 3.3281658647911887. Learning Rate: 3.7660950160950166e-05\n",
      "Batch 7,050 of 9,631 Elased 0:50:03. Training loss: 3.3274605247991307. Learning Rate: 3.7643606393606395e-05\n",
      "Batch 7,100 of 9,631 Elased 0:50:24. Training loss: 3.327633566403053. Learning Rate: 3.7626262626262624e-05\n",
      "Batch 7,150 of 9,631 Elased 0:50:46. Training loss: 3.327562079679716. Learning Rate: 3.760891885891886e-05\n",
      "Batch 7,200 of 9,631 Elased 0:51:07. Training loss: 3.327666920820872. Learning Rate: 3.7591575091575096e-05\n",
      "Batch 7,250 of 9,631 Elased 0:51:29. Training loss: 3.326613701590176. Learning Rate: 3.7574231324231325e-05\n",
      "Batch 7,300 of 9,631 Elased 0:51:50. Training loss: 3.327055172920227. Learning Rate: 3.755688755688756e-05\n",
      "Batch 7,350 of 9,631 Elased 0:52:11. Training loss: 3.326460683913458. Learning Rate: 3.753954378954379e-05\n",
      "Batch 7,400 of 9,631 Elased 0:52:32. Training loss: 3.3251234266242466. Learning Rate: 3.752220002220002e-05\n",
      "Batch 7,450 of 9,631 Elased 0:52:53. Training loss: 3.3248030908955823. Learning Rate: 3.7504856254856256e-05\n",
      "Batch 7,500 of 9,631 Elased 0:53:15. Training loss: 3.3237250442822774. Learning Rate: 3.7487512487512485e-05\n",
      "Batch 7,550 of 9,631 Elased 0:53:36. Training loss: 3.3238382673579334. Learning Rate: 3.747016872016872e-05\n",
      "Batch 7,600 of 9,631 Elased 0:53:57. Training loss: 3.3243541382488453. Learning Rate: 3.745282495282496e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7,650 of 9,631 Elased 0:54:18. Training loss: 3.3244174419976527. Learning Rate: 3.743548118548119e-05\n",
      "Batch 7,700 of 9,631 Elased 0:54:39. Training loss: 3.3234469942922718. Learning Rate: 3.741813741813742e-05\n",
      "Batch 7,750 of 9,631 Elased 0:55:00. Training loss: 3.3219058051263133. Learning Rate: 3.740079365079365e-05\n",
      "Batch 7,800 of 9,631 Elased 0:55:22. Training loss: 3.3224326278613163. Learning Rate: 3.738344988344988e-05\n",
      "Batch 7,850 of 9,631 Elased 0:55:43. Training loss: 3.3228807636734787. Learning Rate: 3.736610611610612e-05\n",
      "Batch 7,900 of 9,631 Elased 0:56:04. Training loss: 3.3216238977335677. Learning Rate: 3.734876234876235e-05\n",
      "Batch 7,950 of 9,631 Elased 0:56:25. Training loss: 3.323619092695368. Learning Rate: 3.733141858141858e-05\n",
      "Batch 8,000 of 9,631 Elased 0:56:47. Training loss: 3.323192697405815. Learning Rate: 3.731407481407482e-05\n",
      "Batch 8,050 of 9,631 Elased 0:57:08. Training loss: 3.3244035425541565. Learning Rate: 3.729673104673105e-05\n",
      "Batch 8,100 of 9,631 Elased 0:57:29. Training loss: 3.324483104694037. Learning Rate: 3.727938727938728e-05\n",
      "Batch 8,150 of 9,631 Elased 0:57:50. Training loss: 3.325062478510149. Learning Rate: 3.726204351204351e-05\n",
      "Batch 8,200 of 9,631 Elased 0:58:12. Training loss: 3.3248009708451063. Learning Rate: 3.724469974469974e-05\n",
      "Batch 8,250 of 9,631 Elased 0:58:33. Training loss: 3.325240917928291. Learning Rate: 3.722735597735598e-05\n",
      "Batch 8,300 of 9,631 Elased 0:58:54. Training loss: 3.3255077090033565. Learning Rate: 3.7210012210012215e-05\n",
      "Batch 8,350 of 9,631 Elased 0:59:15. Training loss: 3.3252477922553787. Learning Rate: 3.7192668442668444e-05\n",
      "Batch 8,400 of 9,631 Elased 0:59:36. Training loss: 3.324391831046059. Learning Rate: 3.717532467532468e-05\n",
      "Batch 8,450 of 9,631 Elased 0:59:58. Training loss: 3.3239053975054498. Learning Rate: 3.715798090798091e-05\n",
      "Batch 8,500 of 9,631 Elased 1:00:19. Training loss: 3.3236226967362796. Learning Rate: 3.714063714063714e-05\n",
      "Batch 8,550 of 9,631 Elased 1:00:40. Training loss: 3.3232287380709287. Learning Rate: 3.7123293373293374e-05\n",
      "Batch 8,600 of 9,631 Elased 1:01:01. Training loss: 3.3238282754532125. Learning Rate: 3.710594960594961e-05\n",
      "Batch 8,650 of 9,631 Elased 1:01:23. Training loss: 3.3228553466576374. Learning Rate: 3.708860583860584e-05\n",
      "Batch 8,700 of 9,631 Elased 1:01:44. Training loss: 3.3225290927667728. Learning Rate: 3.7071262071262076e-05\n",
      "Batch 8,750 of 9,631 Elased 1:02:05. Training loss: 3.322393990816389. Learning Rate: 3.7053918303918305e-05\n",
      "Batch 8,800 of 9,631 Elased 1:02:26. Training loss: 3.3225440172309226. Learning Rate: 3.7036574536574534e-05\n",
      "Batch 8,850 of 9,631 Elased 1:02:48. Training loss: 3.322255396506207. Learning Rate: 3.701923076923077e-05\n",
      "Batch 8,900 of 9,631 Elased 1:03:09. Training loss: 3.322422136014767. Learning Rate: 3.7001887001887006e-05\n",
      "Batch 8,950 of 9,631 Elased 1:03:31. Training loss: 3.321707827219084. Learning Rate: 3.6984543234543236e-05\n",
      "Batch 9,000 of 9,631 Elased 1:03:51. Training loss: 3.3207916672097313. Learning Rate: 3.696719946719947e-05\n",
      "Batch 9,050 of 9,631 Elased 1:04:13. Training loss: 3.3217580989842914. Learning Rate: 3.69498556998557e-05\n",
      "Batch 9,100 of 9,631 Elased 1:04:34. Training loss: 3.321662054782385. Learning Rate: 3.693251193251193e-05\n",
      "Batch 9,150 of 9,631 Elased 1:04:55. Training loss: 3.321888848067633. Learning Rate: 3.6915168165168166e-05\n",
      "Batch 9,200 of 9,631 Elased 1:05:16. Training loss: 3.32260322758685. Learning Rate: 3.6897824397824396e-05\n",
      "Batch 9,250 of 9,631 Elased 1:05:37. Training loss: 3.322162682159527. Learning Rate: 3.688048063048063e-05\n",
      "Batch 9,300 of 9,631 Elased 1:05:58. Training loss: 3.3227076651588563. Learning Rate: 3.686313686313687e-05\n",
      "Batch 9,350 of 9,631 Elased 1:06:19. Training loss: 3.3228952658367668. Learning Rate: 3.68457930957931e-05\n",
      "Batch 9,400 of 9,631 Elased 1:06:41. Training loss: 3.322788937611783. Learning Rate: 3.682844932844933e-05\n",
      "Batch 9,450 of 9,631 Elased 1:07:02. Training loss: 3.3224564779246295. Learning Rate: 3.681110556110556e-05\n",
      "Batch 9,500 of 9,631 Elased 1:07:23. Training loss: 3.3224379422162706. Learning Rate: 3.679376179376179e-05\n",
      "Batch 9,550 of 9,631 Elased 1:07:44. Training loss: 3.3218737920416586. Learning Rate: 3.677641802641803e-05\n",
      "Batch 9,600 of 9,631 Elased 1:08:06. Training loss: 3.3222725587214033. Learning Rate: 3.6759074259074264e-05\n",
      "\n",
      "\n",
      "  Average training loss: 3.32\n",
      "  Training epcoh took: 1:08:19\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b885c9e7089641efa57957d0212b91a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=67.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Validation Loss: 3.46\n",
      "  Validation took: 0:00:43\n",
      "\n",
      "======== Epoch 20 / 30 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87dcac91815a48fc949124b87255371f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch    50 of 9,631 Elased 0:00:21. Training loss: 3.359684557914734. Learning Rate: 3.673097735597736e-05\n",
      "Batch   100 of 9,631 Elased 0:00:43. Training loss: 3.31755033493042. Learning Rate: 3.671363358863359e-05\n",
      "Batch   150 of 9,631 Elased 0:01:04. Training loss: 3.333294529914856. Learning Rate: 3.6696289821289827e-05\n",
      "Batch   200 of 9,631 Elased 0:01:25. Training loss: 3.3624137949943544. Learning Rate: 3.6678946053946056e-05\n",
      "Batch   250 of 9,631 Elased 0:01:46. Training loss: 3.317738871574402. Learning Rate: 3.6661602286602285e-05\n",
      "Batch   300 of 9,631 Elased 0:02:08. Training loss: 3.3081505370140074. Learning Rate: 3.664425851925852e-05\n",
      "Batch   350 of 9,631 Elased 0:02:29. Training loss: 3.3047242150987897. Learning Rate: 3.662691475191475e-05\n",
      "Batch   400 of 9,631 Elased 0:02:50. Training loss: 3.3110473477840423. Learning Rate: 3.6609570984570987e-05\n",
      "Batch   450 of 9,631 Elased 0:03:11. Training loss: 3.312221809493171. Learning Rate: 3.659222721722722e-05\n",
      "Batch   500 of 9,631 Elased 0:03:32. Training loss: 3.3192149991989135. Learning Rate: 3.657488344988345e-05\n",
      "Batch   550 of 9,631 Elased 0:03:53. Training loss: 3.304750280380249. Learning Rate: 3.655753968253968e-05\n",
      "Batch   600 of 9,631 Elased 0:04:15. Training loss: 3.3212783479690553. Learning Rate: 3.654019591519592e-05\n",
      "Batch   650 of 9,631 Elased 0:04:36. Training loss: 3.32260580943181. Learning Rate: 3.6522852147852146e-05\n",
      "Batch   700 of 9,631 Elased 0:04:57. Training loss: 3.3172725200653077. Learning Rate: 3.650550838050838e-05\n",
      "Batch   750 of 9,631 Elased 0:05:18. Training loss: 3.3044415076573688. Learning Rate: 3.648816461316462e-05\n",
      "Batch   800 of 9,631 Elased 0:05:40. Training loss: 3.3032101741433144. Learning Rate: 3.647082084582085e-05\n",
      "Batch   850 of 9,631 Elased 0:06:01. Training loss: 3.2957346122405107. Learning Rate: 3.6453477078477084e-05\n",
      "Batch   900 of 9,631 Elased 0:06:22. Training loss: 3.2924612424108717. Learning Rate: 3.643613331113331e-05\n",
      "Batch   950 of 9,631 Elased 0:06:43. Training loss: 3.289625264468946. Learning Rate: 3.641878954378954e-05\n",
      "Batch 1,000 of 9,631 Elased 0:07:04. Training loss: 3.291174138069153. Learning Rate: 3.640144577644578e-05\n",
      "Batch 1,050 of 9,631 Elased 0:07:25. Training loss: 3.2937596936452955. Learning Rate: 3.6384102009102014e-05\n",
      "Batch 1,100 of 9,631 Elased 0:07:46. Training loss: 3.296729909289967. Learning Rate: 3.6366758241758244e-05\n",
      "Batch 1,150 of 9,631 Elased 0:08:07. Training loss: 3.2976070783449254. Learning Rate: 3.634941447441448e-05\n",
      "Batch 1,200 of 9,631 Elased 0:08:28. Training loss: 3.3001672182480495. Learning Rate: 3.633207070707071e-05\n",
      "Batch 1,250 of 9,631 Elased 0:08:49. Training loss: 3.302180923461914. Learning Rate: 3.631472693972694e-05\n",
      "Batch 1,300 of 9,631 Elased 0:09:10. Training loss: 3.2976591818149275. Learning Rate: 3.6297383172383174e-05\n",
      "Batch 1,350 of 9,631 Elased 0:09:32. Training loss: 3.2968058451899775. Learning Rate: 3.6280039405039404e-05\n",
      "Batch 1,400 of 9,631 Elased 0:09:52. Training loss: 3.295974382332393. Learning Rate: 3.626269563769564e-05\n",
      "Batch 1,450 of 9,631 Elased 0:10:13. Training loss: 3.2950516708143827. Learning Rate: 3.6245351870351876e-05\n",
      "Batch 1,500 of 9,631 Elased 0:10:35. Training loss: 3.292740392843882. Learning Rate: 3.6228008103008105e-05\n",
      "Batch 1,550 of 9,631 Elased 0:10:56. Training loss: 3.2927871519519436. Learning Rate: 3.6210664335664334e-05\n",
      "Batch 1,600 of 9,631 Elased 0:11:17. Training loss: 3.297754755690694. Learning Rate: 3.619332056832057e-05\n",
      "Batch 1,650 of 9,631 Elased 0:11:38. Training loss: 3.300386166500323. Learning Rate: 3.61759768009768e-05\n",
      "Batch 1,700 of 9,631 Elased 0:11:59. Training loss: 3.298274101299398. Learning Rate: 3.6158633033633036e-05\n",
      "Batch 1,750 of 9,631 Elased 0:12:21. Training loss: 3.2976100190707616. Learning Rate: 3.614128926628927e-05\n",
      "Batch 1,800 of 9,631 Elased 0:12:42. Training loss: 3.296204235818651. Learning Rate: 3.61239454989455e-05\n",
      "Batch 1,850 of 9,631 Elased 0:13:03. Training loss: 3.29853828829688. Learning Rate: 3.610660173160174e-05\n",
      "Batch 1,900 of 9,631 Elased 0:13:24. Training loss: 3.298083471373508. Learning Rate: 3.6089257964257966e-05\n",
      "Batch 1,950 of 9,631 Elased 0:13:46. Training loss: 3.294693111884288. Learning Rate: 3.6071914196914195e-05\n",
      "Batch 2,000 of 9,631 Elased 0:14:06. Training loss: 3.2907404928207398. Learning Rate: 3.605457042957043e-05\n",
      "Batch 2,050 of 9,631 Elased 0:14:28. Training loss: 3.292000604257351. Learning Rate: 3.603722666222666e-05\n",
      "Batch 2,100 of 9,631 Elased 0:14:49. Training loss: 3.295530783562433. Learning Rate: 3.60198828948829e-05\n",
      "Batch 2,150 of 9,631 Elased 0:15:10. Training loss: 3.2962445988765983. Learning Rate: 3.600253912753913e-05\n",
      "Batch 2,200 of 9,631 Elased 0:15:31. Training loss: 3.2963780124620956. Learning Rate: 3.598519536019536e-05\n",
      "Batch 2,250 of 9,631 Elased 0:15:53. Training loss: 3.293626487996843. Learning Rate: 3.596785159285159e-05\n",
      "Batch 2,300 of 9,631 Elased 0:16:14. Training loss: 3.295244104084761. Learning Rate: 3.595050782550783e-05\n",
      "Batch 2,350 of 9,631 Elased 0:16:35. Training loss: 3.292842051323424. Learning Rate: 3.593316405816406e-05\n",
      "Batch 2,400 of 9,631 Elased 0:16:57. Training loss: 3.2932414879401524. Learning Rate: 3.591582029082029e-05\n",
      "Batch 2,450 of 9,631 Elased 0:17:18. Training loss: 3.290797677332041. Learning Rate: 3.589847652347653e-05\n",
      "Batch 2,500 of 9,631 Elased 0:17:40. Training loss: 3.289193574523926. Learning Rate: 3.588113275613276e-05\n",
      "Batch 2,550 of 9,631 Elased 0:18:01. Training loss: 3.2940782080444637. Learning Rate: 3.586378898878899e-05\n",
      "Batch 2,600 of 9,631 Elased 0:18:22. Training loss: 3.2971697733035454. Learning Rate: 3.584644522144522e-05\n",
      "Batch 2,650 of 9,631 Elased 0:18:44. Training loss: 3.2998988096669035. Learning Rate: 3.582910145410145e-05\n",
      "Batch 2,700 of 9,631 Elased 0:19:05. Training loss: 3.3014112716250947. Learning Rate: 3.581175768675769e-05\n",
      "Batch 2,750 of 9,631 Elased 0:19:26. Training loss: 3.299241584517739. Learning Rate: 3.5794413919413925e-05\n",
      "Batch 2,800 of 9,631 Elased 0:19:48. Training loss: 3.3000042842967168. Learning Rate: 3.5777070152070154e-05\n",
      "Batch 2,850 of 9,631 Elased 0:20:09. Training loss: 3.3030423672993976. Learning Rate: 3.575972638472639e-05\n",
      "Batch 2,900 of 9,631 Elased 0:20:30. Training loss: 3.303012679280906. Learning Rate: 3.574238261738262e-05\n",
      "Batch 2,950 of 9,631 Elased 0:20:51. Training loss: 3.302861001936056. Learning Rate: 3.572503885003885e-05\n",
      "Batch 3,000 of 9,631 Elased 0:21:12. Training loss: 3.3060736231009167. Learning Rate: 3.5707695082695085e-05\n",
      "Batch 3,050 of 9,631 Elased 0:21:33. Training loss: 3.305598279530885. Learning Rate: 3.5690351315351314e-05\n",
      "Batch 3,100 of 9,631 Elased 0:21:55. Training loss: 3.3066266805894915. Learning Rate: 3.567300754800755e-05\n",
      "Batch 3,150 of 9,631 Elased 0:22:17. Training loss: 3.302840920327202. Learning Rate: 3.5655663780663786e-05\n",
      "Batch 3,200 of 9,631 Elased 0:22:38. Training loss: 3.304882629960775. Learning Rate: 3.5638320013320015e-05\n",
      "Batch 3,250 of 9,631 Elased 0:22:59. Training loss: 3.304415410335247. Learning Rate: 3.5620976245976245e-05\n",
      "Batch 3,300 of 9,631 Elased 0:23:21. Training loss: 3.30695148785909. Learning Rate: 3.560363247863248e-05\n",
      "Batch 3,350 of 9,631 Elased 0:23:42. Training loss: 3.3072411792669723. Learning Rate: 3.558628871128871e-05\n",
      "Batch 3,400 of 9,631 Elased 0:24:03. Training loss: 3.307601797580719. Learning Rate: 3.5568944943944946e-05\n",
      "Batch 3,450 of 9,631 Elased 0:24:24. Training loss: 3.3082220049871913. Learning Rate: 3.555160117660118e-05\n",
      "Batch 3,500 of 9,631 Elased 0:24:46. Training loss: 3.307464816706521. Learning Rate: 3.553425740925741e-05\n",
      "Batch 3,550 of 9,631 Elased 0:25:07. Training loss: 3.3084877808664888. Learning Rate: 3.551691364191365e-05\n",
      "Batch 3,600 of 9,631 Elased 0:25:28. Training loss: 3.3103135157956016. Learning Rate: 3.5499569874569877e-05\n",
      "Batch 3,650 of 9,631 Elased 0:25:49. Training loss: 3.3137153637899113. Learning Rate: 3.5482226107226106e-05\n",
      "Batch 3,700 of 9,631 Elased 0:26:10. Training loss: 3.3127531822952063. Learning Rate: 3.546488233988234e-05\n",
      "Batch 3,750 of 9,631 Elased 0:26:31. Training loss: 3.311387281545003. Learning Rate: 3.544753857253857e-05\n",
      "Batch 3,800 of 9,631 Elased 0:26:52. Training loss: 3.310143626300912. Learning Rate: 3.543019480519481e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3,850 of 9,631 Elased 0:27:14. Training loss: 3.3073208179721583. Learning Rate: 3.541285103785104e-05\n",
      "Batch 3,900 of 9,631 Elased 0:27:35. Training loss: 3.30598136088787. Learning Rate: 3.539550727050727e-05\n",
      "Batch 3,950 of 9,631 Elased 0:27:56. Training loss: 3.3072113784355452. Learning Rate: 3.53781635031635e-05\n",
      "Batch 4,000 of 9,631 Elased 0:28:17. Training loss: 3.307713219881058. Learning Rate: 3.536081973581974e-05\n",
      "Batch 4,050 of 9,631 Elased 0:28:38. Training loss: 3.307524193893244. Learning Rate: 3.534347596847597e-05\n",
      "Batch 4,100 of 9,631 Elased 0:28:59. Training loss: 3.307035369873047. Learning Rate: 3.53261322011322e-05\n",
      "Batch 4,150 of 9,631 Elased 0:29:20. Training loss: 3.3066214182290685. Learning Rate: 3.530878843378844e-05\n",
      "Batch 4,200 of 9,631 Elased 0:29:42. Training loss: 3.3065591989812395. Learning Rate: 3.529144466644467e-05\n",
      "Batch 4,250 of 9,631 Elased 0:30:03. Training loss: 3.306866154726814. Learning Rate: 3.52741008991009e-05\n",
      "Batch 4,300 of 9,631 Elased 0:30:24. Training loss: 3.3061809874689856. Learning Rate: 3.5256757131757134e-05\n",
      "Batch 4,350 of 9,631 Elased 0:30:45. Training loss: 3.3055270761731026. Learning Rate: 3.523941336441336e-05\n",
      "Batch 4,400 of 9,631 Elased 0:31:06. Training loss: 3.307489646131342. Learning Rate: 3.52220695970696e-05\n",
      "Batch 4,450 of 9,631 Elased 0:31:27. Training loss: 3.3081158724259794. Learning Rate: 3.5204725829725835e-05\n",
      "Batch 4,500 of 9,631 Elased 0:31:48. Training loss: 3.3088120199309454. Learning Rate: 3.5187382062382064e-05\n",
      "Batch 4,550 of 9,631 Elased 0:32:10. Training loss: 3.30713992695232. Learning Rate: 3.51700382950383e-05\n",
      "Batch 4,600 of 9,631 Elased 0:32:31. Training loss: 3.308326662156893. Learning Rate: 3.515269452769453e-05\n",
      "Batch 4,650 of 9,631 Elased 0:32:52. Training loss: 3.3073164000562443. Learning Rate: 3.513535076035076e-05\n",
      "Batch 4,700 of 9,631 Elased 0:33:13. Training loss: 3.307442963579868. Learning Rate: 3.5118006993006995e-05\n",
      "Batch 4,750 of 9,631 Elased 0:33:35. Training loss: 3.3079588521656236. Learning Rate: 3.5100663225663224e-05\n",
      "Batch 4,800 of 9,631 Elased 0:33:56. Training loss: 3.3062554991245268. Learning Rate: 3.508331945831946e-05\n",
      "Batch 4,850 of 9,631 Elased 0:34:17. Training loss: 3.3056930613763553. Learning Rate: 3.5065975690975696e-05\n",
      "Batch 4,900 of 9,631 Elased 0:34:39. Training loss: 3.3066067127305634. Learning Rate: 3.5048631923631926e-05\n",
      "Batch 4,950 of 9,631 Elased 0:35:00. Training loss: 3.3068541882254863. Learning Rate: 3.5031288156288155e-05\n",
      "Batch 5,000 of 9,631 Elased 0:35:21. Training loss: 3.3081736812591553. Learning Rate: 3.501394438894439e-05\n",
      "Batch 5,050 of 9,631 Elased 0:35:43. Training loss: 3.308112970201096. Learning Rate: 3.499660062160062e-05\n",
      "Batch 5,100 of 9,631 Elased 0:36:04. Training loss: 3.306343546708425. Learning Rate: 3.4979256854256856e-05\n",
      "Batch 5,150 of 9,631 Elased 0:36:25. Training loss: 3.306851300799731. Learning Rate: 3.496191308691309e-05\n",
      "Batch 5,200 of 9,631 Elased 0:36:47. Training loss: 3.3063688047803366. Learning Rate: 3.494456931956932e-05\n",
      "Batch 5,250 of 9,631 Elased 0:37:08. Training loss: 3.3060928954623994. Learning Rate: 3.492722555222556e-05\n",
      "Batch 5,300 of 9,631 Elased 0:37:30. Training loss: 3.3060043430778214. Learning Rate: 3.490988178488179e-05\n",
      "Batch 5,350 of 9,631 Elased 0:37:52. Training loss: 3.308293133316753. Learning Rate: 3.4892538017538016e-05\n",
      "Batch 5,400 of 9,631 Elased 0:38:14. Training loss: 3.308409088143596. Learning Rate: 3.487519425019425e-05\n",
      "Batch 5,450 of 9,631 Elased 0:38:36. Training loss: 3.30839527961311. Learning Rate: 3.485785048285049e-05\n",
      "Batch 5,500 of 9,631 Elased 0:38:58. Training loss: 3.308665724841031. Learning Rate: 3.484050671550672e-05\n",
      "Batch 5,550 of 9,631 Elased 0:39:20. Training loss: 3.3091214259035953. Learning Rate: 3.4823162948162953e-05\n",
      "Batch 5,600 of 9,631 Elased 0:39:42. Training loss: 3.3085347880210194. Learning Rate: 3.480581918081918e-05\n",
      "Batch 5,650 of 9,631 Elased 0:40:04. Training loss: 3.310306599541048. Learning Rate: 3.478847541347541e-05\n",
      "Batch 5,700 of 9,631 Elased 0:40:25. Training loss: 3.3101483448764735. Learning Rate: 3.477113164613165e-05\n",
      "Batch 5,750 of 9,631 Elased 0:40:46. Training loss: 3.3099671581931736. Learning Rate: 3.475378787878788e-05\n",
      "Batch 5,800 of 9,631 Elased 0:41:07. Training loss: 3.310139642542806. Learning Rate: 3.473644411144411e-05\n",
      "Batch 5,850 of 9,631 Elased 0:41:29. Training loss: 3.3104362994381504. Learning Rate: 3.471910034410035e-05\n",
      "Batch 5,900 of 9,631 Elased 0:41:50. Training loss: 3.310383615837259. Learning Rate: 3.470175657675658e-05\n",
      "Batch 5,950 of 9,631 Elased 0:42:11. Training loss: 3.3106349719672643. Learning Rate: 3.468441280941281e-05\n",
      "Batch 6,000 of 9,631 Elased 0:42:32. Training loss: 3.3102052963376045. Learning Rate: 3.4667069042069044e-05\n",
      "Batch 6,050 of 9,631 Elased 0:42:53. Training loss: 3.310401249345669. Learning Rate: 3.464972527472527e-05\n",
      "Batch 6,100 of 9,631 Elased 0:43:15. Training loss: 3.310650044034739. Learning Rate: 3.463238150738151e-05\n",
      "Batch 6,150 of 9,631 Elased 0:43:36. Training loss: 3.310453485357083. Learning Rate: 3.4615037740037745e-05\n",
      "Batch 6,200 of 9,631 Elased 0:43:58. Training loss: 3.3085080516338348. Learning Rate: 3.4597693972693975e-05\n",
      "Batch 6,250 of 9,631 Elased 0:44:19. Training loss: 3.308011178512573. Learning Rate: 3.458035020535021e-05\n",
      "Batch 6,300 of 9,631 Elased 0:44:40. Training loss: 3.307936493366484. Learning Rate: 3.456300643800644e-05\n",
      "Batch 6,350 of 9,631 Elased 0:45:01. Training loss: 3.3076676561701017. Learning Rate: 3.454566267066267e-05\n",
      "Batch 6,400 of 9,631 Elased 0:45:22. Training loss: 3.3071862388774753. Learning Rate: 3.4528318903318905e-05\n",
      "Batch 6,450 of 9,631 Elased 0:45:44. Training loss: 3.3081358812021655. Learning Rate: 3.4510975135975135e-05\n",
      "Batch 6,500 of 9,631 Elased 0:46:04. Training loss: 3.30665079670686. Learning Rate: 3.449363136863137e-05\n",
      "Batch 6,550 of 9,631 Elased 0:46:26. Training loss: 3.3068155506002994. Learning Rate: 3.4476287601287607e-05\n",
      "Batch 6,600 of 9,631 Elased 0:46:47. Training loss: 3.307464224190423. Learning Rate: 3.4458943833943836e-05\n",
      "Batch 6,650 of 9,631 Elased 0:47:08. Training loss: 3.307820717105292. Learning Rate: 3.4441600066600065e-05\n",
      "Batch 6,700 of 9,631 Elased 0:47:29. Training loss: 3.3090516210136127. Learning Rate: 3.44242562992563e-05\n",
      "Batch 6,750 of 9,631 Elased 0:47:51. Training loss: 3.309185436125155. Learning Rate: 3.440691253191253e-05\n",
      "Batch 6,800 of 9,631 Elased 0:48:12. Training loss: 3.308960932230248. Learning Rate: 3.4389568764568767e-05\n",
      "Batch 6,850 of 9,631 Elased 0:48:33. Training loss: 3.308273470680209. Learning Rate: 3.4372224997225e-05\n",
      "Batch 6,900 of 9,631 Elased 0:48:54. Training loss: 3.3079868620547694. Learning Rate: 3.435488122988123e-05\n",
      "Batch 6,950 of 9,631 Elased 0:49:15. Training loss: 3.308102016946395. Learning Rate: 3.433753746253747e-05\n",
      "Batch 7,000 of 9,631 Elased 0:49:37. Training loss: 3.3073734581981387. Learning Rate: 3.43201936951937e-05\n",
      "Batch 7,050 of 9,631 Elased 0:49:58. Training loss: 3.3068384509560063. Learning Rate: 3.4302849927849926e-05\n",
      "Batch 7,100 of 9,631 Elased 0:50:19. Training loss: 3.3070312832946507. Learning Rate: 3.428550616050616e-05\n",
      "Batch 7,150 of 9,631 Elased 0:50:40. Training loss: 3.3068505339355734. Learning Rate: 3.42681623931624e-05\n",
      "Batch 7,200 of 9,631 Elased 0:51:01. Training loss: 3.3068859902355405. Learning Rate: 3.425081862581863e-05\n",
      "Batch 7,250 of 9,631 Elased 0:51:22. Training loss: 3.306009428616228. Learning Rate: 3.4233474858474864e-05\n",
      "Batch 7,300 of 9,631 Elased 0:51:44. Training loss: 3.3064660176512315. Learning Rate: 3.421613109113109e-05\n",
      "Batch 7,350 of 9,631 Elased 0:52:05. Training loss: 3.3061637096340153. Learning Rate: 3.419878732378732e-05\n",
      "Batch 7,400 of 9,631 Elased 0:52:26. Training loss: 3.304775766456449. Learning Rate: 3.418144355644356e-05\n",
      "Batch 7,450 of 9,631 Elased 0:52:48. Training loss: 3.304564276653648. Learning Rate: 3.416409978909979e-05\n",
      "Batch 7,500 of 9,631 Elased 0:53:09. Training loss: 3.3035609556357066. Learning Rate: 3.4146756021756024e-05\n",
      "Batch 7,550 of 9,631 Elased 0:53:30. Training loss: 3.3033272269703695. Learning Rate: 3.412941225441226e-05\n",
      "Batch 7,600 of 9,631 Elased 0:53:52. Training loss: 3.303844052192412. Learning Rate: 3.411206848706849e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7,650 of 9,631 Elased 0:54:13. Training loss: 3.3039532582744275. Learning Rate: 3.409472471972472e-05\n",
      "Batch 7,700 of 9,631 Elased 0:54:34. Training loss: 3.3031026445580767. Learning Rate: 3.4077380952380954e-05\n",
      "Batch 7,750 of 9,631 Elased 0:54:55. Training loss: 3.301617457866669. Learning Rate: 3.4060037185037184e-05\n",
      "Batch 7,800 of 9,631 Elased 0:55:17. Training loss: 3.302196044784326. Learning Rate: 3.404269341769342e-05\n",
      "Batch 7,850 of 9,631 Elased 0:55:38. Training loss: 3.3026951080219002. Learning Rate: 3.4025349650349656e-05\n",
      "Batch 7,900 of 9,631 Elased 0:55:59. Training loss: 3.301331065002876. Learning Rate: 3.4008005883005885e-05\n",
      "Batch 7,950 of 9,631 Elased 0:56:20. Training loss: 3.302670526204619. Learning Rate: 3.399066211566212e-05\n",
      "Batch 8,000 of 9,631 Elased 0:56:42. Training loss: 3.3024358282089232. Learning Rate: 3.397331834831835e-05\n",
      "Batch 8,050 of 9,631 Elased 0:57:03. Training loss: 3.303332382610866. Learning Rate: 3.395597458097458e-05\n",
      "Batch 8,100 of 9,631 Elased 0:57:24. Training loss: 3.3033474807091703. Learning Rate: 3.3938630813630816e-05\n",
      "Batch 8,150 of 9,631 Elased 0:57:45. Training loss: 3.3042836877612247. Learning Rate: 3.3921287046287045e-05\n",
      "Batch 8,200 of 9,631 Elased 0:58:07. Training loss: 3.304199033103338. Learning Rate: 3.390394327894328e-05\n",
      "Batch 8,250 of 9,631 Elased 0:58:28. Training loss: 3.304793758016644. Learning Rate: 3.388659951159952e-05\n",
      "Batch 8,300 of 9,631 Elased 0:58:49. Training loss: 3.305030322735568. Learning Rate: 3.3869255744255746e-05\n",
      "Batch 8,350 of 9,631 Elased 0:59:11. Training loss: 3.304849471703261. Learning Rate: 3.3851911976911975e-05\n",
      "Batch 8,400 of 9,631 Elased 0:59:32. Training loss: 3.3040837527456737. Learning Rate: 3.383456820956821e-05\n",
      "Batch 8,450 of 9,631 Elased 0:59:53. Training loss: 3.3036111684099456. Learning Rate: 3.381722444222444e-05\n",
      "Batch 8,500 of 9,631 Elased 1:00:14. Training loss: 3.3032498552658978. Learning Rate: 3.379988067488068e-05\n",
      "Batch 8,550 of 9,631 Elased 1:00:36. Training loss: 3.3031014079936067. Learning Rate: 3.378253690753691e-05\n",
      "Batch 8,600 of 9,631 Elased 1:00:57. Training loss: 3.3037449027771175. Learning Rate: 3.376519314019314e-05\n",
      "Batch 8,650 of 9,631 Elased 1:01:18. Training loss: 3.3026883353801133. Learning Rate: 3.374784937284938e-05\n",
      "Batch 8,700 of 9,631 Elased 1:01:40. Training loss: 3.3025762685414017. Learning Rate: 3.373050560550561e-05\n",
      "Batch 8,750 of 9,631 Elased 1:02:01. Training loss: 3.3024868412699018. Learning Rate: 3.371316183816184e-05\n",
      "Batch 8,800 of 9,631 Elased 1:02:22. Training loss: 3.302359679151665. Learning Rate: 3.369581807081807e-05\n",
      "Batch 8,850 of 9,631 Elased 1:02:43. Training loss: 3.302071916127609. Learning Rate: 3.367847430347431e-05\n",
      "Batch 8,900 of 9,631 Elased 1:03:04. Training loss: 3.302244208635909. Learning Rate: 3.366113053613054e-05\n",
      "Batch 8,950 of 9,631 Elased 1:03:25. Training loss: 3.301857813004009. Learning Rate: 3.3643786768786774e-05\n",
      "Batch 9,000 of 9,631 Elased 1:03:47. Training loss: 3.3008109101454415. Learning Rate: 3.3626443001443e-05\n",
      "Batch 9,050 of 9,631 Elased 1:04:08. Training loss: 3.301760805377644. Learning Rate: 3.360909923409923e-05\n",
      "Batch 9,100 of 9,631 Elased 1:04:29. Training loss: 3.3016703592289933. Learning Rate: 3.359175546675547e-05\n",
      "Batch 9,150 of 9,631 Elased 1:04:50. Training loss: 3.3019521686157893. Learning Rate: 3.35744116994117e-05\n",
      "Batch 9,200 of 9,631 Elased 1:05:12. Training loss: 3.3023443176564964. Learning Rate: 3.3557067932067934e-05\n",
      "Batch 9,250 of 9,631 Elased 1:05:33. Training loss: 3.3021189983342145. Learning Rate: 3.353972416472417e-05\n",
      "Batch 9,300 of 9,631 Elased 1:05:54. Training loss: 3.3023488092422486. Learning Rate: 3.35223803973804e-05\n",
      "Batch 9,350 of 9,631 Elased 1:06:16. Training loss: 3.302339585171664. Learning Rate: 3.350503663003663e-05\n",
      "Batch 9,400 of 9,631 Elased 1:06:37. Training loss: 3.302153155816362. Learning Rate: 3.3487692862692865e-05\n",
      "Batch 9,450 of 9,631 Elased 1:06:58. Training loss: 3.3020363317595587. Learning Rate: 3.3470349095349094e-05\n",
      "Batch 9,500 of 9,631 Elased 1:07:19. Training loss: 3.3016686635644814. Learning Rate: 3.345300532800533e-05\n",
      "Batch 9,550 of 9,631 Elased 1:07:41. Training loss: 3.301043847705681. Learning Rate: 3.3435661560661566e-05\n",
      "Batch 9,600 of 9,631 Elased 1:08:03. Training loss: 3.301505047418177. Learning Rate: 3.3418317793317795e-05\n",
      "\n",
      "\n",
      "  Average training loss: 3.30\n",
      "  Training epcoh took: 1:08:16\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb31b158627f4472b664a2e122891bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=67.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Validation Loss: 3.45\n",
      "  Validation took: 0:00:43\n",
      "\n",
      "======== Epoch 21 / 30 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c18924449ab5462d84fb2a45e3aeee95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch    50 of 9,631 Elased 0:00:21. Training loss: 3.3649933385849. Learning Rate: 3.339022089022089e-05\n",
      "Batch   100 of 9,631 Elased 0:00:42. Training loss: 3.302482535839081. Learning Rate: 3.337287712287712e-05\n",
      "Batch   150 of 9,631 Elased 0:01:03. Training loss: 3.3099103728930155. Learning Rate: 3.335553335553336e-05\n",
      "Batch   200 of 9,631 Elased 0:01:25. Training loss: 3.3385828226804732. Learning Rate: 3.333818958818959e-05\n",
      "Batch   250 of 9,631 Elased 0:01:46. Training loss: 3.2913887124061585. Learning Rate: 3.3320845820845824e-05\n",
      "Batch   300 of 9,631 Elased 0:02:07. Training loss: 3.279579863945643. Learning Rate: 3.330350205350205e-05\n",
      "Batch   350 of 9,631 Elased 0:02:29. Training loss: 3.281764369351523. Learning Rate: 3.328615828615829e-05\n",
      "Batch   400 of 9,631 Elased 0:02:50. Training loss: 3.2960089060664175. Learning Rate: 3.3268814518814525e-05\n",
      "Batch   450 of 9,631 Elased 0:03:11. Training loss: 3.2974634438090855. Learning Rate: 3.3251470751470754e-05\n",
      "Batch   500 of 9,631 Elased 0:03:32. Training loss: 3.3041878516674044. Learning Rate: 3.3234126984126983e-05\n",
      "Batch   550 of 9,631 Elased 0:03:53. Training loss: 3.2908259493654426. Learning Rate: 3.321678321678322e-05\n",
      "Batch   600 of 9,631 Elased 0:04:14. Training loss: 3.3056582615772885. Learning Rate: 3.319943944943945e-05\n",
      "Batch   650 of 9,631 Elased 0:04:35. Training loss: 3.306958802846762. Learning Rate: 3.3182095682095685e-05\n",
      "Batch   700 of 9,631 Elased 0:04:56. Training loss: 3.3001512134075166. Learning Rate: 3.316475191475192e-05\n",
      "Batch   750 of 9,631 Elased 0:05:17. Training loss: 3.2869483160972597. Learning Rate: 3.314740814740815e-05\n",
      "Batch   800 of 9,631 Elased 0:05:38. Training loss: 3.2842235681414604. Learning Rate: 3.313006438006438e-05\n",
      "Batch   850 of 9,631 Elased 0:05:59. Training loss: 3.278643817060134. Learning Rate: 3.3112720612720615e-05\n",
      "Batch   900 of 9,631 Elased 0:06:21. Training loss: 3.2776934112442864. Learning Rate: 3.3095376845376845e-05\n",
      "Batch   950 of 9,631 Elased 0:06:42. Training loss: 3.2731574826491507. Learning Rate: 3.3078033078033074e-05\n",
      "Batch 1,000 of 9,631 Elased 0:07:03. Training loss: 3.274615352153778. Learning Rate: 3.306068931068932e-05\n",
      "Batch 1,050 of 9,631 Elased 0:07:24. Training loss: 3.2778984884988693. Learning Rate: 3.3043345543345546e-05\n",
      "Batch 1,100 of 9,631 Elased 0:07:45. Training loss: 3.2801804592392663. Learning Rate: 3.302600177600178e-05\n",
      "Batch 1,150 of 9,631 Elased 0:08:06. Training loss: 3.278452409454014. Learning Rate: 3.300865800865801e-05\n",
      "Batch 1,200 of 9,631 Elased 0:08:28. Training loss: 3.2810351032018663. Learning Rate: 3.299131424131424e-05\n",
      "Batch 1,250 of 9,631 Elased 0:08:49. Training loss: 3.2820729429244997. Learning Rate: 3.297397047397048e-05\n",
      "Batch 1,300 of 9,631 Elased 0:09:10. Training loss: 3.2779288048010606. Learning Rate: 3.2956626706626706e-05\n",
      "Batch 1,350 of 9,631 Elased 0:09:31. Training loss: 3.2787281539705067. Learning Rate: 3.293928293928294e-05\n",
      "Batch 1,400 of 9,631 Elased 0:09:53. Training loss: 3.277378692456654. Learning Rate: 3.292193917193918e-05\n",
      "Batch 1,450 of 9,631 Elased 0:10:14. Training loss: 3.2753535481156972. Learning Rate: 3.290459540459541e-05\n",
      "Batch 1,500 of 9,631 Elased 0:10:35. Training loss: 3.2728756381670636. Learning Rate: 3.2887251637251637e-05\n",
      "Batch 1,550 of 9,631 Elased 0:10:57. Training loss: 3.2729675762114985. Learning Rate: 3.286990786990787e-05\n",
      "Batch 1,600 of 9,631 Elased 0:11:18. Training loss: 3.2794922725856304. Learning Rate: 3.28525641025641e-05\n",
      "Batch 1,650 of 9,631 Elased 0:11:39. Training loss: 3.282480157071894. Learning Rate: 3.283522033522034e-05\n",
      "Batch 1,700 of 9,631 Elased 0:12:00. Training loss: 3.27993098840994. Learning Rate: 3.2817876567876574e-05\n",
      "Batch 1,750 of 9,631 Elased 0:12:22. Training loss: 3.280146654810224. Learning Rate: 3.28005328005328e-05\n",
      "Batch 1,800 of 9,631 Elased 0:12:43. Training loss: 3.279259903298484. Learning Rate: 3.278318903318903e-05\n",
      "Batch 1,850 of 9,631 Elased 0:13:04. Training loss: 3.2816094398498534. Learning Rate: 3.276584526584527e-05\n",
      "Batch 1,900 of 9,631 Elased 0:13:26. Training loss: 3.280511621675993. Learning Rate: 3.27485014985015e-05\n",
      "Batch 1,950 of 9,631 Elased 0:13:47. Training loss: 3.2770879222185183. Learning Rate: 3.273115773115773e-05\n",
      "Batch 2,000 of 9,631 Elased 0:14:08. Training loss: 3.273191550254822. Learning Rate: 3.271381396381396e-05\n",
      "Batch 2,050 of 9,631 Elased 0:14:29. Training loss: 3.274356927406497. Learning Rate: 3.26964701964702e-05\n",
      "Batch 2,100 of 9,631 Elased 0:14:51. Training loss: 3.278403914655958. Learning Rate: 3.2679126429126435e-05\n",
      "Batch 2,150 of 9,631 Elased 0:15:12. Training loss: 3.278529193456783. Learning Rate: 3.2661782661782664e-05\n",
      "Batch 2,200 of 9,631 Elased 0:15:33. Training loss: 3.2781661222197793. Learning Rate: 3.2644438894438894e-05\n",
      "Batch 2,250 of 9,631 Elased 0:15:53. Training loss: 3.276478904724121. Learning Rate: 3.262709512709513e-05\n",
      "Batch 2,300 of 9,631 Elased 0:16:15. Training loss: 3.277520364471104. Learning Rate: 3.260975135975136e-05\n",
      "Batch 2,350 of 9,631 Elased 0:16:36. Training loss: 3.275391797004862. Learning Rate: 3.2592407592407595e-05\n",
      "Batch 2,400 of 9,631 Elased 0:16:57. Training loss: 3.2749073258042336. Learning Rate: 3.257506382506383e-05\n",
      "Batch 2,450 of 9,631 Elased 0:17:19. Training loss: 3.272110167814761. Learning Rate: 3.255772005772006e-05\n",
      "Batch 2,500 of 9,631 Elased 0:17:40. Training loss: 3.270258719062805. Learning Rate: 3.254037629037629e-05\n",
      "Batch 2,550 of 9,631 Elased 0:18:01. Training loss: 3.2751005470051484. Learning Rate: 3.2523032523032526e-05\n",
      "Batch 2,600 of 9,631 Elased 0:18:22. Training loss: 3.278080593714347. Learning Rate: 3.2505688755688755e-05\n",
      "Batch 2,650 of 9,631 Elased 0:18:44. Training loss: 3.2803127742263505. Learning Rate: 3.2488344988344984e-05\n",
      "Batch 2,700 of 9,631 Elased 0:19:05. Training loss: 3.2829421001893504. Learning Rate: 3.247100122100123e-05\n",
      "Batch 2,750 of 9,631 Elased 0:19:26. Training loss: 3.2805837067257273. Learning Rate: 3.2453657453657456e-05\n",
      "Batch 2,800 of 9,631 Elased 0:19:48. Training loss: 3.281426205422197. Learning Rate: 3.2436313686313686e-05\n",
      "Batch 2,850 of 9,631 Elased 0:20:09. Training loss: 3.2848972266598753. Learning Rate: 3.241896991896992e-05\n",
      "Batch 2,900 of 9,631 Elased 0:20:30. Training loss: 3.285341150390691. Learning Rate: 3.240162615162615e-05\n",
      "Batch 2,950 of 9,631 Elased 0:20:51. Training loss: 3.285824382385965. Learning Rate: 3.238428238428239e-05\n",
      "Batch 3,000 of 9,631 Elased 0:21:13. Training loss: 3.2886327801942827. Learning Rate: 3.2366938616938616e-05\n",
      "Batch 3,050 of 9,631 Elased 0:21:34. Training loss: 3.288504053451976. Learning Rate: 3.234959484959485e-05\n",
      "Batch 3,100 of 9,631 Elased 0:21:55. Training loss: 3.2888396038932184. Learning Rate: 3.233225108225109e-05\n",
      "Batch 3,150 of 9,631 Elased 0:22:16. Training loss: 3.285051163756658. Learning Rate: 3.231490731490732e-05\n",
      "Batch 3,200 of 9,631 Elased 0:22:38. Training loss: 3.2874759012833237. Learning Rate: 3.229756354756355e-05\n",
      "Batch 3,250 of 9,631 Elased 0:22:59. Training loss: 3.2873880041929393. Learning Rate: 3.228021978021978e-05\n",
      "Batch 3,300 of 9,631 Elased 0:23:20. Training loss: 3.2895238454775377. Learning Rate: 3.226287601287601e-05\n",
      "Batch 3,350 of 9,631 Elased 0:23:42. Training loss: 3.290065951596445. Learning Rate: 3.224553224553225e-05\n",
      "Batch 3,400 of 9,631 Elased 0:24:03. Training loss: 3.2902492364013898. Learning Rate: 3.2228188478188484e-05\n",
      "Batch 3,450 of 9,631 Elased 0:24:24. Training loss: 3.290712413304094. Learning Rate: 3.2210844710844714e-05\n",
      "Batch 3,500 of 9,631 Elased 0:24:45. Training loss: 3.2901760036604744. Learning Rate: 3.219350094350094e-05\n",
      "Batch 3,550 of 9,631 Elased 0:25:07. Training loss: 3.29137451984513. Learning Rate: 3.217615717615718e-05\n",
      "Batch 3,600 of 9,631 Elased 0:25:28. Training loss: 3.292842445373535. Learning Rate: 3.215881340881341e-05\n",
      "Batch 3,650 of 9,631 Elased 0:25:49. Training loss: 3.2969071025064545. Learning Rate: 3.214146964146964e-05\n",
      "Batch 3,700 of 9,631 Elased 0:26:11. Training loss: 3.296188009622935. Learning Rate: 3.2124125874125873e-05\n",
      "Batch 3,750 of 9,631 Elased 0:26:32. Training loss: 3.294907348092397. Learning Rate: 3.210678210678211e-05\n",
      "Batch 3,800 of 9,631 Elased 0:26:53. Training loss: 3.2938186416186785. Learning Rate: 3.2089438339438346e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3,850 of 9,631 Elased 0:27:14. Training loss: 3.291153255035351. Learning Rate: 3.2072094572094575e-05\n",
      "Batch 3,900 of 9,631 Elased 0:27:36. Training loss: 3.289906772069442. Learning Rate: 3.2054750804750804e-05\n",
      "Batch 3,950 of 9,631 Elased 0:27:57. Training loss: 3.29107799828807. Learning Rate: 3.203740703740704e-05\n",
      "Batch 4,000 of 9,631 Elased 0:28:19. Training loss: 3.2915778216421603. Learning Rate: 3.202006327006327e-05\n",
      "Batch 4,050 of 9,631 Elased 0:28:40. Training loss: 3.2908860143908747. Learning Rate: 3.2002719502719505e-05\n",
      "Batch 4,100 of 9,631 Elased 0:29:01. Training loss: 3.2902248482878615. Learning Rate: 3.198537573537574e-05\n",
      "Batch 4,150 of 9,631 Elased 0:29:23. Training loss: 3.2898141803224403. Learning Rate: 3.196803196803197e-05\n",
      "Batch 4,200 of 9,631 Elased 0:29:44. Training loss: 3.2897007758560637. Learning Rate: 3.19506882006882e-05\n",
      "Batch 4,250 of 9,631 Elased 0:30:05. Training loss: 3.29023745951933. Learning Rate: 3.1933344433344436e-05\n",
      "Batch 4,300 of 9,631 Elased 0:30:26. Training loss: 3.289868578522704. Learning Rate: 3.1916000666000665e-05\n",
      "Batch 4,350 of 9,631 Elased 0:30:48. Training loss: 3.2891713916844334. Learning Rate: 3.1898656898656895e-05\n",
      "Batch 4,400 of 9,631 Elased 0:31:09. Training loss: 3.290479115952145. Learning Rate: 3.188131313131314e-05\n",
      "Batch 4,450 of 9,631 Elased 0:31:30. Training loss: 3.291181692273429. Learning Rate: 3.186396936396937e-05\n",
      "Batch 4,500 of 9,631 Elased 0:31:51. Training loss: 3.292538850016064. Learning Rate: 3.1846625596625596e-05\n",
      "Batch 4,550 of 9,631 Elased 0:32:13. Training loss: 3.290916230049762. Learning Rate: 3.182928182928183e-05\n",
      "Batch 4,600 of 9,631 Elased 0:32:34. Training loss: 3.292327588148739. Learning Rate: 3.181193806193806e-05\n",
      "Batch 4,650 of 9,631 Elased 0:32:56. Training loss: 3.2917568469047547. Learning Rate: 3.17945942945943e-05\n",
      "Batch 4,700 of 9,631 Elased 0:33:17. Training loss: 3.2921674066655178. Learning Rate: 3.1777250527250527e-05\n",
      "Batch 4,750 of 9,631 Elased 0:33:38. Training loss: 3.292090872864974. Learning Rate: 3.175990675990676e-05\n",
      "Batch 4,800 of 9,631 Elased 0:33:59. Training loss: 3.290352260917425. Learning Rate: 3.1742562992563e-05\n",
      "Batch 4,850 of 9,631 Elased 0:34:20. Training loss: 3.2895056873498505. Learning Rate: 3.172521922521923e-05\n",
      "Batch 4,900 of 9,631 Elased 0:34:41. Training loss: 3.290548381416165. Learning Rate: 3.170787545787546e-05\n",
      "Batch 4,950 of 9,631 Elased 0:35:02. Training loss: 3.2908747582965425. Learning Rate: 3.169053169053169e-05\n",
      "Batch 5,000 of 9,631 Elased 0:35:24. Training loss: 3.2919955754995347. Learning Rate: 3.167318792318792e-05\n",
      "Batch 5,050 of 9,631 Elased 0:35:45. Training loss: 3.2918368771052595. Learning Rate: 3.165584415584416e-05\n",
      "Batch 5,100 of 9,631 Elased 0:36:06. Training loss: 3.289886645195531. Learning Rate: 3.1638500388500395e-05\n",
      "Batch 5,150 of 9,631 Elased 0:36:28. Training loss: 3.2900952969477015. Learning Rate: 3.1621156621156624e-05\n",
      "Batch 5,200 of 9,631 Elased 0:36:49. Training loss: 3.2895295110803384. Learning Rate: 3.160381285381285e-05\n",
      "Batch 5,250 of 9,631 Elased 0:37:10. Training loss: 3.2898146803719657. Learning Rate: 3.158646908646909e-05\n",
      "Batch 5,300 of 9,631 Elased 0:37:31. Training loss: 3.289325464536559. Learning Rate: 3.156912531912532e-05\n",
      "Batch 5,350 of 9,631 Elased 0:37:53. Training loss: 3.2914732792221497. Learning Rate: 3.155178155178155e-05\n",
      "Batch 5,400 of 9,631 Elased 0:38:14. Training loss: 3.2913459165449495. Learning Rate: 3.1534437784437784e-05\n",
      "Batch 5,450 of 9,631 Elased 0:38:35. Training loss: 3.2912833650396505. Learning Rate: 3.151709401709402e-05\n",
      "Batch 5,500 of 9,631 Elased 0:38:57. Training loss: 3.291687902623957. Learning Rate: 3.1499750249750256e-05\n",
      "Batch 5,550 of 9,631 Elased 0:39:18. Training loss: 3.2921730916134946. Learning Rate: 3.1482406482406485e-05\n",
      "Batch 5,600 of 9,631 Elased 0:39:39. Training loss: 3.291436392409461. Learning Rate: 3.1465062715062714e-05\n",
      "Batch 5,650 of 9,631 Elased 0:40:00. Training loss: 3.293066591710116. Learning Rate: 3.144771894771895e-05\n",
      "Batch 5,700 of 9,631 Elased 0:40:22. Training loss: 3.2930441943177007. Learning Rate: 3.143037518037518e-05\n",
      "Batch 5,750 of 9,631 Elased 0:40:43. Training loss: 3.2925917370215707. Learning Rate: 3.1413031413031416e-05\n",
      "Batch 5,800 of 9,631 Elased 0:41:04. Training loss: 3.2929002696275713. Learning Rate: 3.139568764568765e-05\n",
      "Batch 5,850 of 9,631 Elased 0:41:25. Training loss: 3.2933525928880414. Learning Rate: 3.137834387834388e-05\n",
      "Batch 5,900 of 9,631 Elased 0:41:47. Training loss: 3.293291608014349. Learning Rate: 3.136100011100011e-05\n",
      "Batch 5,950 of 9,631 Elased 0:42:08. Training loss: 3.2938741626058308. Learning Rate: 3.1343656343656346e-05\n",
      "Batch 6,000 of 9,631 Elased 0:42:29. Training loss: 3.2933056490619976. Learning Rate: 3.1326312576312576e-05\n",
      "Batch 6,050 of 9,631 Elased 0:42:51. Training loss: 3.293443086974877. Learning Rate: 3.1308968808968805e-05\n",
      "Batch 6,100 of 9,631 Elased 0:43:12. Training loss: 3.293784202943083. Learning Rate: 3.129162504162505e-05\n",
      "Batch 6,150 of 9,631 Elased 0:43:33. Training loss: 3.2937254137333816. Learning Rate: 3.127428127428128e-05\n",
      "Batch 6,200 of 9,631 Elased 0:43:54. Training loss: 3.2917591658907552. Learning Rate: 3.1256937506937506e-05\n",
      "Batch 6,250 of 9,631 Elased 0:44:16. Training loss: 3.2912302648353577. Learning Rate: 3.123959373959374e-05\n",
      "Batch 6,300 of 9,631 Elased 0:44:37. Training loss: 3.2911222831975846. Learning Rate: 3.122224997224997e-05\n",
      "Batch 6,350 of 9,631 Elased 0:44:58. Training loss: 3.290734028328122. Learning Rate: 3.120490620490621e-05\n",
      "Batch 6,400 of 9,631 Elased 0:45:20. Training loss: 3.2902823213115333. Learning Rate: 3.118756243756244e-05\n",
      "Batch 6,450 of 9,631 Elased 0:45:41. Training loss: 3.291474691768025. Learning Rate: 3.117021867021867e-05\n",
      "Batch 6,500 of 9,631 Elased 0:46:02. Training loss: 3.289942120570403. Learning Rate: 3.115287490287491e-05\n",
      "Batch 6,550 of 9,631 Elased 0:46:23. Training loss: 3.290111496594116. Learning Rate: 3.113553113553114e-05\n",
      "Batch 6,600 of 9,631 Elased 0:46:44. Training loss: 3.29077489276727. Learning Rate: 3.111818736818737e-05\n",
      "Batch 6,650 of 9,631 Elased 0:47:06. Training loss: 3.291050151685127. Learning Rate: 3.1100843600843604e-05\n",
      "Batch 6,700 of 9,631 Elased 0:47:27. Training loss: 3.292208653083488. Learning Rate: 3.108349983349983e-05\n",
      "Batch 6,750 of 9,631 Elased 0:47:48. Training loss: 3.292102755352303. Learning Rate: 3.106615606615607e-05\n",
      "Batch 6,800 of 9,631 Elased 0:48:10. Training loss: 3.291930852199302. Learning Rate: 3.1048812298812305e-05\n",
      "Batch 6,850 of 9,631 Elased 0:48:31. Training loss: 3.2914299433596814. Learning Rate: 3.1031468531468534e-05\n",
      "Batch 6,900 of 9,631 Elased 0:48:52. Training loss: 3.290927173797635. Learning Rate: 3.1014124764124763e-05\n",
      "Batch 6,950 of 9,631 Elased 0:49:13. Training loss: 3.290816301630555. Learning Rate: 3.0996780996781e-05\n",
      "Batch 7,000 of 9,631 Elased 0:49:34. Training loss: 3.2902060641731534. Learning Rate: 3.097943722943723e-05\n",
      "Batch 7,050 of 9,631 Elased 0:49:55. Training loss: 3.289549476897463. Learning Rate: 3.096209346209346e-05\n",
      "Batch 7,100 of 9,631 Elased 0:50:17. Training loss: 3.2896586257135363. Learning Rate: 3.0944749694749694e-05\n",
      "Batch 7,150 of 9,631 Elased 0:50:39. Training loss: 3.2895137530940395. Learning Rate: 3.092740592740593e-05\n",
      "Batch 7,200 of 9,631 Elased 0:51:01. Training loss: 3.2896980874737105. Learning Rate: 3.0910062160062166e-05\n",
      "Batch 7,250 of 9,631 Elased 0:51:23. Training loss: 3.288818094056228. Learning Rate: 3.0892718392718395e-05\n",
      "Batch 7,300 of 9,631 Elased 0:51:45. Training loss: 3.289227957741855. Learning Rate: 3.0875374625374625e-05\n",
      "Batch 7,350 of 9,631 Elased 0:52:07. Training loss: 3.288873196799739. Learning Rate: 3.085803085803086e-05\n",
      "Batch 7,400 of 9,631 Elased 0:52:29. Training loss: 3.287768309937941. Learning Rate: 3.084068709068709e-05\n",
      "Batch 7,450 of 9,631 Elased 0:52:51. Training loss: 3.2877733604219936. Learning Rate: 3.0823343323343326e-05\n",
      "Batch 7,500 of 9,631 Elased 0:53:13. Training loss: 3.2867586247285208. Learning Rate: 3.080599955599956e-05\n",
      "Batch 7,550 of 9,631 Elased 0:53:35. Training loss: 3.2865570003623206. Learning Rate: 3.078865578865579e-05\n",
      "Batch 7,600 of 9,631 Elased 0:53:56. Training loss: 3.287023316998231. Learning Rate: 3.077131202131202e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7,650 of 9,631 Elased 0:54:17. Training loss: 3.287280001063752. Learning Rate: 3.075396825396826e-05\n",
      "Batch 7,700 of 9,631 Elased 0:54:39. Training loss: 3.286356069066308. Learning Rate: 3.0736624486624486e-05\n",
      "Batch 7,750 of 9,631 Elased 0:55:00. Training loss: 3.2849566207239707. Learning Rate: 3.071928071928072e-05\n",
      "Batch 7,800 of 9,631 Elased 0:55:21. Training loss: 3.285325195774054. Learning Rate: 3.070193695193696e-05\n",
      "Batch 7,850 of 9,631 Elased 0:55:43. Training loss: 3.285769348858268. Learning Rate: 3.068459318459319e-05\n",
      "Batch 7,900 of 9,631 Elased 0:56:04. Training loss: 3.2845289031614233. Learning Rate: 3.0667249417249417e-05\n",
      "Batch 7,950 of 9,631 Elased 0:56:25. Training loss: 3.2856919700244687. Learning Rate: 3.064990564990565e-05\n",
      "Batch 8,000 of 9,631 Elased 0:56:46. Training loss: 3.2851782439500092. Learning Rate: 3.063256188256188e-05\n",
      "Batch 8,050 of 9,631 Elased 0:57:07. Training loss: 3.2864305380560594. Learning Rate: 3.061521811521812e-05\n",
      "Batch 8,100 of 9,631 Elased 0:57:28. Training loss: 3.2865592860439676. Learning Rate: 3.059787434787435e-05\n",
      "Batch 8,150 of 9,631 Elased 0:57:49. Training loss: 3.2872895569450287. Learning Rate: 3.058053058053058e-05\n",
      "Batch 8,200 of 9,631 Elased 0:58:11. Training loss: 3.2868667716950903. Learning Rate: 3.056318681318682e-05\n",
      "Batch 8,250 of 9,631 Elased 0:58:32. Training loss: 3.2872611940701804. Learning Rate: 3.054584304584305e-05\n",
      "Batch 8,300 of 9,631 Elased 0:58:53. Training loss: 3.2873289603353983. Learning Rate: 3.052849927849928e-05\n",
      "Batch 8,350 of 9,631 Elased 0:59:14. Training loss: 3.2869873244462613. Learning Rate: 3.0511155511155514e-05\n",
      "Batch 8,400 of 9,631 Elased 0:59:36. Training loss: 3.2865205333914074. Learning Rate: 3.0493811743811746e-05\n",
      "Batch 8,450 of 9,631 Elased 0:59:57. Training loss: 3.285856847960568. Learning Rate: 3.0476467976467976e-05\n",
      "Batch 8,500 of 9,631 Elased 1:00:18. Training loss: 3.2855629140348994. Learning Rate: 3.0459124209124212e-05\n",
      "Batch 8,550 of 9,631 Elased 1:00:39. Training loss: 3.2851474143468846. Learning Rate: 3.0441780441780444e-05\n",
      "Batch 8,600 of 9,631 Elased 1:01:01. Training loss: 3.285493767815967. Learning Rate: 3.0424436674436674e-05\n",
      "Batch 8,650 of 9,631 Elased 1:01:22. Training loss: 3.284678416307262. Learning Rate: 3.040709290709291e-05\n",
      "Batch 8,700 of 9,631 Elased 1:01:43. Training loss: 3.284523664589586. Learning Rate: 3.038974913974914e-05\n",
      "Batch 8,750 of 9,631 Elased 1:02:05. Training loss: 3.284318944249834. Learning Rate: 3.0372405372405372e-05\n",
      "Batch 8,800 of 9,631 Elased 1:02:26. Training loss: 3.2843521049483257. Learning Rate: 3.0355061605061608e-05\n",
      "Batch 8,850 of 9,631 Elased 1:02:47. Training loss: 3.2841528774654796. Learning Rate: 3.0337717837717837e-05\n",
      "Batch 8,900 of 9,631 Elased 1:03:08. Training loss: 3.284096617417389. Learning Rate: 3.032037407037407e-05\n",
      "Batch 8,950 of 9,631 Elased 1:03:30. Training loss: 3.2835986439342606. Learning Rate: 3.0303030303030306e-05\n",
      "Batch 9,000 of 9,631 Elased 1:03:51. Training loss: 3.282563898086548. Learning Rate: 3.0285686535686535e-05\n",
      "Batch 9,050 of 9,631 Elased 1:04:12. Training loss: 3.2834755253660086. Learning Rate: 3.026834276834277e-05\n",
      "Batch 9,100 of 9,631 Elased 1:04:34. Training loss: 3.28326668376451. Learning Rate: 3.0250999000999004e-05\n",
      "Batch 9,150 of 9,631 Elased 1:04:55. Training loss: 3.2834777523389933. Learning Rate: 3.0233655233655233e-05\n",
      "Batch 9,200 of 9,631 Elased 1:05:16. Training loss: 3.284067299897256. Learning Rate: 3.021631146631147e-05\n",
      "Batch 9,250 of 9,631 Elased 1:05:37. Training loss: 3.2837621633039937. Learning Rate: 3.01989676989677e-05\n",
      "Batch 9,300 of 9,631 Elased 1:05:58. Training loss: 3.284243542776313. Learning Rate: 3.018162393162393e-05\n",
      "Batch 9,350 of 9,631 Elased 1:06:19. Training loss: 3.2841081264694743. Learning Rate: 3.0164280164280167e-05\n",
      "Batch 9,400 of 9,631 Elased 1:06:40. Training loss: 3.2841280067347465. Learning Rate: 3.01469363969364e-05\n",
      "Batch 9,450 of 9,631 Elased 1:07:02. Training loss: 3.284075524731288. Learning Rate: 3.012959262959263e-05\n",
      "Batch 9,500 of 9,631 Elased 1:07:23. Training loss: 3.283825149699261. Learning Rate: 3.0112248862248865e-05\n",
      "Batch 9,550 of 9,631 Elased 1:07:45. Training loss: 3.2832363352226337. Learning Rate: 3.0094905094905098e-05\n",
      "Batch 9,600 of 9,631 Elased 1:08:06. Training loss: 3.2837061067298055. Learning Rate: 3.0077561327561327e-05\n",
      "\n",
      "\n",
      "  Average training loss: 3.28\n",
      "  Training epcoh took: 1:08:20\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e587f7bbd5e4486c89d7b68b36d93d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=67.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Validation Loss: 3.44\n",
      "  Validation took: 0:00:43\n",
      "\n",
      "======== Epoch 22 / 30 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a77454cd0b4321921cc80218f5e2ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch    50 of 9,631 Elased 0:00:22. Training loss: 3.3528574323654174. Learning Rate: 3.0049464424464425e-05\n",
      "Batch   100 of 9,631 Elased 0:00:43. Training loss: 3.2941577935218813. Learning Rate: 3.003212065712066e-05\n",
      "Batch   150 of 9,631 Elased 0:01:04. Training loss: 3.3036844031016033. Learning Rate: 3.001477688977689e-05\n",
      "Batch   200 of 9,631 Elased 0:01:26. Training loss: 3.3283004140853882. Learning Rate: 2.9997433122433122e-05\n",
      "Batch   250 of 9,631 Elased 0:01:47. Training loss: 3.285742060661316. Learning Rate: 2.998008935508936e-05\n",
      "Batch   300 of 9,631 Elased 0:02:08. Training loss: 3.2715244913101196. Learning Rate: 2.9962745587745588e-05\n",
      "Batch   350 of 9,631 Elased 0:02:29. Training loss: 3.2732850660596573. Learning Rate: 2.994540182040182e-05\n",
      "Batch   400 of 9,631 Elased 0:02:50. Training loss: 3.2829938343167306. Learning Rate: 2.9928058053058056e-05\n",
      "Batch   450 of 9,631 Elased 0:03:11. Training loss: 3.289698684745365. Learning Rate: 2.9910714285714286e-05\n",
      "Batch   500 of 9,631 Elased 0:03:33. Training loss: 3.294865373849869. Learning Rate: 2.989337051837052e-05\n",
      "Batch   550 of 9,631 Elased 0:03:54. Training loss: 3.283133713765578. Learning Rate: 2.9876026751026754e-05\n",
      "Batch   600 of 9,631 Elased 0:04:15. Training loss: 3.2972411276896794. Learning Rate: 2.9858682983682984e-05\n",
      "Batch   650 of 9,631 Elased 0:04:36. Training loss: 3.2999166420789865. Learning Rate: 2.984133921633922e-05\n",
      "Batch   700 of 9,631 Elased 0:04:58. Training loss: 3.2928465289728983. Learning Rate: 2.9823995448995452e-05\n",
      "Batch   750 of 9,631 Elased 0:05:19. Training loss: 3.2790327258110046. Learning Rate: 2.980665168165168e-05\n",
      "Batch   800 of 9,631 Elased 0:05:40. Training loss: 3.2776709859073163. Learning Rate: 2.9789307914307918e-05\n",
      "Batch   850 of 9,631 Elased 0:06:02. Training loss: 3.271653775467592. Learning Rate: 2.9771964146964147e-05\n",
      "Batch   900 of 9,631 Elased 0:06:23. Training loss: 3.270813453859753. Learning Rate: 2.975462037962038e-05\n",
      "Batch   950 of 9,631 Elased 0:06:44. Training loss: 3.264709269749491. Learning Rate: 2.9737276612276616e-05\n",
      "Batch 1,000 of 9,631 Elased 0:07:05. Training loss: 3.2659731355905532. Learning Rate: 2.9719932844932845e-05\n",
      "Batch 1,050 of 9,631 Elased 0:07:27. Training loss: 3.2687420203572226. Learning Rate: 2.9702589077589078e-05\n",
      "Batch 1,100 of 9,631 Elased 0:07:48. Training loss: 3.2705236865173686. Learning Rate: 2.9685245310245314e-05\n",
      "Batch 1,150 of 9,631 Elased 0:08:09. Training loss: 3.2683148407936096. Learning Rate: 2.9667901542901543e-05\n",
      "Batch 1,200 of 9,631 Elased 0:08:31. Training loss: 3.2685733953118326. Learning Rate: 2.9650557775557776e-05\n",
      "Batch 1,250 of 9,631 Elased 0:08:52. Training loss: 3.270463318157196. Learning Rate: 2.963321400821401e-05\n",
      "Batch 1,300 of 9,631 Elased 0:09:14. Training loss: 3.2660693219074837. Learning Rate: 2.961587024087024e-05\n",
      "Batch 1,350 of 9,631 Elased 0:09:35. Training loss: 3.2651307474242315. Learning Rate: 2.9598526473526474e-05\n",
      "Batch 1,400 of 9,631 Elased 0:09:56. Training loss: 3.2638909301587513. Learning Rate: 2.958118270618271e-05\n",
      "Batch 1,450 of 9,631 Elased 0:10:18. Training loss: 3.2612480276206446. Learning Rate: 2.956383893883894e-05\n",
      "Batch 1,500 of 9,631 Elased 0:10:39. Training loss: 3.2599922285874685. Learning Rate: 2.9546495171495175e-05\n",
      "Batch 1,550 of 9,631 Elased 0:11:00. Training loss: 3.2594378110670275. Learning Rate: 2.9529151404151408e-05\n",
      "Batch 1,600 of 9,631 Elased 0:11:21. Training loss: 3.264827862009406. Learning Rate: 2.9511807636807637e-05\n",
      "Batch 1,650 of 9,631 Elased 0:11:42. Training loss: 3.267404035727183. Learning Rate: 2.9494463869463873e-05\n",
      "Batch 1,700 of 9,631 Elased 0:12:04. Training loss: 3.26445281736991. Learning Rate: 2.9477120102120102e-05\n",
      "Batch 1,750 of 9,631 Elased 0:12:25. Training loss: 3.2645839743614196. Learning Rate: 2.9459776334776335e-05\n",
      "Batch 1,800 of 9,631 Elased 0:12:47. Training loss: 3.26352476047145. Learning Rate: 2.944243256743257e-05\n",
      "Batch 1,850 of 9,631 Elased 0:13:08. Training loss: 3.2663949075260676. Learning Rate: 2.94250888000888e-05\n",
      "Batch 1,900 of 9,631 Elased 0:13:29. Training loss: 3.2658980829464763. Learning Rate: 2.9407745032745033e-05\n",
      "Batch 1,950 of 9,631 Elased 0:13:50. Training loss: 3.2626210204760233. Learning Rate: 2.939040126540127e-05\n",
      "Batch 2,000 of 9,631 Elased 0:14:12. Training loss: 3.2588764730095865. Learning Rate: 2.9373057498057498e-05\n",
      "Batch 2,050 of 9,631 Elased 0:14:33. Training loss: 3.260402168006432. Learning Rate: 2.935571373071373e-05\n",
      "Batch 2,100 of 9,631 Elased 0:14:54. Training loss: 3.264174227090109. Learning Rate: 2.9338369963369967e-05\n",
      "Batch 2,150 of 9,631 Elased 0:15:15. Training loss: 3.2637291287821393. Learning Rate: 2.9321026196026196e-05\n",
      "Batch 2,200 of 9,631 Elased 0:15:37. Training loss: 3.2635669095949695. Learning Rate: 2.930368242868243e-05\n",
      "Batch 2,250 of 9,631 Elased 0:15:58. Training loss: 3.261337152904934. Learning Rate: 2.9286338661338665e-05\n",
      "Batch 2,300 of 9,631 Elased 0:16:19. Training loss: 3.2622929970077847. Learning Rate: 2.9268994893994894e-05\n",
      "Batch 2,350 of 9,631 Elased 0:16:41. Training loss: 3.260243705739366. Learning Rate: 2.925165112665113e-05\n",
      "Batch 2,400 of 9,631 Elased 0:17:02. Training loss: 3.2596314152578514. Learning Rate: 2.9234307359307363e-05\n",
      "Batch 2,450 of 9,631 Elased 0:17:23. Training loss: 3.256855609611589. Learning Rate: 2.9216963591963592e-05\n",
      "Batch 2,500 of 9,631 Elased 0:17:44. Training loss: 3.2555202518939974. Learning Rate: 2.9199619824619828e-05\n",
      "Batch 2,550 of 9,631 Elased 0:18:06. Training loss: 3.2605875349512288. Learning Rate: 2.9182276057276057e-05\n",
      "Batch 2,600 of 9,631 Elased 0:18:27. Training loss: 3.2632365381259185. Learning Rate: 2.916493228993229e-05\n",
      "Batch 2,650 of 9,631 Elased 0:18:48. Training loss: 3.265875514993128. Learning Rate: 2.9147588522588526e-05\n",
      "Batch 2,700 of 9,631 Elased 0:19:09. Training loss: 3.267057624083978. Learning Rate: 2.9130244755244755e-05\n",
      "Batch 2,750 of 9,631 Elased 0:19:30. Training loss: 3.2642435018799523. Learning Rate: 2.9112900987900988e-05\n",
      "Batch 2,800 of 9,631 Elased 0:19:52. Training loss: 3.2648715334704943. Learning Rate: 2.9095557220557224e-05\n",
      "Batch 2,850 of 9,631 Elased 0:20:13. Training loss: 3.2672253038590413. Learning Rate: 2.9078213453213453e-05\n",
      "Batch 2,900 of 9,631 Elased 0:20:34. Training loss: 3.267332810410138. Learning Rate: 2.9060869685869686e-05\n",
      "Batch 2,950 of 9,631 Elased 0:20:55. Training loss: 3.267255176406796. Learning Rate: 2.9043525918525922e-05\n",
      "Batch 3,000 of 9,631 Elased 0:21:17. Training loss: 3.270019084175428. Learning Rate: 2.902618215118215e-05\n",
      "Batch 3,050 of 9,631 Elased 0:21:38. Training loss: 3.2697283152674066. Learning Rate: 2.9008838383838384e-05\n",
      "Batch 3,100 of 9,631 Elased 0:21:59. Training loss: 3.2703421490038593. Learning Rate: 2.899149461649462e-05\n",
      "Batch 3,150 of 9,631 Elased 0:22:20. Training loss: 3.266749751681373. Learning Rate: 2.897415084915085e-05\n",
      "Batch 3,200 of 9,631 Elased 0:22:42. Training loss: 3.2690299893915653. Learning Rate: 2.8956807081807085e-05\n",
      "Batch 3,250 of 9,631 Elased 0:23:03. Training loss: 3.268925398533161. Learning Rate: 2.8939463314463318e-05\n",
      "Batch 3,300 of 9,631 Elased 0:23:24. Training loss: 3.2720699617356965. Learning Rate: 2.8922119547119547e-05\n",
      "Batch 3,350 of 9,631 Elased 0:23:45. Training loss: 3.2726639865761373. Learning Rate: 2.8904775779775783e-05\n",
      "Batch 3,400 of 9,631 Elased 0:24:06. Training loss: 3.2730854285113953. Learning Rate: 2.8887432012432012e-05\n",
      "Batch 3,450 of 9,631 Elased 0:24:27. Training loss: 3.2738207928685172. Learning Rate: 2.8870088245088245e-05\n",
      "Batch 3,500 of 9,631 Elased 0:24:48. Training loss: 3.273177510363715. Learning Rate: 2.885274447774448e-05\n",
      "Batch 3,550 of 9,631 Elased 0:25:10. Training loss: 3.274535701879313. Learning Rate: 2.883540071040071e-05\n",
      "Batch 3,600 of 9,631 Elased 0:25:31. Training loss: 3.2763846960994933. Learning Rate: 2.8818056943056943e-05\n",
      "Batch 3,650 of 9,631 Elased 0:25:52. Training loss: 3.2798109566675473. Learning Rate: 2.880071317571318e-05\n",
      "Batch 3,700 of 9,631 Elased 0:26:14. Training loss: 3.2784480187699603. Learning Rate: 2.878336940836941e-05\n",
      "Batch 3,750 of 9,631 Elased 0:26:35. Training loss: 3.2769400595347085. Learning Rate: 2.876602564102564e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3,800 of 9,631 Elased 0:26:56. Training loss: 3.2757543125278072. Learning Rate: 2.8748681873681877e-05\n",
      "Batch 3,850 of 9,631 Elased 0:27:17. Training loss: 3.2732106488091604. Learning Rate: 2.8731338106338106e-05\n",
      "Batch 3,900 of 9,631 Elased 0:27:39. Training loss: 3.271906531407283. Learning Rate: 2.871399433899434e-05\n",
      "Batch 3,950 of 9,631 Elased 0:28:00. Training loss: 3.272763466412508. Learning Rate: 2.8696650571650575e-05\n",
      "Batch 4,000 of 9,631 Elased 0:28:21. Training loss: 3.273128686130047. Learning Rate: 2.8679306804306804e-05\n",
      "Batch 4,050 of 9,631 Elased 0:28:42. Training loss: 3.272384021135024. Learning Rate: 2.866196303696304e-05\n",
      "Batch 4,100 of 9,631 Elased 0:29:04. Training loss: 3.271358841628563. Learning Rate: 2.8644619269619273e-05\n",
      "Batch 4,150 of 9,631 Elased 0:29:25. Training loss: 3.270952025781195. Learning Rate: 2.8627275502275502e-05\n",
      "Batch 4,200 of 9,631 Elased 0:29:46. Training loss: 3.271088030338287. Learning Rate: 2.860993173493174e-05\n",
      "Batch 4,250 of 9,631 Elased 0:30:08. Training loss: 3.2714599845269148. Learning Rate: 2.8592587967587968e-05\n",
      "Batch 4,300 of 9,631 Elased 0:30:29. Training loss: 3.2708399320480437. Learning Rate: 2.85752442002442e-05\n",
      "Batch 4,350 of 9,631 Elased 0:30:50. Training loss: 3.2705287164655226. Learning Rate: 2.8557900432900436e-05\n",
      "Batch 4,400 of 9,631 Elased 0:31:12. Training loss: 3.271659033108841. Learning Rate: 2.8540556665556666e-05\n",
      "Batch 4,450 of 9,631 Elased 0:31:33. Training loss: 3.272030561613233. Learning Rate: 2.8523212898212898e-05\n",
      "Batch 4,500 of 9,631 Elased 0:31:54. Training loss: 3.2730724279350705. Learning Rate: 2.8505869130869134e-05\n",
      "Batch 4,550 of 9,631 Elased 0:32:15. Training loss: 3.271344115262503. Learning Rate: 2.8488525363525364e-05\n",
      "Batch 4,600 of 9,631 Elased 0:32:36. Training loss: 3.2725739983112914. Learning Rate: 2.8471181596181596e-05\n",
      "Batch 4,650 of 9,631 Elased 0:32:58. Training loss: 3.2719955194380974. Learning Rate: 2.8453837828837832e-05\n",
      "Batch 4,700 of 9,631 Elased 0:33:19. Training loss: 3.2723083317279817. Learning Rate: 2.843649406149406e-05\n",
      "Batch 4,750 of 9,631 Elased 0:33:40. Training loss: 3.2725894437338177. Learning Rate: 2.8419150294150294e-05\n",
      "Batch 4,800 of 9,631 Elased 0:34:02. Training loss: 3.2710614891598624. Learning Rate: 2.840180652680653e-05\n",
      "Batch 4,850 of 9,631 Elased 0:34:23. Training loss: 3.270340746776345. Learning Rate: 2.838446275946276e-05\n",
      "Batch 4,900 of 9,631 Elased 0:34:44. Training loss: 3.271001964272285. Learning Rate: 2.8367118992118996e-05\n",
      "Batch 4,950 of 9,631 Elased 0:35:05. Training loss: 3.2713181434255656. Learning Rate: 2.8349775224775228e-05\n",
      "Batch 5,000 of 9,631 Elased 0:35:26. Training loss: 3.2724317895650863. Learning Rate: 2.8332431457431457e-05\n",
      "Batch 5,050 of 9,631 Elased 0:35:47. Training loss: 3.272072424959428. Learning Rate: 2.8315087690087694e-05\n",
      "Batch 5,100 of 9,631 Elased 0:36:08. Training loss: 3.2703321438910913. Learning Rate: 2.8297743922743926e-05\n",
      "Batch 5,150 of 9,631 Elased 0:36:30. Training loss: 3.270674157235229. Learning Rate: 2.8280400155400155e-05\n",
      "Batch 5,200 of 9,631 Elased 0:36:51. Training loss: 3.270080376313283. Learning Rate: 2.826305638805639e-05\n",
      "Batch 5,250 of 9,631 Elased 0:37:12. Training loss: 3.2701300150553387. Learning Rate: 2.824571262071262e-05\n",
      "Batch 5,300 of 9,631 Elased 0:37:33. Training loss: 3.2700039789361774. Learning Rate: 2.8228368853368853e-05\n",
      "Batch 5,350 of 9,631 Elased 0:37:54. Training loss: 3.2717473141946525. Learning Rate: 2.821102508602509e-05\n",
      "Batch 5,400 of 9,631 Elased 0:38:15. Training loss: 3.2718394373522863. Learning Rate: 2.819368131868132e-05\n",
      "Batch 5,450 of 9,631 Elased 0:38:37. Training loss: 3.271832684201932. Learning Rate: 2.817633755133755e-05\n",
      "Batch 5,500 of 9,631 Elased 0:38:58. Training loss: 3.2721567383679475. Learning Rate: 2.8158993783993787e-05\n",
      "Batch 5,550 of 9,631 Elased 0:39:19. Training loss: 3.272547180588181. Learning Rate: 2.8141650016650017e-05\n",
      "Batch 5,600 of 9,631 Elased 0:39:40. Training loss: 3.27187351328986. Learning Rate: 2.812430624930625e-05\n",
      "Batch 5,650 of 9,631 Elased 0:40:02. Training loss: 3.2735607539658. Learning Rate: 2.8106962481962485e-05\n",
      "Batch 5,700 of 9,631 Elased 0:40:23. Training loss: 3.2735237663252312. Learning Rate: 2.8089618714618715e-05\n",
      "Batch 5,750 of 9,631 Elased 0:40:44. Training loss: 3.2733234054731284. Learning Rate: 2.807227494727495e-05\n",
      "Batch 5,800 of 9,631 Elased 0:41:05. Training loss: 3.2733156056856285. Learning Rate: 2.8054931179931183e-05\n",
      "Batch 5,850 of 9,631 Elased 0:41:26. Training loss: 3.2739587142936184. Learning Rate: 2.8037587412587413e-05\n",
      "Batch 5,900 of 9,631 Elased 0:41:48. Training loss: 3.273884299084292. Learning Rate: 2.802024364524365e-05\n",
      "Batch 5,950 of 9,631 Elased 0:42:09. Training loss: 3.274335501855161. Learning Rate: 2.800289987789988e-05\n",
      "Batch 6,000 of 9,631 Elased 0:42:30. Training loss: 3.2742396832704546. Learning Rate: 2.798555611055611e-05\n",
      "Batch 6,050 of 9,631 Elased 0:42:52. Training loss: 3.274907481217187. Learning Rate: 2.7968212343212347e-05\n",
      "Batch 6,100 of 9,631 Elased 0:43:13. Training loss: 3.2753958311628124. Learning Rate: 2.7950868575868576e-05\n",
      "Batch 6,150 of 9,631 Elased 0:43:34. Training loss: 3.2750791774920334. Learning Rate: 2.793352480852481e-05\n",
      "Batch 6,200 of 9,631 Elased 0:43:55. Training loss: 3.2731786009765442. Learning Rate: 2.7916181041181045e-05\n",
      "Batch 6,250 of 9,631 Elased 0:44:17. Training loss: 3.272801079978943. Learning Rate: 2.7898837273837274e-05\n",
      "Batch 6,300 of 9,631 Elased 0:44:38. Training loss: 3.272638054972603. Learning Rate: 2.7881493506493507e-05\n",
      "Batch 6,350 of 9,631 Elased 0:44:59. Training loss: 3.2724335382303855. Learning Rate: 2.7864149739149743e-05\n",
      "Batch 6,400 of 9,631 Elased 0:45:20. Training loss: 3.272157831955701. Learning Rate: 2.7846805971805972e-05\n",
      "Batch 6,450 of 9,631 Elased 0:45:42. Training loss: 3.272910392838855. Learning Rate: 2.7829462204462205e-05\n",
      "Batch 6,500 of 9,631 Elased 0:46:03. Training loss: 3.2713548783522386. Learning Rate: 2.781211843711844e-05\n",
      "Batch 6,550 of 9,631 Elased 0:46:24. Training loss: 3.271779921910235. Learning Rate: 2.779477466977467e-05\n",
      "Batch 6,600 of 9,631 Elased 0:46:45. Training loss: 3.2722143868244054. Learning Rate: 2.7777430902430906e-05\n",
      "Batch 6,650 of 9,631 Elased 0:47:07. Training loss: 3.272615117668209. Learning Rate: 2.776008713508714e-05\n",
      "Batch 6,700 of 9,631 Elased 0:47:28. Training loss: 3.2737157647645296. Learning Rate: 2.7742743367743368e-05\n",
      "Batch 6,750 of 9,631 Elased 0:47:49. Training loss: 3.273808061917623. Learning Rate: 2.7725399600399604e-05\n",
      "Batch 6,800 of 9,631 Elased 0:48:10. Training loss: 3.273604993154021. Learning Rate: 2.7708055833055836e-05\n",
      "Batch 6,850 of 9,631 Elased 0:48:32. Training loss: 3.2729768477043097. Learning Rate: 2.7690712065712066e-05\n",
      "Batch 6,900 of 9,631 Elased 0:48:53. Training loss: 3.2725621113224306. Learning Rate: 2.7673368298368302e-05\n",
      "Batch 6,950 of 9,631 Elased 0:49:14. Training loss: 3.2724910815328143. Learning Rate: 2.765602453102453e-05\n",
      "Batch 7,000 of 9,631 Elased 0:49:35. Training loss: 3.2718344857011523. Learning Rate: 2.7638680763680764e-05\n",
      "Batch 7,050 of 9,631 Elased 0:49:56. Training loss: 3.271019295766844. Learning Rate: 2.7621336996337e-05\n",
      "Batch 7,100 of 9,631 Elased 0:50:17. Training loss: 3.271041329763305. Learning Rate: 2.760399322899323e-05\n",
      "Batch 7,150 of 9,631 Elased 0:50:39. Training loss: 3.2709119250724368. Learning Rate: 2.758664946164946e-05\n",
      "Batch 7,200 of 9,631 Elased 0:51:00. Training loss: 3.271080235193173. Learning Rate: 2.7569305694305698e-05\n",
      "Batch 7,250 of 9,631 Elased 0:51:21. Training loss: 3.270108927710303. Learning Rate: 2.7551961926961927e-05\n",
      "Batch 7,300 of 9,631 Elased 0:51:43. Training loss: 3.27064685725186. Learning Rate: 2.753461815961816e-05\n",
      "Batch 7,350 of 9,631 Elased 0:52:04. Training loss: 3.2702232710033856. Learning Rate: 2.7517274392274396e-05\n",
      "Batch 7,400 of 9,631 Elased 0:52:26. Training loss: 3.268830216204798. Learning Rate: 2.7499930624930625e-05\n",
      "Batch 7,450 of 9,631 Elased 0:52:47. Training loss: 3.2686553582249074. Learning Rate: 2.7482586857586858e-05\n",
      "Batch 7,500 of 9,631 Elased 0:53:09. Training loss: 3.267748295402527. Learning Rate: 2.7465243090243094e-05\n",
      "Batch 7,550 of 9,631 Elased 0:53:30. Training loss: 3.267729878678227. Learning Rate: 2.7447899322899323e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7,600 of 9,631 Elased 0:53:51. Training loss: 3.2680704412021133. Learning Rate: 2.743055555555556e-05\n",
      "Batch 7,650 of 9,631 Elased 0:54:12. Training loss: 3.2682801725506003. Learning Rate: 2.741321178821179e-05\n",
      "Batch 7,700 of 9,631 Elased 0:54:33. Training loss: 3.267225925690168. Learning Rate: 2.739586802086802e-05\n",
      "Batch 7,750 of 9,631 Elased 0:54:54. Training loss: 3.265737507558638. Learning Rate: 2.7378524253524257e-05\n",
      "Batch 7,800 of 9,631 Elased 0:55:15. Training loss: 3.2661159552824803. Learning Rate: 2.7361180486180486e-05\n",
      "Batch 7,850 of 9,631 Elased 0:55:36. Training loss: 3.266489635409823. Learning Rate: 2.734383671883672e-05\n",
      "Batch 7,900 of 9,631 Elased 0:55:58. Training loss: 3.2652126367635366. Learning Rate: 2.7326492951492955e-05\n",
      "Batch 7,950 of 9,631 Elased 0:56:19. Training loss: 3.2666518288888273. Learning Rate: 2.7309149184149184e-05\n",
      "Batch 8,000 of 9,631 Elased 0:56:40. Training loss: 3.2663841782063248. Learning Rate: 2.7291805416805417e-05\n",
      "Batch 8,050 of 9,631 Elased 0:57:02. Training loss: 3.267460419539339. Learning Rate: 2.7274461649461653e-05\n",
      "Batch 8,100 of 9,631 Elased 0:57:23. Training loss: 3.2675754584353647. Learning Rate: 2.7257117882117882e-05\n",
      "Batch 8,150 of 9,631 Elased 0:57:44. Training loss: 3.268260799814587. Learning Rate: 2.7239774114774115e-05\n",
      "Batch 8,200 of 9,631 Elased 0:58:05. Training loss: 3.267853406682247. Learning Rate: 2.722243034743035e-05\n",
      "Batch 8,250 of 9,631 Elased 0:58:26. Training loss: 3.268453436316866. Learning Rate: 2.720508658008658e-05\n",
      "Batch 8,300 of 9,631 Elased 0:58:48. Training loss: 3.2685197279108578. Learning Rate: 2.7187742812742813e-05\n",
      "Batch 8,350 of 9,631 Elased 0:59:09. Training loss: 3.2683042146345813. Learning Rate: 2.717039904539905e-05\n",
      "Batch 8,400 of 9,631 Elased 0:59:30. Training loss: 3.2674489369420776. Learning Rate: 2.7153055278055278e-05\n",
      "Batch 8,450 of 9,631 Elased 0:59:51. Training loss: 3.2669081313088095. Learning Rate: 2.7135711510711514e-05\n",
      "Batch 8,500 of 9,631 Elased 1:00:13. Training loss: 3.2666639868091134. Learning Rate: 2.7118367743367747e-05\n",
      "Batch 8,550 of 9,631 Elased 1:00:34. Training loss: 3.2661574778501055. Learning Rate: 2.7101023976023976e-05\n",
      "Batch 8,600 of 9,631 Elased 1:00:55. Training loss: 3.2668769930545674. Learning Rate: 2.7083680208680212e-05\n",
      "Batch 8,650 of 9,631 Elased 1:01:16. Training loss: 3.26582353195014. Learning Rate: 2.706633644133644e-05\n",
      "Batch 8,700 of 9,631 Elased 1:01:38. Training loss: 3.2656200431681226. Learning Rate: 2.7048992673992674e-05\n",
      "Batch 8,750 of 9,631 Elased 1:01:59. Training loss: 3.265432462814876. Learning Rate: 2.703164890664891e-05\n",
      "Batch 8,800 of 9,631 Elased 1:02:20. Training loss: 3.265494157875126. Learning Rate: 2.701430513930514e-05\n",
      "Batch 8,850 of 9,631 Elased 1:02:42. Training loss: 3.265115464541872. Learning Rate: 2.6996961371961372e-05\n",
      "Batch 8,900 of 9,631 Elased 1:03:03. Training loss: 3.265407584048389. Learning Rate: 2.6979617604617608e-05\n",
      "Batch 8,950 of 9,631 Elased 1:03:25. Training loss: 3.265157505573507. Learning Rate: 2.6962273837273837e-05\n",
      "Batch 9,000 of 9,631 Elased 1:03:46. Training loss: 3.264230951918496. Learning Rate: 2.694493006993007e-05\n",
      "Batch 9,050 of 9,631 Elased 1:04:08. Training loss: 3.2652071290253275. Learning Rate: 2.6927586302586306e-05\n",
      "Batch 9,100 of 9,631 Elased 1:04:29. Training loss: 3.2648866857670167. Learning Rate: 2.6910242535242535e-05\n",
      "Batch 9,150 of 9,631 Elased 1:04:51. Training loss: 3.2650200079959597. Learning Rate: 2.6892898767898768e-05\n",
      "Batch 9,200 of 9,631 Elased 1:05:13. Training loss: 3.265679574492185. Learning Rate: 2.6875555000555004e-05\n",
      "Batch 9,250 of 9,631 Elased 1:05:35. Training loss: 3.2655767860799223. Learning Rate: 2.6858211233211233e-05\n",
      "Batch 9,300 of 9,631 Elased 1:05:57. Training loss: 3.2659872103891066. Learning Rate: 2.684086746586747e-05\n",
      "Batch 9,350 of 9,631 Elased 1:06:19. Training loss: 3.266009628313748. Learning Rate: 2.6823523698523702e-05\n",
      "Batch 9,400 of 9,631 Elased 1:06:41. Training loss: 3.2658517758262917. Learning Rate: 2.680617993117993e-05\n",
      "Batch 9,450 of 9,631 Elased 1:07:02. Training loss: 3.265659216308089. Learning Rate: 2.6788836163836167e-05\n",
      "Batch 9,500 of 9,631 Elased 1:07:23. Training loss: 3.2653310711132852. Learning Rate: 2.6771492396492397e-05\n",
      "Batch 9,550 of 9,631 Elased 1:07:45. Training loss: 3.264691971846276. Learning Rate: 2.675414862914863e-05\n",
      "Batch 9,600 of 9,631 Elased 1:08:06. Training loss: 3.265071988639732. Learning Rate: 2.6736804861804865e-05\n",
      "\n",
      "\n",
      "  Average training loss: 3.26\n",
      "  Training epcoh took: 1:08:19\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a508dad00343c692fa0d7d785f8ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=67.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Validation Loss: 3.43\n",
      "  Validation took: 0:00:43\n",
      "\n",
      "======== Epoch 23 / 30 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f5607e8ddc40849ea718c2082de0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch    50 of 9,631 Elased 0:00:21. Training loss: 3.3618515968322753. Learning Rate: 2.6708707958707963e-05\n",
      "Batch   100 of 9,631 Elased 0:00:43. Training loss: 3.3083610582351684. Learning Rate: 2.6691364191364192e-05\n",
      "Batch   150 of 9,631 Elased 0:01:04. Training loss: 3.3096572160720825. Learning Rate: 2.6674020424020425e-05\n",
      "Batch   200 of 9,631 Elased 0:01:25. Training loss: 3.3353985357284546. Learning Rate: 2.665667665667666e-05\n",
      "Batch   250 of 9,631 Elased 0:01:46. Training loss: 3.2887386083602905. Learning Rate: 2.663933288933289e-05\n",
      "Batch   300 of 9,631 Elased 0:02:07. Training loss: 3.2745476158459983. Learning Rate: 2.6621989121989123e-05\n",
      "Batch   350 of 9,631 Elased 0:02:28. Training loss: 3.268012693268912. Learning Rate: 2.660464535464536e-05\n",
      "Batch   400 of 9,631 Elased 0:02:50. Training loss: 3.2763263887166976. Learning Rate: 2.6587301587301588e-05\n",
      "Batch   450 of 9,631 Elased 0:03:11. Training loss: 3.2786345434188844. Learning Rate: 2.656995781995782e-05\n",
      "Batch   500 of 9,631 Elased 0:03:32. Training loss: 3.2821601634025575. Learning Rate: 2.6552614052614057e-05\n",
      "Batch   550 of 9,631 Elased 0:03:53. Training loss: 3.2667990853569724. Learning Rate: 2.6535270285270286e-05\n",
      "Batch   600 of 9,631 Elased 0:04:15. Training loss: 3.281962419350942. Learning Rate: 2.6517926517926515e-05\n",
      "Batch   650 of 9,631 Elased 0:04:36. Training loss: 3.28236666055826. Learning Rate: 2.6500582750582755e-05\n",
      "Batch   700 of 9,631 Elased 0:04:57. Training loss: 3.2749911379814147. Learning Rate: 2.6483238983238984e-05\n",
      "Batch   750 of 9,631 Elased 0:05:18. Training loss: 3.2619576463699342. Learning Rate: 2.6465895215895213e-05\n",
      "Batch   800 of 9,631 Elased 0:05:40. Training loss: 3.259282768517733. Learning Rate: 2.644855144855145e-05\n",
      "Batch   850 of 9,631 Elased 0:06:01. Training loss: 3.2521762321977055. Learning Rate: 2.6431207681207682e-05\n",
      "Batch   900 of 9,631 Elased 0:06:22. Training loss: 3.2497394409444595. Learning Rate: 2.6413863913863918e-05\n",
      "Batch   950 of 9,631 Elased 0:06:43. Training loss: 3.2461754057281897. Learning Rate: 2.6396520146520147e-05\n",
      "Batch 1,000 of 9,631 Elased 0:07:05. Training loss: 3.247318019747734. Learning Rate: 2.637917637917638e-05\n",
      "Batch 1,050 of 9,631 Elased 0:07:26. Training loss: 3.248891405491602. Learning Rate: 2.6361832611832616e-05\n",
      "Batch 1,100 of 9,631 Elased 0:07:47. Training loss: 3.251182478016073. Learning Rate: 2.6344488844488845e-05\n",
      "Batch 1,150 of 9,631 Elased 0:08:08. Training loss: 3.250639873276586. Learning Rate: 2.6327145077145078e-05\n",
      "Batch 1,200 of 9,631 Elased 0:08:29. Training loss: 3.2534410182634987. Learning Rate: 2.6309801309801314e-05\n",
      "Batch 1,250 of 9,631 Elased 0:08:50. Training loss: 3.2551067491531374. Learning Rate: 2.6292457542457543e-05\n",
      "Batch 1,300 of 9,631 Elased 0:09:11. Training loss: 3.250030884009141. Learning Rate: 2.6275113775113776e-05\n",
      "Batch 1,350 of 9,631 Elased 0:09:33. Training loss: 3.250940939232155. Learning Rate: 2.6257770007770012e-05\n",
      "Batch 1,400 of 9,631 Elased 0:09:54. Training loss: 3.250104333247457. Learning Rate: 2.624042624042624e-05\n",
      "Batch 1,450 of 9,631 Elased 0:10:15. Training loss: 3.2484243150415093. Learning Rate: 2.622308247308247e-05\n",
      "Batch 1,500 of 9,631 Elased 0:10:36. Training loss: 3.2471648779710134. Learning Rate: 2.620573870573871e-05\n",
      "Batch 1,550 of 9,631 Elased 0:10:58. Training loss: 3.246733477884723. Learning Rate: 2.618839493839494e-05\n",
      "Batch 1,600 of 9,631 Elased 0:11:19. Training loss: 3.252821378558874. Learning Rate: 2.617105117105117e-05\n",
      "Batch 1,650 of 9,631 Elased 0:11:40. Training loss: 3.256092782020569. Learning Rate: 2.6153707403707404e-05\n",
      "Batch 1,700 of 9,631 Elased 0:12:02. Training loss: 3.253409124472562. Learning Rate: 2.6136363636363637e-05\n",
      "Batch 1,750 of 9,631 Elased 0:12:23. Training loss: 3.2536164075306484. Learning Rate: 2.6119019869019873e-05\n",
      "Batch 1,800 of 9,631 Elased 0:12:44. Training loss: 3.2524376663896772. Learning Rate: 2.6101676101676102e-05\n",
      "Batch 1,850 of 9,631 Elased 0:13:05. Training loss: 3.255086349925479. Learning Rate: 2.6084332334332335e-05\n",
      "Batch 1,900 of 9,631 Elased 0:13:27. Training loss: 3.2541459385972273. Learning Rate: 2.606698856698857e-05\n",
      "Batch 1,950 of 9,631 Elased 0:13:48. Training loss: 3.2504708375686255. Learning Rate: 2.60496447996448e-05\n",
      "Batch 2,000 of 9,631 Elased 0:14:09. Training loss: 3.2471311919689176. Learning Rate: 2.6032301032301033e-05\n",
      "Batch 2,050 of 9,631 Elased 0:14:31. Training loss: 3.248318325019464. Learning Rate: 2.601495726495727e-05\n",
      "Batch 2,100 of 9,631 Elased 0:14:52. Training loss: 3.2513881701514835. Learning Rate: 2.59976134976135e-05\n",
      "Batch 2,150 of 9,631 Elased 0:15:13. Training loss: 3.2511925219380577. Learning Rate: 2.598026973026973e-05\n",
      "Batch 2,200 of 9,631 Elased 0:15:34. Training loss: 3.2514754903316496. Learning Rate: 2.5962925962925967e-05\n",
      "Batch 2,250 of 9,631 Elased 0:15:56. Training loss: 3.248546058124966. Learning Rate: 2.5945582195582196e-05\n",
      "Batch 2,300 of 9,631 Elased 0:16:17. Training loss: 3.2500581496694814. Learning Rate: 2.5928238428238426e-05\n",
      "Batch 2,350 of 9,631 Elased 0:16:38. Training loss: 3.247822378949916. Learning Rate: 2.5910894660894665e-05\n",
      "Batch 2,400 of 9,631 Elased 0:16:59. Training loss: 3.2473559840520223. Learning Rate: 2.5893550893550894e-05\n",
      "Batch 2,450 of 9,631 Elased 0:17:21. Training loss: 3.24418221687784. Learning Rate: 2.5876207126207124e-05\n",
      "Batch 2,500 of 9,631 Elased 0:17:42. Training loss: 3.2431146734237672. Learning Rate: 2.585886335886336e-05\n",
      "Batch 2,550 of 9,631 Elased 0:18:04. Training loss: 3.2471835432800593. Learning Rate: 2.5841519591519592e-05\n",
      "Batch 2,600 of 9,631 Elased 0:18:25. Training loss: 3.2508495833323554. Learning Rate: 2.582417582417583e-05\n",
      "Batch 2,650 of 9,631 Elased 0:18:46. Training loss: 3.253395921239313. Learning Rate: 2.5806832056832058e-05\n",
      "Batch 2,700 of 9,631 Elased 0:19:08. Training loss: 3.2557217815187243. Learning Rate: 2.578948828948829e-05\n",
      "Batch 2,750 of 9,631 Elased 0:19:29. Training loss: 3.2531609468026597. Learning Rate: 2.5772144522144526e-05\n",
      "Batch 2,800 of 9,631 Elased 0:19:51. Training loss: 3.2543925125684057. Learning Rate: 2.5754800754800756e-05\n",
      "Batch 2,850 of 9,631 Elased 0:20:12. Training loss: 3.2580320272529333. Learning Rate: 2.5737456987456988e-05\n",
      "Batch 2,900 of 9,631 Elased 0:20:32. Training loss: 3.2587245041337507. Learning Rate: 2.5720113220113224e-05\n",
      "Batch 2,950 of 9,631 Elased 0:20:54. Training loss: 3.25872007438692. Learning Rate: 2.5702769452769454e-05\n",
      "Batch 3,000 of 9,631 Elased 0:21:15. Training loss: 3.2609397059679033. Learning Rate: 2.5685425685425686e-05\n",
      "Batch 3,050 of 9,631 Elased 0:21:36. Training loss: 3.259941346645355. Learning Rate: 2.5668081918081922e-05\n",
      "Batch 3,100 of 9,631 Elased 0:21:58. Training loss: 3.260223727264712. Learning Rate: 2.565073815073815e-05\n",
      "Batch 3,150 of 9,631 Elased 0:22:19. Training loss: 3.256393977838849. Learning Rate: 2.563339438339438e-05\n",
      "Batch 3,200 of 9,631 Elased 0:22:40. Training loss: 3.2588095005229114. Learning Rate: 2.561605061605062e-05\n",
      "Batch 3,250 of 9,631 Elased 0:23:02. Training loss: 3.258704417485457. Learning Rate: 2.559870684870685e-05\n",
      "Batch 3,300 of 9,631 Elased 0:23:23. Training loss: 3.2610204910148273. Learning Rate: 2.558136308136308e-05\n",
      "Batch 3,350 of 9,631 Elased 0:23:44. Training loss: 3.2612257356430168. Learning Rate: 2.5564019314019315e-05\n",
      "Batch 3,400 of 9,631 Elased 0:24:06. Training loss: 3.2610441579187617. Learning Rate: 2.5546675546675547e-05\n",
      "Batch 3,450 of 9,631 Elased 0:24:27. Training loss: 3.261253383366958. Learning Rate: 2.5529331779331784e-05\n",
      "Batch 3,500 of 9,631 Elased 0:24:48. Training loss: 3.260893650872367. Learning Rate: 2.5511988011988013e-05\n",
      "Batch 3,550 of 9,631 Elased 0:25:09. Training loss: 3.2622687848856753. Learning Rate: 2.5494644244644245e-05\n",
      "Batch 3,600 of 9,631 Elased 0:25:30. Training loss: 3.2638898238870833. Learning Rate: 2.547730047730048e-05\n",
      "Batch 3,650 of 9,631 Elased 0:25:52. Training loss: 3.2671801862324754. Learning Rate: 2.545995670995671e-05\n",
      "Batch 3,700 of 9,631 Elased 0:26:14. Training loss: 3.2664314933725307. Learning Rate: 2.5442612942612943e-05\n",
      "Batch 3,750 of 9,631 Elased 0:26:35. Training loss: 3.2650625319798787. Learning Rate: 2.542526917526918e-05\n",
      "Batch 3,800 of 9,631 Elased 0:26:56. Training loss: 3.2637459454410953. Learning Rate: 2.540792540792541e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3,850 of 9,631 Elased 0:27:17. Training loss: 3.2614639074152167. Learning Rate: 2.539058164058164e-05\n",
      "Batch 3,900 of 9,631 Elased 0:27:39. Training loss: 3.260320024123559. Learning Rate: 2.5373237873237877e-05\n",
      "Batch 3,950 of 9,631 Elased 0:28:00. Training loss: 3.26134359933153. Learning Rate: 2.5355894105894107e-05\n",
      "Batch 4,000 of 9,631 Elased 0:28:21. Training loss: 3.2618575400710106. Learning Rate: 2.5338550338550336e-05\n",
      "Batch 4,050 of 9,631 Elased 0:28:42. Training loss: 3.261520241690271. Learning Rate: 2.5321206571206575e-05\n",
      "Batch 4,100 of 9,631 Elased 0:29:04. Training loss: 3.2609666180610657. Learning Rate: 2.5303862803862805e-05\n",
      "Batch 4,150 of 9,631 Elased 0:29:25. Training loss: 3.260364394130477. Learning Rate: 2.5286519036519034e-05\n",
      "Batch 4,200 of 9,631 Elased 0:29:46. Training loss: 3.2602971027578627. Learning Rate: 2.526917526917527e-05\n",
      "Batch 4,250 of 9,631 Elased 0:30:07. Training loss: 3.2606062141867245. Learning Rate: 2.5251831501831503e-05\n",
      "Batch 4,300 of 9,631 Elased 0:30:28. Training loss: 3.26053402462671. Learning Rate: 2.523448773448774e-05\n",
      "Batch 4,350 of 9,631 Elased 0:30:49. Training loss: 3.260104604260675. Learning Rate: 2.5217143967143968e-05\n",
      "Batch 4,400 of 9,631 Elased 0:31:11. Training loss: 3.2613090035590258. Learning Rate: 2.51998001998002e-05\n",
      "Batch 4,450 of 9,631 Elased 0:31:31. Training loss: 3.2619908016719177. Learning Rate: 2.5182456432456437e-05\n",
      "Batch 4,500 of 9,631 Elased 0:31:52. Training loss: 3.2633498876359726. Learning Rate: 2.5165112665112666e-05\n",
      "Batch 4,550 of 9,631 Elased 0:32:13. Training loss: 3.261559488275549. Learning Rate: 2.51477688977689e-05\n",
      "Batch 4,600 of 9,631 Elased 0:32:34. Training loss: 3.262301740439042. Learning Rate: 2.5130425130425135e-05\n",
      "Batch 4,650 of 9,631 Elased 0:32:55. Training loss: 3.261542313380908. Learning Rate: 2.5113081363081364e-05\n",
      "Batch 4,700 of 9,631 Elased 0:33:17. Training loss: 3.26165534810817. Learning Rate: 2.5095737595737597e-05\n",
      "Batch 4,750 of 9,631 Elased 0:33:38. Training loss: 3.2615010220879004. Learning Rate: 2.5078393828393833e-05\n",
      "Batch 4,800 of 9,631 Elased 0:33:59. Training loss: 3.26003070940574. Learning Rate: 2.5061050061050062e-05\n",
      "Batch 4,850 of 9,631 Elased 0:34:20. Training loss: 3.25942671289149. Learning Rate: 2.504370629370629e-05\n",
      "Batch 4,900 of 9,631 Elased 0:34:41. Training loss: 3.260351229687126. Learning Rate: 2.502636252636253e-05\n",
      "Batch 4,950 of 9,631 Elased 0:35:03. Training loss: 3.260599379587655. Learning Rate: 2.500901875901876e-05\n",
      "Batch 5,000 of 9,631 Elased 0:35:24. Training loss: 3.2620265213489534. Learning Rate: 2.4991674991674992e-05\n",
      "Batch 5,050 of 9,631 Elased 0:35:45. Training loss: 3.2616655883222525. Learning Rate: 2.4974331224331225e-05\n",
      "Batch 5,100 of 9,631 Elased 0:36:06. Training loss: 3.260187627147226. Learning Rate: 2.4956987456987458e-05\n",
      "Batch 5,150 of 9,631 Elased 0:36:28. Training loss: 3.2604700895883503. Learning Rate: 2.493964368964369e-05\n",
      "Batch 5,200 of 9,631 Elased 0:36:49. Training loss: 3.2599712963746144. Learning Rate: 2.4922299922299923e-05\n",
      "Batch 5,250 of 9,631 Elased 0:37:10. Training loss: 3.260074666477385. Learning Rate: 2.4904956154956156e-05\n",
      "Batch 5,300 of 9,631 Elased 0:37:31. Training loss: 3.2598247314399145. Learning Rate: 2.488761238761239e-05\n",
      "Batch 5,350 of 9,631 Elased 0:37:52. Training loss: 3.261873708127815. Learning Rate: 2.487026862026862e-05\n",
      "Batch 5,400 of 9,631 Elased 0:38:13. Training loss: 3.2619562547957455. Learning Rate: 2.4852924852924854e-05\n",
      "Batch 5,450 of 9,631 Elased 0:38:35. Training loss: 3.2617666813430435. Learning Rate: 2.4835581085581086e-05\n",
      "Batch 5,500 of 9,631 Elased 0:38:56. Training loss: 3.2620022630474783. Learning Rate: 2.481823731823732e-05\n",
      "Batch 5,550 of 9,631 Elased 0:39:17. Training loss: 3.2624521297807094. Learning Rate: 2.480089355089355e-05\n",
      "Batch 5,600 of 9,631 Elased 0:39:39. Training loss: 3.261925253293344. Learning Rate: 2.4783549783549784e-05\n",
      "Batch 5,650 of 9,631 Elased 0:40:00. Training loss: 3.263664825637784. Learning Rate: 2.4766206016206017e-05\n",
      "Batch 5,700 of 9,631 Elased 0:40:22. Training loss: 3.26354190644465. Learning Rate: 2.474886224886225e-05\n",
      "Batch 5,750 of 9,631 Elased 0:40:43. Training loss: 3.2634240461639736. Learning Rate: 2.4731518481518482e-05\n",
      "Batch 5,800 of 9,631 Elased 0:41:04. Training loss: 3.2636355496891616. Learning Rate: 2.4714174714174715e-05\n",
      "Batch 5,850 of 9,631 Elased 0:41:25. Training loss: 3.264013956734258. Learning Rate: 2.4696830946830948e-05\n",
      "Batch 5,900 of 9,631 Elased 0:41:46. Training loss: 3.2639500211255026. Learning Rate: 2.467948717948718e-05\n",
      "Batch 5,950 of 9,631 Elased 0:42:08. Training loss: 3.264267808269052. Learning Rate: 2.4662143412143413e-05\n",
      "Batch 6,000 of 9,631 Elased 0:42:29. Training loss: 3.2637142238815624. Learning Rate: 2.4644799644799646e-05\n",
      "Batch 6,050 of 9,631 Elased 0:42:50. Training loss: 3.263911198210125. Learning Rate: 2.4627455877455878e-05\n",
      "Batch 6,100 of 9,631 Elased 0:43:11. Training loss: 3.264326535424248. Learning Rate: 2.461011211011211e-05\n",
      "Batch 6,150 of 9,631 Elased 0:43:32. Training loss: 3.2640551558355004. Learning Rate: 2.4592768342768344e-05\n",
      "Batch 6,200 of 9,631 Elased 0:43:54. Training loss: 3.2621503246407353. Learning Rate: 2.4575424575424576e-05\n",
      "Batch 6,250 of 9,631 Elased 0:44:15. Training loss: 3.2616285869026185. Learning Rate: 2.4558080808080812e-05\n",
      "Batch 6,300 of 9,631 Elased 0:44:36. Training loss: 3.261455386536462. Learning Rate: 2.454073704073704e-05\n",
      "Batch 6,350 of 9,631 Elased 0:44:57. Training loss: 3.2610017568295397. Learning Rate: 2.4523393273393274e-05\n",
      "Batch 6,400 of 9,631 Elased 0:45:19. Training loss: 3.2606971312128006. Learning Rate: 2.4506049506049507e-05\n",
      "Batch 6,450 of 9,631 Elased 0:45:40. Training loss: 3.2618740422041843. Learning Rate: 2.448870573870574e-05\n",
      "Batch 6,500 of 9,631 Elased 0:46:01. Training loss: 3.260505839439539. Learning Rate: 2.4471361971361972e-05\n",
      "Batch 6,550 of 9,631 Elased 0:46:22. Training loss: 3.260709916780923. Learning Rate: 2.4454018204018205e-05\n",
      "Batch 6,600 of 9,631 Elased 0:46:44. Training loss: 3.2612909234111958. Learning Rate: 2.4436674436674437e-05\n",
      "Batch 6,650 of 9,631 Elased 0:47:05. Training loss: 3.261599709772526. Learning Rate: 2.441933066933067e-05\n",
      "Batch 6,700 of 9,631 Elased 0:47:26. Training loss: 3.2627462278373205. Learning Rate: 2.4401986901986903e-05\n",
      "Batch 6,750 of 9,631 Elased 0:47:48. Training loss: 3.2625708496835495. Learning Rate: 2.4384643134643135e-05\n",
      "Batch 6,800 of 9,631 Elased 0:48:09. Training loss: 3.262351807969458. Learning Rate: 2.4367299367299368e-05\n",
      "Batch 6,850 of 9,631 Elased 0:48:30. Training loss: 3.2619064278324155. Learning Rate: 2.43499555999556e-05\n",
      "Batch 6,900 of 9,631 Elased 0:48:51. Training loss: 3.2613929195853246. Learning Rate: 2.4332611832611833e-05\n",
      "Batch 6,950 of 9,631 Elased 0:49:13. Training loss: 3.2613510194785302. Learning Rate: 2.4315268065268066e-05\n",
      "Batch 7,000 of 9,631 Elased 0:49:34. Training loss: 3.260911400437355. Learning Rate: 2.42979242979243e-05\n",
      "Batch 7,050 of 9,631 Elased 0:49:55. Training loss: 3.2604414494832357. Learning Rate: 2.428058053058053e-05\n",
      "Batch 7,100 of 9,631 Elased 0:50:16. Training loss: 3.2606658391213754. Learning Rate: 2.4263236763236767e-05\n",
      "Batch 7,150 of 9,631 Elased 0:50:38. Training loss: 3.260451067210911. Learning Rate: 2.4245892995892997e-05\n",
      "Batch 7,200 of 9,631 Elased 0:50:59. Training loss: 3.2604942573938103. Learning Rate: 2.422854922854923e-05\n",
      "Batch 7,250 of 9,631 Elased 0:51:20. Training loss: 3.259678697668273. Learning Rate: 2.4211205461205462e-05\n",
      "Batch 7,300 of 9,631 Elased 0:51:42. Training loss: 3.2599956399120695. Learning Rate: 2.4193861693861695e-05\n",
      "Batch 7,350 of 9,631 Elased 0:52:03. Training loss: 3.259441029149659. Learning Rate: 2.4176517926517927e-05\n",
      "Batch 7,400 of 9,631 Elased 0:52:24. Training loss: 3.258027258289827. Learning Rate: 2.415917415917416e-05\n",
      "Batch 7,450 of 9,631 Elased 0:52:46. Training loss: 3.257842576183729. Learning Rate: 2.4141830391830393e-05\n",
      "Batch 7,500 of 9,631 Elased 0:53:07. Training loss: 3.256960477654139. Learning Rate: 2.4124486624486625e-05\n",
      "Batch 7,550 of 9,631 Elased 0:53:28. Training loss: 3.2567697423025472. Learning Rate: 2.4107142857142858e-05\n",
      "Batch 7,600 of 9,631 Elased 0:53:50. Training loss: 3.257228546644512. Learning Rate: 2.408979908979909e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7,650 of 9,631 Elased 0:54:12. Training loss: 3.2573320095990996. Learning Rate: 2.4072455322455323e-05\n",
      "Batch 7,700 of 9,631 Elased 0:54:33. Training loss: 3.2564077816845534. Learning Rate: 2.4055111555111556e-05\n",
      "Batch 7,750 of 9,631 Elased 0:54:54. Training loss: 3.25476305203284. Learning Rate: 2.403776778776779e-05\n",
      "Batch 7,800 of 9,631 Elased 0:55:16. Training loss: 3.255255827857898. Learning Rate: 2.402042402042402e-05\n",
      "Batch 7,850 of 9,631 Elased 0:55:37. Training loss: 3.2555540058870984. Learning Rate: 2.4003080253080254e-05\n",
      "Batch 7,900 of 9,631 Elased 0:55:58. Training loss: 3.254497897428802. Learning Rate: 2.3985736485736487e-05\n",
      "Batch 7,950 of 9,631 Elased 0:56:19. Training loss: 3.255949710015231. Learning Rate: 2.396839271839272e-05\n",
      "Batch 8,000 of 9,631 Elased 0:56:41. Training loss: 3.2556532890349628. Learning Rate: 2.3951048951048952e-05\n",
      "Batch 8,050 of 9,631 Elased 0:57:02. Training loss: 3.2566601659496377. Learning Rate: 2.3933705183705184e-05\n",
      "Batch 8,100 of 9,631 Elased 0:57:24. Training loss: 3.2567336551201196. Learning Rate: 2.3916361416361417e-05\n",
      "Batch 8,150 of 9,631 Elased 0:57:45. Training loss: 3.2574613025437102. Learning Rate: 2.389901764901765e-05\n",
      "Batch 8,200 of 9,631 Elased 0:58:06. Training loss: 3.257242633525918. Learning Rate: 2.3881673881673882e-05\n",
      "Batch 8,250 of 9,631 Elased 0:58:27. Training loss: 3.2577116012428746. Learning Rate: 2.3864330114330115e-05\n",
      "Batch 8,300 of 9,631 Elased 0:58:49. Training loss: 3.2578441649603556. Learning Rate: 2.3846986346986348e-05\n",
      "Batch 8,350 of 9,631 Elased 0:59:10. Training loss: 3.257540740352905. Learning Rate: 2.382964257964258e-05\n",
      "Batch 8,400 of 9,631 Elased 0:59:31. Training loss: 3.256835358611175. Learning Rate: 2.3812298812298813e-05\n",
      "Batch 8,450 of 9,631 Elased 0:59:53. Training loss: 3.256234394883263. Learning Rate: 2.3794955044955046e-05\n",
      "Batch 8,500 of 9,631 Elased 1:00:14. Training loss: 3.255959394721424. Learning Rate: 2.377761127761128e-05\n",
      "Batch 8,550 of 9,631 Elased 1:00:35. Training loss: 3.2556283147572076. Learning Rate: 2.376026751026751e-05\n",
      "Batch 8,600 of 9,631 Elased 1:00:56. Training loss: 3.256304530085519. Learning Rate: 2.3742923742923744e-05\n",
      "Batch 8,650 of 9,631 Elased 1:01:17. Training loss: 3.255293335266885. Learning Rate: 2.3725579975579976e-05\n",
      "Batch 8,700 of 9,631 Elased 1:01:39. Training loss: 3.2549559191588697. Learning Rate: 2.370823620823621e-05\n",
      "Batch 8,750 of 9,631 Elased 1:02:00. Training loss: 3.2548690999848504. Learning Rate: 2.369089244089244e-05\n",
      "Batch 8,800 of 9,631 Elased 1:02:21. Training loss: 3.2548730995980177. Learning Rate: 2.3673548673548674e-05\n",
      "Batch 8,850 of 9,631 Elased 1:02:42. Training loss: 3.25442033307027. Learning Rate: 2.3656204906204907e-05\n",
      "Batch 8,900 of 9,631 Elased 1:03:04. Training loss: 3.2546284474415725. Learning Rate: 2.363886113886114e-05\n",
      "Batch 8,950 of 9,631 Elased 1:03:25. Training loss: 3.2542161620795396. Learning Rate: 2.3621517371517372e-05\n",
      "Batch 9,000 of 9,631 Elased 1:03:46. Training loss: 3.253306572225359. Learning Rate: 2.3604173604173605e-05\n",
      "Batch 9,050 of 9,631 Elased 1:04:07. Training loss: 3.2541558265686037. Learning Rate: 2.3586829836829838e-05\n",
      "Batch 9,100 of 9,631 Elased 1:04:28. Training loss: 3.2539625673241668. Learning Rate: 2.356948606948607e-05\n",
      "Batch 9,150 of 9,631 Elased 1:04:49. Training loss: 3.2541964072086773. Learning Rate: 2.3552142302142303e-05\n",
      "Batch 9,200 of 9,631 Elased 1:05:11. Training loss: 3.2548840115251747. Learning Rate: 2.3534798534798536e-05\n",
      "Batch 9,250 of 9,631 Elased 1:05:32. Training loss: 3.2546365789593876. Learning Rate: 2.3517454767454768e-05\n",
      "Batch 9,300 of 9,631 Elased 1:05:53. Training loss: 3.2552237699493287. Learning Rate: 2.3500111000111e-05\n",
      "Batch 9,350 of 9,631 Elased 1:06:14. Training loss: 3.255257262762855. Learning Rate: 2.3482767232767234e-05\n",
      "Batch 9,400 of 9,631 Elased 1:06:35. Training loss: 3.2551162038331336. Learning Rate: 2.3465423465423466e-05\n",
      "Batch 9,450 of 9,631 Elased 1:06:57. Training loss: 3.2548394845150135. Learning Rate: 2.34480796980797e-05\n",
      "Batch 9,500 of 9,631 Elased 1:07:18. Training loss: 3.25441186216003. Learning Rate: 2.343073593073593e-05\n",
      "Batch 9,550 of 9,631 Elased 1:07:39. Training loss: 3.2538595512025643. Learning Rate: 2.3413392163392164e-05\n",
      "Batch 9,600 of 9,631 Elased 1:08:00. Training loss: 3.2542097176238896. Learning Rate: 2.3396048396048397e-05\n",
      "\n",
      "\n",
      "  Average training loss: 3.25\n",
      "  Training epcoh took: 1:08:14\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73209f210c34aa8bd17cd1392b7c82d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=67.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Validation Loss: 3.42\n",
      "  Validation took: 0:00:42\n",
      "\n",
      "======== Epoch 24 / 30 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd66ebc4f9054d63b81c1aceff9885ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch    50 of 9,631 Elased 0:00:21. Training loss: 3.343650369644165. Learning Rate: 2.3367951492951494e-05\n",
      "Batch   100 of 9,631 Elased 0:00:42. Training loss: 3.275720292329788. Learning Rate: 2.3350607725607727e-05\n",
      "Batch   150 of 9,631 Elased 0:01:04. Training loss: 3.285197899341583. Learning Rate: 2.333326395826396e-05\n",
      "Batch   200 of 9,631 Elased 0:01:25. Training loss: 3.3129568809270857. Learning Rate: 2.3315920190920192e-05\n",
      "Batch   250 of 9,631 Elased 0:01:46. Training loss: 3.2638204140663145. Learning Rate: 2.3298576423576425e-05\n",
      "Batch   300 of 9,631 Elased 0:02:08. Training loss: 3.249939578771591. Learning Rate: 2.3281232656232658e-05\n",
      "Batch   350 of 9,631 Elased 0:02:29. Training loss: 3.253496688774654. Learning Rate: 2.326388888888889e-05\n",
      "Batch   400 of 9,631 Elased 0:02:50. Training loss: 3.260457110106945. Learning Rate: 2.324654512154512e-05\n",
      "Batch   450 of 9,631 Elased 0:03:12. Training loss: 3.2623467479811774. Learning Rate: 2.3229201354201356e-05\n",
      "Batch   500 of 9,631 Elased 0:03:33. Training loss: 3.267387989282608. Learning Rate: 2.321185758685759e-05\n",
      "Batch   550 of 9,631 Elased 0:03:53. Training loss: 3.255140223719857. Learning Rate: 2.319451381951382e-05\n",
      "Batch   600 of 9,631 Elased 0:04:15. Training loss: 3.2688878951470057. Learning Rate: 2.3177170052170054e-05\n",
      "Batch   650 of 9,631 Elased 0:04:36. Training loss: 3.2690044448925897. Learning Rate: 2.3159826284826286e-05\n",
      "Batch   700 of 9,631 Elased 0:04:57. Training loss: 3.26263818860054. Learning Rate: 2.314248251748252e-05\n",
      "Batch   750 of 9,631 Elased 0:05:18. Training loss: 3.249679117043813. Learning Rate: 2.312513875013875e-05\n",
      "Batch   800 of 9,631 Elased 0:05:39. Training loss: 3.2476194281876087. Learning Rate: 2.3107794982794984e-05\n",
      "Batch   850 of 9,631 Elased 0:06:00. Training loss: 3.241538228006924. Learning Rate: 2.3090451215451217e-05\n",
      "Batch   900 of 9,631 Elased 0:06:22. Training loss: 3.239880303674274. Learning Rate: 2.307310744810745e-05\n",
      "Batch   950 of 9,631 Elased 0:06:43. Training loss: 3.2356431043775458. Learning Rate: 2.3055763680763682e-05\n",
      "Batch 1,000 of 9,631 Elased 0:07:04. Training loss: 3.237737918972969. Learning Rate: 2.3038419913419915e-05\n",
      "Batch 1,050 of 9,631 Elased 0:07:25. Training loss: 3.239508133729299. Learning Rate: 2.3021076146076148e-05\n",
      "Batch 1,100 of 9,631 Elased 0:07:46. Training loss: 3.241818191463297. Learning Rate: 2.300373237873238e-05\n",
      "Batch 1,150 of 9,631 Elased 0:08:08. Training loss: 3.2410983567652494. Learning Rate: 2.2986388611388613e-05\n",
      "Batch 1,200 of 9,631 Elased 0:08:30. Training loss: 3.2423364050189654. Learning Rate: 2.2969044844044846e-05\n",
      "Batch 1,250 of 9,631 Elased 0:08:52. Training loss: 3.245064664554596. Learning Rate: 2.2951701076701075e-05\n",
      "Batch 1,300 of 9,631 Elased 0:09:14. Training loss: 3.24083556166062. Learning Rate: 2.293435730935731e-05\n",
      "Batch 1,350 of 9,631 Elased 0:09:35. Training loss: 3.242281165564502. Learning Rate: 2.2917013542013544e-05\n",
      "Batch 1,400 of 9,631 Elased 0:09:57. Training loss: 3.2410648278679166. Learning Rate: 2.2899669774669776e-05\n",
      "Batch 1,450 of 9,631 Elased 0:10:19. Training loss: 3.239285036202135. Learning Rate: 2.288232600732601e-05\n",
      "Batch 1,500 of 9,631 Elased 0:10:41. Training loss: 3.2373056489626566. Learning Rate: 2.286498223998224e-05\n",
      "Batch 1,550 of 9,631 Elased 0:11:03. Training loss: 3.2375194774135467. Learning Rate: 2.2847638472638474e-05\n",
      "Batch 1,600 of 9,631 Elased 0:11:25. Training loss: 3.242489463686943. Learning Rate: 2.2830294705294707e-05\n",
      "Batch 1,650 of 9,631 Elased 0:11:46. Training loss: 3.2451803187168005. Learning Rate: 2.281295093795094e-05\n",
      "Batch 1,700 of 9,631 Elased 0:12:07. Training loss: 3.241925224696889. Learning Rate: 2.2795607170607172e-05\n",
      "Batch 1,750 of 9,631 Elased 0:12:29. Training loss: 3.241949246542794. Learning Rate: 2.2778263403263405e-05\n",
      "Batch 1,800 of 9,631 Elased 0:12:50. Training loss: 3.240576214922799. Learning Rate: 2.2760919635919637e-05\n",
      "Batch 1,850 of 9,631 Elased 0:13:11. Training loss: 3.242574629268131. Learning Rate: 2.274357586857587e-05\n",
      "Batch 1,900 of 9,631 Elased 0:13:32. Training loss: 3.242096570792951. Learning Rate: 2.2726232101232103e-05\n",
      "Batch 1,950 of 9,631 Elased 0:13:53. Training loss: 3.2388245820387818. Learning Rate: 2.2708888333888335e-05\n",
      "Batch 2,000 of 9,631 Elased 0:14:15. Training loss: 3.2351639015078546. Learning Rate: 2.2691544566544568e-05\n",
      "Batch 2,050 of 9,631 Elased 0:14:36. Training loss: 3.23614205854695. Learning Rate: 2.26742007992008e-05\n",
      "Batch 2,100 of 9,631 Elased 0:14:58. Training loss: 3.239462813366027. Learning Rate: 2.265685703185703e-05\n",
      "Batch 2,150 of 9,631 Elased 0:15:19. Training loss: 3.2394902652363444. Learning Rate: 2.2639513264513266e-05\n",
      "Batch 2,200 of 9,631 Elased 0:15:40. Training loss: 3.2394243851033124. Learning Rate: 2.26221694971695e-05\n",
      "Batch 2,250 of 9,631 Elased 0:16:01. Training loss: 3.2373712459670174. Learning Rate: 2.260482572982573e-05\n",
      "Batch 2,300 of 9,631 Elased 0:16:22. Training loss: 3.2387806569493334. Learning Rate: 2.2587481962481964e-05\n",
      "Batch 2,350 of 9,631 Elased 0:16:43. Training loss: 3.236950919628143. Learning Rate: 2.2570138195138197e-05\n",
      "Batch 2,400 of 9,631 Elased 0:17:05. Training loss: 3.2365870515008766. Learning Rate: 2.255279442779443e-05\n",
      "Batch 2,450 of 9,631 Elased 0:17:27. Training loss: 3.2333184850945766. Learning Rate: 2.2535450660450662e-05\n",
      "Batch 2,500 of 9,631 Elased 0:17:50. Training loss: 3.2314452970027925. Learning Rate: 2.2518106893106895e-05\n",
      "Batch 2,550 of 9,631 Elased 0:18:12. Training loss: 3.2359054094669863. Learning Rate: 2.2500763125763127e-05\n",
      "Batch 2,600 of 9,631 Elased 0:18:35. Training loss: 3.2394887313934473. Learning Rate: 2.248341935841936e-05\n",
      "Batch 2,650 of 9,631 Elased 0:18:58. Training loss: 3.241904860757432. Learning Rate: 2.2466075591075593e-05\n",
      "Batch 2,700 of 9,631 Elased 0:19:21. Training loss: 3.2443665583045393. Learning Rate: 2.2448731823731825e-05\n",
      "Batch 2,750 of 9,631 Elased 0:19:43. Training loss: 3.2417882237867874. Learning Rate: 2.2431388056388058e-05\n",
      "Batch 2,800 of 9,631 Elased 0:20:05. Training loss: 3.2430413679565704. Learning Rate: 2.241404428904429e-05\n",
      "Batch 2,850 of 9,631 Elased 0:20:27. Training loss: 3.2467744755326655. Learning Rate: 2.2396700521700523e-05\n",
      "Batch 2,900 of 9,631 Elased 0:20:48. Training loss: 3.246872328643141. Learning Rate: 2.2379356754356756e-05\n",
      "Batch 2,950 of 9,631 Elased 0:21:09. Training loss: 3.247243151583914. Learning Rate: 2.236201298701299e-05\n",
      "Batch 3,000 of 9,631 Elased 0:21:30. Training loss: 3.2501585026582083. Learning Rate: 2.234466921966922e-05\n",
      "Batch 3,050 of 9,631 Elased 0:21:52. Training loss: 3.2488905818345115. Learning Rate: 2.2327325452325454e-05\n",
      "Batch 3,100 of 9,631 Elased 0:22:13. Training loss: 3.24961051348717. Learning Rate: 2.2309981684981687e-05\n",
      "Batch 3,150 of 9,631 Elased 0:22:34. Training loss: 3.245844384223696. Learning Rate: 2.229263791763792e-05\n",
      "Batch 3,200 of 9,631 Elased 0:22:56. Training loss: 3.248024080172181. Learning Rate: 2.2275294150294152e-05\n",
      "Batch 3,250 of 9,631 Elased 0:23:17. Training loss: 3.24742919254303. Learning Rate: 2.2257950382950384e-05\n",
      "Batch 3,300 of 9,631 Elased 0:23:38. Training loss: 3.2493734436324146. Learning Rate: 2.2240606615606617e-05\n",
      "Batch 3,350 of 9,631 Elased 0:23:59. Training loss: 3.2501139837948245. Learning Rate: 2.222326284826285e-05\n",
      "Batch 3,400 of 9,631 Elased 0:24:22. Training loss: 3.250475702566259. Learning Rate: 2.2205919080919082e-05\n",
      "Batch 3,450 of 9,631 Elased 0:24:45. Training loss: 3.250635081374127. Learning Rate: 2.2188575313575312e-05\n",
      "Batch 3,500 of 9,631 Elased 0:25:07. Training loss: 3.250070813996451. Learning Rate: 2.2171231546231548e-05\n",
      "Batch 3,550 of 9,631 Elased 0:25:30. Training loss: 3.251325115754571. Learning Rate: 2.215388777888778e-05\n",
      "Batch 3,600 of 9,631 Elased 0:25:52. Training loss: 3.2524946679671607. Learning Rate: 2.2136544011544013e-05\n",
      "Batch 3,650 of 9,631 Elased 0:26:15. Training loss: 3.256034352616088. Learning Rate: 2.2119200244200246e-05\n",
      "Batch 3,700 of 9,631 Elased 0:26:37. Training loss: 3.255421101982529. Learning Rate: 2.210185647685648e-05\n",
      "Batch 3,750 of 9,631 Elased 0:27:00. Training loss: 3.254126989587148. Learning Rate: 2.208451270951271e-05\n",
      "Batch 3,800 of 9,631 Elased 0:27:23. Training loss: 3.252931123564118. Learning Rate: 2.2067168942168944e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3,850 of 9,631 Elased 0:27:45. Training loss: 3.2510141491889955. Learning Rate: 2.2049825174825176e-05\n",
      "Batch 3,900 of 9,631 Elased 0:28:08. Training loss: 3.2499612722335716. Learning Rate: 2.203248140748141e-05\n",
      "Batch 3,950 of 9,631 Elased 0:28:31. Training loss: 3.2509357567376727. Learning Rate: 2.201513764013764e-05\n",
      "Batch 4,000 of 9,631 Elased 0:28:53. Training loss: 3.251441173106432. Learning Rate: 2.1997793872793874e-05\n",
      "Batch 4,050 of 9,631 Elased 0:29:16. Training loss: 3.2509161353699954. Learning Rate: 2.1980450105450107e-05\n",
      "Batch 4,100 of 9,631 Elased 0:29:38. Training loss: 3.2500185220125246. Learning Rate: 2.196310633810634e-05\n",
      "Batch 4,150 of 9,631 Elased 0:30:01. Training loss: 3.2496521478388685. Learning Rate: 2.1945762570762572e-05\n",
      "Batch 4,200 of 9,631 Elased 0:30:23. Training loss: 3.249862011642683. Learning Rate: 2.1928418803418805e-05\n",
      "Batch 4,250 of 9,631 Elased 0:30:46. Training loss: 3.250105042710024. Learning Rate: 2.1911075036075038e-05\n",
      "Batch 4,300 of 9,631 Elased 0:31:08. Training loss: 3.2494633850663206. Learning Rate: 2.1893731268731267e-05\n",
      "Batch 4,350 of 9,631 Elased 0:31:31. Training loss: 3.249078116005865. Learning Rate: 2.1876387501387503e-05\n",
      "Batch 4,400 of 9,631 Elased 0:31:53. Training loss: 3.2511140635067766. Learning Rate: 2.1859043734043736e-05\n",
      "Batch 4,450 of 9,631 Elased 0:32:16. Training loss: 3.25157256718432. Learning Rate: 2.1841699966699968e-05\n",
      "Batch 4,500 of 9,631 Elased 0:32:39. Training loss: 3.2528130077785917. Learning Rate: 2.18243561993562e-05\n",
      "Batch 4,550 of 9,631 Elased 0:33:01. Training loss: 3.250884830611093. Learning Rate: 2.1807012432012434e-05\n",
      "Batch 4,600 of 9,631 Elased 0:33:24. Training loss: 3.2519340628644695. Learning Rate: 2.1789668664668666e-05\n",
      "Batch 4,650 of 9,631 Elased 0:33:46. Training loss: 3.251180744017324. Learning Rate: 2.17723248973249e-05\n",
      "Batch 4,700 of 9,631 Elased 0:34:09. Training loss: 3.2514577195492196. Learning Rate: 2.175498112998113e-05\n",
      "Batch 4,750 of 9,631 Elased 0:34:31. Training loss: 3.251411110752507. Learning Rate: 2.1737637362637364e-05\n",
      "Batch 4,800 of 9,631 Elased 0:34:54. Training loss: 3.2495670148978633. Learning Rate: 2.1720293595293597e-05\n",
      "Batch 4,850 of 9,631 Elased 0:35:17. Training loss: 3.249097129610396. Learning Rate: 2.170294982794983e-05\n",
      "Batch 4,900 of 9,631 Elased 0:35:39. Training loss: 3.2502574146280483. Learning Rate: 2.1685606060606062e-05\n",
      "Batch 4,950 of 9,631 Elased 0:36:02. Training loss: 3.250366858737637. Learning Rate: 2.1668262293262295e-05\n",
      "Batch 5,000 of 9,631 Elased 0:36:25. Training loss: 3.251565285563469. Learning Rate: 2.1650918525918527e-05\n",
      "Batch 5,050 of 9,631 Elased 0:36:47. Training loss: 3.2510742293253982. Learning Rate: 2.163357475857476e-05\n",
      "Batch 5,100 of 9,631 Elased 0:37:10. Training loss: 3.249175483712963. Learning Rate: 2.1616230991230993e-05\n",
      "Batch 5,150 of 9,631 Elased 0:37:33. Training loss: 3.2492269422012625. Learning Rate: 2.1598887223887222e-05\n",
      "Batch 5,200 of 9,631 Elased 0:37:56. Training loss: 3.249031527088239. Learning Rate: 2.1581543456543458e-05\n",
      "Batch 5,250 of 9,631 Elased 0:38:18. Training loss: 3.2490948673884072. Learning Rate: 2.156419968919969e-05\n",
      "Batch 5,300 of 9,631 Elased 0:38:41. Training loss: 3.2488654539720065. Learning Rate: 2.1546855921855923e-05\n",
      "Batch 5,350 of 9,631 Elased 0:39:03. Training loss: 3.250720213439977. Learning Rate: 2.1529512154512156e-05\n",
      "Batch 5,400 of 9,631 Elased 0:39:26. Training loss: 3.250591747164726. Learning Rate: 2.151216838716839e-05\n",
      "Batch 5,450 of 9,631 Elased 0:39:48. Training loss: 3.2504302409154557. Learning Rate: 2.149482461982462e-05\n",
      "Batch 5,500 of 9,631 Elased 0:40:11. Training loss: 3.250882934331894. Learning Rate: 2.1477480852480854e-05\n",
      "Batch 5,550 of 9,631 Elased 0:40:34. Training loss: 3.2519234584043692. Learning Rate: 2.1460137085137087e-05\n",
      "Batch 5,600 of 9,631 Elased 0:40:56. Training loss: 3.2510510942978517. Learning Rate: 2.144279331779332e-05\n",
      "Batch 5,650 of 9,631 Elased 0:41:19. Training loss: 3.25271492078241. Learning Rate: 2.1425449550449552e-05\n",
      "Batch 5,700 of 9,631 Elased 0:41:42. Training loss: 3.2524789013151536. Learning Rate: 2.1408105783105785e-05\n",
      "Batch 5,750 of 9,631 Elased 0:42:04. Training loss: 3.2519964176468226. Learning Rate: 2.1390762015762017e-05\n",
      "Batch 5,800 of 9,631 Elased 0:42:27. Training loss: 3.252084275627958. Learning Rate: 2.137341824841825e-05\n",
      "Batch 5,850 of 9,631 Elased 0:42:49. Training loss: 3.2524501870024918. Learning Rate: 2.1356074481074483e-05\n",
      "Batch 5,900 of 9,631 Elased 0:43:12. Training loss: 3.2522933044069906. Learning Rate: 2.1338730713730715e-05\n",
      "Batch 5,950 of 9,631 Elased 0:43:35. Training loss: 3.252424711760353. Learning Rate: 2.1321386946386948e-05\n",
      "Batch 6,000 of 9,631 Elased 0:43:57. Training loss: 3.252107009192308. Learning Rate: 2.1304043179043177e-05\n",
      "Batch 6,050 of 9,631 Elased 0:44:20. Training loss: 3.2522163984007086. Learning Rate: 2.1286699411699413e-05\n",
      "Batch 6,100 of 9,631 Elased 0:44:42. Training loss: 3.252606678575766. Learning Rate: 2.1269355644355646e-05\n",
      "Batch 6,150 of 9,631 Elased 0:45:05. Training loss: 3.252642885630693. Learning Rate: 2.125201187701188e-05\n",
      "Batch 6,200 of 9,631 Elased 0:45:27. Training loss: 3.25090365517524. Learning Rate: 2.123466810966811e-05\n",
      "Batch 6,250 of 9,631 Elased 0:45:50. Training loss: 3.250471907234192. Learning Rate: 2.1217324342324344e-05\n",
      "Batch 6,300 of 9,631 Elased 0:46:13. Training loss: 3.2503484515538292. Learning Rate: 2.1199980574980577e-05\n",
      "Batch 6,350 of 9,631 Elased 0:46:35. Training loss: 3.249854921675104. Learning Rate: 2.118263680763681e-05\n",
      "Batch 6,400 of 9,631 Elased 0:46:58. Training loss: 3.249600711967796. Learning Rate: 2.1165293040293042e-05\n",
      "Batch 6,450 of 9,631 Elased 0:47:21. Training loss: 3.250403408763945. Learning Rate: 2.1147949272949274e-05\n",
      "Batch 6,500 of 9,631 Elased 0:47:43. Training loss: 3.2493231602265285. Learning Rate: 2.1130605505605507e-05\n",
      "Batch 6,550 of 9,631 Elased 0:48:05. Training loss: 3.2494901714434152. Learning Rate: 2.111326173826174e-05\n",
      "Batch 6,600 of 9,631 Elased 0:48:28. Training loss: 3.2503753098574553. Learning Rate: 2.1095917970917972e-05\n",
      "Batch 6,650 of 9,631 Elased 0:48:50. Training loss: 3.25087646484375. Learning Rate: 2.1078574203574205e-05\n",
      "Batch 6,700 of 9,631 Elased 0:49:13. Training loss: 3.2518743347054095. Learning Rate: 2.1061230436230438e-05\n",
      "Batch 6,750 of 9,631 Elased 0:49:36. Training loss: 3.2518436145076044. Learning Rate: 2.104388666888667e-05\n",
      "Batch 6,800 of 9,631 Elased 0:49:58. Training loss: 3.2517774616970736. Learning Rate: 2.1026542901542903e-05\n",
      "Batch 6,850 of 9,631 Elased 0:50:21. Training loss: 3.2512260406730817. Learning Rate: 2.1009199134199132e-05\n",
      "Batch 6,900 of 9,631 Elased 0:50:43. Training loss: 3.250744943653328. Learning Rate: 2.099185536685537e-05\n",
      "Batch 6,950 of 9,631 Elased 0:51:06. Training loss: 3.250560676725648. Learning Rate: 2.09745115995116e-05\n",
      "Batch 7,000 of 9,631 Elased 0:51:29. Training loss: 3.249987200805119. Learning Rate: 2.0957167832167834e-05\n",
      "Batch 7,050 of 9,631 Elased 0:51:51. Training loss: 3.2493053707163386. Learning Rate: 2.0939824064824066e-05\n",
      "Batch 7,100 of 9,631 Elased 0:52:14. Training loss: 3.2494390032324993. Learning Rate: 2.09224802974803e-05\n",
      "Batch 7,150 of 9,631 Elased 0:52:37. Training loss: 3.2491292284251925. Learning Rate: 2.090513653013653e-05\n",
      "Batch 7,200 of 9,631 Elased 0:53:00. Training loss: 3.249027787314521. Learning Rate: 2.0887792762792764e-05\n",
      "Batch 7,250 of 9,631 Elased 0:53:22. Training loss: 3.248019829421208. Learning Rate: 2.0870448995448997e-05\n",
      "Batch 7,300 of 9,631 Elased 0:53:45. Training loss: 3.2483855468932896. Learning Rate: 2.085310522810523e-05\n",
      "Batch 7,350 of 9,631 Elased 0:54:08. Training loss: 3.247932149131282. Learning Rate: 2.083576146076146e-05\n",
      "Batch 7,400 of 9,631 Elased 0:54:30. Training loss: 3.246712480896228. Learning Rate: 2.0818417693417695e-05\n",
      "Batch 7,450 of 9,631 Elased 0:54:52. Training loss: 3.2464645395502947. Learning Rate: 2.0801073926073928e-05\n",
      "Batch 7,500 of 9,631 Elased 0:55:15. Training loss: 3.2455266707261403. Learning Rate: 2.078373015873016e-05\n",
      "Batch 7,550 of 9,631 Elased 0:55:37. Training loss: 3.245500880629811. Learning Rate: 2.0766386391386393e-05\n",
      "Batch 7,600 of 9,631 Elased 0:56:00. Training loss: 3.2460354934083786. Learning Rate: 2.0749042624042626e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7,650 of 9,631 Elased 0:56:22. Training loss: 3.2460790909817017. Learning Rate: 2.0731698856698858e-05\n",
      "Batch 7,700 of 9,631 Elased 0:56:45. Training loss: 3.245213945360927. Learning Rate: 2.0714355089355088e-05\n",
      "Batch 7,750 of 9,631 Elased 0:57:08. Training loss: 3.243731603760873. Learning Rate: 2.0697011322011324e-05\n",
      "Batch 7,800 of 9,631 Elased 0:57:30. Training loss: 3.2443183273994007. Learning Rate: 2.0679667554667556e-05\n",
      "Batch 7,850 of 9,631 Elased 0:57:53. Training loss: 3.2448484092001704. Learning Rate: 2.066232378732379e-05\n",
      "Batch 7,900 of 9,631 Elased 0:58:16. Training loss: 3.2435917603969573. Learning Rate: 2.064498001998002e-05\n",
      "Batch 7,950 of 9,631 Elased 0:58:38. Training loss: 3.2449230711864976. Learning Rate: 2.0627636252636254e-05\n",
      "Batch 8,000 of 9,631 Elased 0:59:01. Training loss: 3.2445726666897534. Learning Rate: 2.0610292485292487e-05\n",
      "Batch 8,050 of 9,631 Elased 0:59:24. Training loss: 3.245468304542281. Learning Rate: 2.059294871794872e-05\n",
      "Batch 8,100 of 9,631 Elased 0:59:46. Training loss: 3.2456322634514465. Learning Rate: 2.0575604950604952e-05\n",
      "Batch 8,150 of 9,631 Elased 1:00:09. Training loss: 3.246411109804376. Learning Rate: 2.0558261183261185e-05\n",
      "Batch 8,200 of 9,631 Elased 1:00:31. Training loss: 3.2462404685921786. Learning Rate: 2.0540917415917414e-05\n",
      "Batch 8,250 of 9,631 Elased 1:00:54. Training loss: 3.246784401619073. Learning Rate: 2.052357364857365e-05\n",
      "Batch 8,300 of 9,631 Elased 1:01:16. Training loss: 3.2470907662144626. Learning Rate: 2.0506229881229883e-05\n",
      "Batch 8,350 of 9,631 Elased 1:01:39. Training loss: 3.2468754036126737. Learning Rate: 2.0488886113886115e-05\n",
      "Batch 8,400 of 9,631 Elased 1:02:02. Training loss: 3.246187833107653. Learning Rate: 2.0471542346542348e-05\n",
      "Batch 8,450 of 9,631 Elased 1:02:24. Training loss: 3.2456270756382914. Learning Rate: 2.045419857919858e-05\n",
      "Batch 8,500 of 9,631 Elased 1:02:47. Training loss: 3.2452476761621587. Learning Rate: 2.0436854811854813e-05\n",
      "Batch 8,550 of 9,631 Elased 1:03:09. Training loss: 3.244822478642938. Learning Rate: 2.0419511044511043e-05\n",
      "Batch 8,600 of 9,631 Elased 1:03:32. Training loss: 3.24569306488647. Learning Rate: 2.040216727716728e-05\n",
      "Batch 8,650 of 9,631 Elased 1:03:55. Training loss: 3.2447472132837154. Learning Rate: 2.038482350982351e-05\n",
      "Batch 8,700 of 9,631 Elased 1:04:17. Training loss: 3.2444974515218843. Learning Rate: 2.0367479742479744e-05\n",
      "Batch 8,750 of 9,631 Elased 1:04:40. Training loss: 3.244345311205728. Learning Rate: 2.0350135975135977e-05\n",
      "Batch 8,800 of 9,631 Elased 1:05:03. Training loss: 3.244173297746615. Learning Rate: 2.033279220779221e-05\n",
      "Batch 8,850 of 9,631 Elased 1:05:25. Training loss: 3.243840947716923. Learning Rate: 2.0315448440448442e-05\n",
      "Batch 8,900 of 9,631 Elased 1:05:48. Training loss: 3.244007629249873. Learning Rate: 2.0298104673104675e-05\n",
      "Batch 8,950 of 9,631 Elased 1:06:11. Training loss: 3.2435396970450547. Learning Rate: 2.0280760905760907e-05\n",
      "Batch 9,000 of 9,631 Elased 1:06:33. Training loss: 3.242697279638714. Learning Rate: 2.026341713841714e-05\n",
      "Batch 9,050 of 9,631 Elased 1:06:56. Training loss: 3.2436765015586304. Learning Rate: 2.024607337107337e-05\n",
      "Batch 9,100 of 9,631 Elased 1:07:18. Training loss: 3.243489935555301. Learning Rate: 2.0228729603729605e-05\n",
      "Batch 9,150 of 9,631 Elased 1:07:41. Training loss: 3.243820702964491. Learning Rate: 2.0211385836385838e-05\n",
      "Batch 9,200 of 9,631 Elased 1:08:03. Training loss: 3.2446058007686034. Learning Rate: 2.019404206904207e-05\n",
      "Batch 9,250 of 9,631 Elased 1:08:26. Training loss: 3.2445456579826972. Learning Rate: 2.0176698301698303e-05\n",
      "Batch 9,300 of 9,631 Elased 1:08:49. Training loss: 3.245018499820463. Learning Rate: 2.0159354534354536e-05\n",
      "Batch 9,350 of 9,631 Elased 1:09:11. Training loss: 3.2449149680520124. Learning Rate: 2.014201076701077e-05\n",
      "Batch 9,400 of 9,631 Elased 1:09:33. Training loss: 3.2449105992342564. Learning Rate: 2.0124666999667e-05\n",
      "Batch 9,450 of 9,631 Elased 1:09:56. Training loss: 3.2448200749720213. Learning Rate: 2.0107323232323234e-05\n",
      "Batch 9,500 of 9,631 Elased 1:10:18. Training loss: 3.2443105258816165. Learning Rate: 2.0089979464979467e-05\n",
      "Batch 9,550 of 9,631 Elased 1:10:41. Training loss: 3.2437025781576545. Learning Rate: 2.00726356976357e-05\n",
      "Batch 9,600 of 9,631 Elased 1:11:04. Training loss: 3.2441184196000297. Learning Rate: 2.0055291930291932e-05\n",
      "\n",
      "\n",
      "  Average training loss: 3.24\n",
      "  Training epcoh took: 1:11:17\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb907bb26d654f61a6e153ed8678941a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=67.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Validation Loss: 3.42\n",
      "  Validation took: 0:00:46\n",
      "\n",
      "======== Epoch 25 / 30 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f614893d5aed46b1adecaeb5cc032ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch    50 of 9,631 Elased 0:00:23. Training loss: 3.3215202283859253. Learning Rate: 2.002719502719503e-05\n",
      "Batch   100 of 9,631 Elased 0:00:45. Training loss: 3.2618450713157654. Learning Rate: 2.0009851259851262e-05\n",
      "Batch   150 of 9,631 Elased 0:01:08. Training loss: 3.2760854156812034. Learning Rate: 1.999250749250749e-05\n",
      "Batch   200 of 9,631 Elased 0:01:31. Training loss: 3.294571301341057. Learning Rate: 1.9975163725163727e-05\n",
      "Batch   250 of 9,631 Elased 0:01:53. Training loss: 3.249743784427643. Learning Rate: 1.995781995781996e-05\n",
      "Batch   300 of 9,631 Elased 0:02:15. Training loss: 3.236830816666285. Learning Rate: 1.9940476190476193e-05\n",
      "Batch   350 of 9,631 Elased 0:02:38. Training loss: 3.2337176156044007. Learning Rate: 1.9923132423132422e-05\n",
      "Batch   400 of 9,631 Elased 0:03:01. Training loss: 3.241830362379551. Learning Rate: 1.9905788655788658e-05\n",
      "Batch   450 of 9,631 Elased 0:03:23. Training loss: 3.244956919617123. Learning Rate: 1.988844488844489e-05\n",
      "Batch   500 of 9,631 Elased 0:03:45. Training loss: 3.251362654924393. Learning Rate: 1.987110112110112e-05\n",
      "Batch   550 of 9,631 Elased 0:04:08. Training loss: 3.2404220760952342. Learning Rate: 1.9853757353757356e-05\n",
      "Batch   600 of 9,631 Elased 0:04:30. Training loss: 3.254467712044716. Learning Rate: 1.983641358641359e-05\n",
      "Batch   650 of 9,631 Elased 0:04:53. Training loss: 3.2549288832224335. Learning Rate: 1.9819069819069818e-05\n",
      "Batch   700 of 9,631 Elased 0:05:16. Training loss: 3.249170323269708. Learning Rate: 1.980172605172605e-05\n",
      "Batch   750 of 9,631 Elased 0:05:38. Training loss: 3.2371105831464133. Learning Rate: 1.9784382284382287e-05\n",
      "Batch   800 of 9,631 Elased 0:06:01. Training loss: 3.2345740254223347. Learning Rate: 1.976703851703852e-05\n",
      "Batch   850 of 9,631 Elased 0:06:24. Training loss: 3.228111482928781. Learning Rate: 1.974969474969475e-05\n",
      "Batch   900 of 9,631 Elased 0:06:46. Training loss: 3.2270299277040695. Learning Rate: 1.9732350982350985e-05\n",
      "Batch   950 of 9,631 Elased 0:07:08. Training loss: 3.2239609591584455. Learning Rate: 1.9715007215007217e-05\n",
      "Batch 1,000 of 9,631 Elased 0:07:31. Training loss: 3.2251560319662094. Learning Rate: 1.9697663447663447e-05\n",
      "Batch 1,050 of 9,631 Elased 0:07:54. Training loss: 3.2269198436964124. Learning Rate: 1.9680319680319683e-05\n",
      "Batch 1,100 of 9,631 Elased 0:08:16. Training loss: 3.228389086398211. Learning Rate: 1.9662975912975915e-05\n",
      "Batch 1,150 of 9,631 Elased 0:08:39. Training loss: 3.2274041315783624. Learning Rate: 1.9645632145632148e-05\n",
      "Batch 1,200 of 9,631 Elased 0:09:02. Training loss: 3.227167765001456. Learning Rate: 1.9628288378288377e-05\n",
      "Batch 1,250 of 9,631 Elased 0:09:24. Training loss: 3.2280101037979128. Learning Rate: 1.9610944610944613e-05\n",
      "Batch 1,300 of 9,631 Elased 0:09:47. Training loss: 3.2242827477821936. Learning Rate: 1.9593600843600846e-05\n",
      "Batch 1,350 of 9,631 Elased 0:10:10. Training loss: 3.2233654555567988. Learning Rate: 1.9576257076257075e-05\n",
      "Batch 1,400 of 9,631 Elased 0:10:32. Training loss: 3.2223656700338634. Learning Rate: 1.955891330891331e-05\n",
      "Batch 1,450 of 9,631 Elased 0:10:55. Training loss: 3.2203237499861883. Learning Rate: 1.9541569541569544e-05\n",
      "Batch 1,500 of 9,631 Elased 0:11:18. Training loss: 3.2187374455134075. Learning Rate: 1.9524225774225773e-05\n",
      "Batch 1,550 of 9,631 Elased 0:11:40. Training loss: 3.2188997808579476. Learning Rate: 1.9506882006882006e-05\n",
      "Batch 1,600 of 9,631 Elased 0:12:03. Training loss: 3.22403645850718. Learning Rate: 1.9489538239538242e-05\n",
      "Batch 1,650 of 9,631 Elased 0:12:25. Training loss: 3.225975754839001. Learning Rate: 1.9472194472194474e-05\n",
      "Batch 1,700 of 9,631 Elased 0:12:48. Training loss: 3.22377911455491. Learning Rate: 1.9454850704850704e-05\n",
      "Batch 1,750 of 9,631 Elased 0:13:11. Training loss: 3.224424370833806. Learning Rate: 1.943750693750694e-05\n",
      "Batch 1,800 of 9,631 Elased 0:13:34. Training loss: 3.2229188466072083. Learning Rate: 1.9420163170163172e-05\n",
      "Batch 1,850 of 9,631 Elased 0:13:56. Training loss: 3.2256651420851012. Learning Rate: 1.9402819402819402e-05\n",
      "Batch 1,900 of 9,631 Elased 0:14:19. Training loss: 3.2256271185373007. Learning Rate: 1.9385475635475638e-05\n",
      "Batch 1,950 of 9,631 Elased 0:14:42. Training loss: 3.2225557997899177. Learning Rate: 1.936813186813187e-05\n",
      "Batch 2,000 of 9,631 Elased 0:15:04. Training loss: 3.2189518213868142. Learning Rate: 1.93507881007881e-05\n",
      "Batch 2,050 of 9,631 Elased 0:15:27. Training loss: 3.219740164977748. Learning Rate: 1.9333444333444332e-05\n",
      "Batch 2,100 of 9,631 Elased 0:15:49. Training loss: 3.223544552723567. Learning Rate: 1.931610056610057e-05\n",
      "Batch 2,150 of 9,631 Elased 0:16:12. Training loss: 3.2238957485487294. Learning Rate: 1.92987567987568e-05\n",
      "Batch 2,200 of 9,631 Elased 0:16:35. Training loss: 3.2241442528096114. Learning Rate: 1.928141303141303e-05\n",
      "Batch 2,250 of 9,631 Elased 0:16:57. Training loss: 3.22221115732193. Learning Rate: 1.9264069264069266e-05\n",
      "Batch 2,300 of 9,631 Elased 0:17:20. Training loss: 3.223630549752194. Learning Rate: 1.92467254967255e-05\n",
      "Batch 2,350 of 9,631 Elased 0:17:42. Training loss: 3.2220848444167602. Learning Rate: 1.9229381729381728e-05\n",
      "Batch 2,400 of 9,631 Elased 0:18:05. Training loss: 3.2216964031755926. Learning Rate: 1.921203796203796e-05\n",
      "Batch 2,450 of 9,631 Elased 0:18:27. Training loss: 3.218684103488922. Learning Rate: 1.9194694194694197e-05\n",
      "Batch 2,500 of 9,631 Elased 0:18:50. Training loss: 3.2176566247463225. Learning Rate: 1.917735042735043e-05\n",
      "Batch 2,550 of 9,631 Elased 0:19:12. Training loss: 3.222105990531398. Learning Rate: 1.916000666000666e-05\n",
      "Batch 2,600 of 9,631 Elased 0:19:35. Training loss: 3.225064702630043. Learning Rate: 1.9142662892662895e-05\n",
      "Batch 2,650 of 9,631 Elased 0:19:58. Training loss: 3.22744140809437. Learning Rate: 1.9125319125319128e-05\n",
      "Batch 2,700 of 9,631 Elased 0:20:20. Training loss: 3.229666008287006. Learning Rate: 1.9107975357975357e-05\n",
      "Batch 2,750 of 9,631 Elased 0:20:43. Training loss: 3.226806869940324. Learning Rate: 1.9090631590631593e-05\n",
      "Batch 2,800 of 9,631 Elased 0:21:07. Training loss: 3.2269431832007. Learning Rate: 1.9073287823287826e-05\n",
      "Batch 2,850 of 9,631 Elased 0:21:30. Training loss: 3.230339567284835. Learning Rate: 1.9055944055944055e-05\n",
      "Batch 2,900 of 9,631 Elased 0:21:53. Training loss: 3.230637631498534. Learning Rate: 1.9038600288600287e-05\n",
      "Batch 2,950 of 9,631 Elased 0:22:17. Training loss: 3.230834926508241. Learning Rate: 1.9021256521256524e-05\n",
      "Batch 3,000 of 9,631 Elased 0:22:40. Training loss: 3.2337453238169354. Learning Rate: 1.9003912753912756e-05\n",
      "Batch 3,050 of 9,631 Elased 0:23:04. Training loss: 3.2328735797913346. Learning Rate: 1.8986568986568985e-05\n",
      "Batch 3,100 of 9,631 Elased 0:23:27. Training loss: 3.23350018870446. Learning Rate: 1.896922521922522e-05\n",
      "Batch 3,150 of 9,631 Elased 0:23:50. Training loss: 3.2304199836746093. Learning Rate: 1.8951881451881454e-05\n",
      "Batch 3,200 of 9,631 Elased 0:24:13. Training loss: 3.2325155072286726. Learning Rate: 1.8934537684537683e-05\n",
      "Batch 3,250 of 9,631 Elased 0:24:35. Training loss: 3.232313864524548. Learning Rate: 1.8917193917193916e-05\n",
      "Batch 3,300 of 9,631 Elased 0:24:58. Training loss: 3.2345518564816675. Learning Rate: 1.8899850149850152e-05\n",
      "Batch 3,350 of 9,631 Elased 0:25:21. Training loss: 3.234886031684591. Learning Rate: 1.8882506382506385e-05\n",
      "Batch 3,400 of 9,631 Elased 0:25:43. Training loss: 3.235201131876777. Learning Rate: 1.8865162615162614e-05\n",
      "Batch 3,450 of 9,631 Elased 0:26:06. Training loss: 3.234890176869821. Learning Rate: 1.884781884781885e-05\n",
      "Batch 3,500 of 9,631 Elased 0:26:29. Training loss: 3.2343580600534168. Learning Rate: 1.8830475080475083e-05\n",
      "Batch 3,550 of 9,631 Elased 0:26:51. Training loss: 3.235535168882827. Learning Rate: 1.8813131313131312e-05\n",
      "Batch 3,600 of 9,631 Elased 0:27:14. Training loss: 3.2373675570554203. Learning Rate: 1.8795787545787548e-05\n",
      "Batch 3,650 of 9,631 Elased 0:27:36. Training loss: 3.2410020451676353. Learning Rate: 1.877844377844378e-05\n",
      "Batch 3,700 of 9,631 Elased 0:27:59. Training loss: 3.2401103382819407. Learning Rate: 1.876110001110001e-05\n",
      "Batch 3,750 of 9,631 Elased 0:28:21. Training loss: 3.239318192323049. Learning Rate: 1.8743756243756243e-05\n",
      "Batch 3,800 of 9,631 Elased 0:28:44. Training loss: 3.238302773017632. Learning Rate: 1.872641247641248e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3,850 of 9,631 Elased 0:29:06. Training loss: 3.2357755940610713. Learning Rate: 1.870906870906871e-05\n",
      "Batch 3,900 of 9,631 Elased 0:29:29. Training loss: 3.234629892599888. Learning Rate: 1.869172494172494e-05\n",
      "Batch 3,950 of 9,631 Elased 0:29:51. Training loss: 3.235641578843322. Learning Rate: 1.8674381174381177e-05\n",
      "Batch 4,000 of 9,631 Elased 0:30:14. Training loss: 3.2361679674386976. Learning Rate: 1.865703740703741e-05\n",
      "Batch 4,050 of 9,631 Elased 0:30:36. Training loss: 3.2357724130889514. Learning Rate: 1.863969363969364e-05\n",
      "Batch 4,100 of 9,631 Elased 0:30:59. Training loss: 3.2353510495511495. Learning Rate: 1.862234987234987e-05\n",
      "Batch 4,150 of 9,631 Elased 0:31:21. Training loss: 3.234749788778374. Learning Rate: 1.8605006105006107e-05\n",
      "Batch 4,200 of 9,631 Elased 0:31:44. Training loss: 3.2347468499910264. Learning Rate: 1.858766233766234e-05\n",
      "Batch 4,250 of 9,631 Elased 0:32:07. Training loss: 3.235180524601656. Learning Rate: 1.857031857031857e-05\n",
      "Batch 4,300 of 9,631 Elased 0:32:29. Training loss: 3.2346592795017153. Learning Rate: 1.8552974802974805e-05\n",
      "Batch 4,350 of 9,631 Elased 0:32:52. Training loss: 3.2341438217820793. Learning Rate: 1.8535631035631038e-05\n",
      "Batch 4,400 of 9,631 Elased 0:33:15. Training loss: 3.235518205003305. Learning Rate: 1.8518287268287267e-05\n",
      "Batch 4,450 of 9,631 Elased 0:33:37. Training loss: 3.235972630200761. Learning Rate: 1.8500943500943503e-05\n",
      "Batch 4,500 of 9,631 Elased 0:34:00. Training loss: 3.2371305730077955. Learning Rate: 1.8483599733599736e-05\n",
      "Batch 4,550 of 9,631 Elased 0:34:22. Training loss: 3.235537838595254. Learning Rate: 1.8466255966255965e-05\n",
      "Batch 4,600 of 9,631 Elased 0:34:45. Training loss: 3.2363823503774145. Learning Rate: 1.8448912198912198e-05\n",
      "Batch 4,650 of 9,631 Elased 0:35:08. Training loss: 3.235516755068174. Learning Rate: 1.8431568431568434e-05\n",
      "Batch 4,700 of 9,631 Elased 0:35:30. Training loss: 3.235809128715637. Learning Rate: 1.8414224664224667e-05\n",
      "Batch 4,750 of 9,631 Elased 0:35:53. Training loss: 3.236074562800558. Learning Rate: 1.8396880896880896e-05\n",
      "Batch 4,800 of 9,631 Elased 0:36:16. Training loss: 3.234846179013451. Learning Rate: 1.8379537129537132e-05\n",
      "Batch 4,850 of 9,631 Elased 0:36:38. Training loss: 3.2341822963154194. Learning Rate: 1.8362193362193364e-05\n",
      "Batch 4,900 of 9,631 Elased 0:37:00. Training loss: 3.2348433604289073. Learning Rate: 1.8344849594849594e-05\n",
      "Batch 4,950 of 9,631 Elased 0:37:23. Training loss: 3.2353032761149936. Learning Rate: 1.832750582750583e-05\n",
      "Batch 5,000 of 9,631 Elased 0:37:46. Training loss: 3.2364791332006453. Learning Rate: 1.8310162060162062e-05\n",
      "Batch 5,050 of 9,631 Elased 0:38:08. Training loss: 3.235858866200589. Learning Rate: 1.8292818292818295e-05\n",
      "Batch 5,100 of 9,631 Elased 0:38:31. Training loss: 3.2340589253575196. Learning Rate: 1.8275474525474524e-05\n",
      "Batch 5,150 of 9,631 Elased 0:38:54. Training loss: 3.2342037392588496. Learning Rate: 1.825813075813076e-05\n",
      "Batch 5,200 of 9,631 Elased 0:39:16. Training loss: 3.233938447420414. Learning Rate: 1.8240786990786993e-05\n",
      "Batch 5,250 of 9,631 Elased 0:39:39. Training loss: 3.2340046463012695. Learning Rate: 1.8223443223443222e-05\n",
      "Batch 5,300 of 9,631 Elased 0:40:02. Training loss: 3.233487969209563. Learning Rate: 1.820609945609946e-05\n",
      "Batch 5,350 of 9,631 Elased 0:40:24. Training loss: 3.2356450826876633. Learning Rate: 1.818875568875569e-05\n",
      "Batch 5,400 of 9,631 Elased 0:40:47. Training loss: 3.2358717798082917. Learning Rate: 1.817141192141192e-05\n",
      "Batch 5,450 of 9,631 Elased 0:41:09. Training loss: 3.2360229215928173. Learning Rate: 1.8154068154068153e-05\n",
      "Batch 5,500 of 9,631 Elased 0:41:32. Training loss: 3.2362233855724334. Learning Rate: 1.813672438672439e-05\n",
      "Batch 5,550 of 9,631 Elased 0:41:55. Training loss: 3.2370046240359813. Learning Rate: 1.811938061938062e-05\n",
      "Batch 5,600 of 9,631 Elased 0:42:17. Training loss: 3.236462941872222. Learning Rate: 1.810203685203685e-05\n",
      "Batch 5,650 of 9,631 Elased 0:42:40. Training loss: 3.2382129603149616. Learning Rate: 1.8084693084693087e-05\n",
      "Batch 5,700 of 9,631 Elased 0:43:02. Training loss: 3.2384070392867974. Learning Rate: 1.806734931734932e-05\n",
      "Batch 5,750 of 9,631 Elased 0:43:25. Training loss: 3.2382694416668105. Learning Rate: 1.805000555000555e-05\n",
      "Batch 5,800 of 9,631 Elased 0:43:48. Training loss: 3.238636820398528. Learning Rate: 1.8032661782661785e-05\n",
      "Batch 5,850 of 9,631 Elased 0:44:10. Training loss: 3.2390924892670068. Learning Rate: 1.8015318015318018e-05\n",
      "Batch 5,900 of 9,631 Elased 0:44:33. Training loss: 3.2392051964291073. Learning Rate: 1.7997974247974247e-05\n",
      "Batch 5,950 of 9,631 Elased 0:44:55. Training loss: 3.2393059743753. Learning Rate: 1.798063048063048e-05\n",
      "Batch 6,000 of 9,631 Elased 0:45:18. Training loss: 3.2390468618472417. Learning Rate: 1.7963286713286716e-05\n",
      "Batch 6,050 of 9,631 Elased 0:45:41. Training loss: 3.239396000440456. Learning Rate: 1.7945942945942948e-05\n",
      "Batch 6,100 of 9,631 Elased 0:46:03. Training loss: 3.2397412095304396. Learning Rate: 1.7928599178599177e-05\n",
      "Batch 6,150 of 9,631 Elased 0:46:26. Training loss: 3.239601215347042. Learning Rate: 1.7911255411255414e-05\n",
      "Batch 6,200 of 9,631 Elased 0:46:48. Training loss: 3.237817402308987. Learning Rate: 1.7893911643911646e-05\n",
      "Batch 6,250 of 9,631 Elased 0:47:11. Training loss: 3.237385687713623. Learning Rate: 1.7876567876567875e-05\n",
      "Batch 6,300 of 9,631 Elased 0:47:34. Training loss: 3.237216906339403. Learning Rate: 1.7859224109224108e-05\n",
      "Batch 6,350 of 9,631 Elased 0:47:56. Training loss: 3.2369709170521714. Learning Rate: 1.7841880341880344e-05\n",
      "Batch 6,400 of 9,631 Elased 0:48:19. Training loss: 3.23672016793862. Learning Rate: 1.7824536574536577e-05\n",
      "Batch 6,450 of 9,631 Elased 0:48:41. Training loss: 3.2375386300013047. Learning Rate: 1.7807192807192806e-05\n",
      "Batch 6,500 of 9,631 Elased 0:49:04. Training loss: 3.2361946001236257. Learning Rate: 1.7789849039849042e-05\n",
      "Batch 6,550 of 9,631 Elased 0:49:27. Training loss: 3.2364883678924037. Learning Rate: 1.7772505272505275e-05\n",
      "Batch 6,600 of 9,631 Elased 0:49:49. Training loss: 3.237084870826114. Learning Rate: 1.7755161505161504e-05\n",
      "Batch 6,650 of 9,631 Elased 0:50:12. Training loss: 3.2376145788601467. Learning Rate: 1.773781773781774e-05\n",
      "Batch 6,700 of 9,631 Elased 0:50:34. Training loss: 3.2385006216568732. Learning Rate: 1.7720473970473973e-05\n",
      "Batch 6,750 of 9,631 Elased 0:50:57. Training loss: 3.2384494124695107. Learning Rate: 1.7703130203130202e-05\n",
      "Batch 6,800 of 9,631 Elased 0:51:20. Training loss: 3.238537375383517. Learning Rate: 1.7685786435786435e-05\n",
      "Batch 6,850 of 9,631 Elased 0:51:42. Training loss: 3.237931055448351. Learning Rate: 1.766844266844267e-05\n",
      "Batch 6,900 of 9,631 Elased 0:52:05. Training loss: 3.2376095431265624. Learning Rate: 1.7651098901098903e-05\n",
      "Batch 6,950 of 9,631 Elased 0:52:28. Training loss: 3.23746203753588. Learning Rate: 1.7633755133755133e-05\n",
      "Batch 7,000 of 9,631 Elased 0:52:50. Training loss: 3.236868614384106. Learning Rate: 1.761641136641137e-05\n",
      "Batch 7,050 of 9,631 Elased 0:53:13. Training loss: 3.2361431543032326. Learning Rate: 1.75990675990676e-05\n",
      "Batch 7,100 of 9,631 Elased 0:53:36. Training loss: 3.2362199314715157. Learning Rate: 1.758172383172383e-05\n",
      "Batch 7,150 of 9,631 Elased 0:53:58. Training loss: 3.2361724347167917. Learning Rate: 1.7564380064380063e-05\n",
      "Batch 7,200 of 9,631 Elased 0:54:21. Training loss: 3.2362592876454195. Learning Rate: 1.75470362970363e-05\n",
      "Batch 7,250 of 9,631 Elased 0:54:43. Training loss: 3.235357262792258. Learning Rate: 1.7529692529692532e-05\n",
      "Batch 7,300 of 9,631 Elased 0:55:06. Training loss: 3.2359451512931146. Learning Rate: 1.751234876234876e-05\n",
      "Batch 7,350 of 9,631 Elased 0:55:28. Training loss: 3.235672927859689. Learning Rate: 1.7495004995004997e-05\n",
      "Batch 7,400 of 9,631 Elased 0:55:51. Training loss: 3.2344252005300005. Learning Rate: 1.747766122766123e-05\n",
      "Batch 7,450 of 9,631 Elased 0:56:14. Training loss: 3.2342442598758927. Learning Rate: 1.746031746031746e-05\n",
      "Batch 7,500 of 9,631 Elased 0:56:36. Training loss: 3.233398502747218. Learning Rate: 1.7442973692973695e-05\n",
      "Batch 7,550 of 9,631 Elased 0:56:58. Training loss: 3.2332579955202063. Learning Rate: 1.7425629925629928e-05\n",
      "Batch 7,600 of 9,631 Elased 0:57:21. Training loss: 3.233714263172526. Learning Rate: 1.7408286158286157e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7,650 of 9,631 Elased 0:57:44. Training loss: 3.233849714986639. Learning Rate: 1.739094239094239e-05\n",
      "Batch 7,700 of 9,631 Elased 0:58:06. Training loss: 3.23305348447391. Learning Rate: 1.7373598623598626e-05\n",
      "Batch 7,750 of 9,631 Elased 0:58:29. Training loss: 3.231451953934085. Learning Rate: 1.735625485625486e-05\n",
      "Batch 7,800 of 9,631 Elased 0:58:52. Training loss: 3.231994763658597. Learning Rate: 1.7338911088911088e-05\n",
      "Batch 7,850 of 9,631 Elased 0:59:14. Training loss: 3.232467349334887. Learning Rate: 1.7321567321567324e-05\n",
      "Batch 7,900 of 9,631 Elased 0:59:36. Training loss: 3.2313388222833224. Learning Rate: 1.7304223554223557e-05\n",
      "Batch 7,950 of 9,631 Elased 0:59:59. Training loss: 3.232647803849394. Learning Rate: 1.7286879786879786e-05\n",
      "Batch 8,000 of 9,631 Elased 1:00:22. Training loss: 3.232315565273166. Learning Rate: 1.726953601953602e-05\n",
      "Batch 8,050 of 9,631 Elased 1:00:44. Training loss: 3.233326691292828. Learning Rate: 1.7252192252192254e-05\n",
      "Batch 8,100 of 9,631 Elased 1:01:07. Training loss: 3.233588598466214. Learning Rate: 1.7234848484848487e-05\n",
      "Batch 8,150 of 9,631 Elased 1:01:29. Training loss: 3.234369586245414. Learning Rate: 1.7217504717504716e-05\n",
      "Batch 8,200 of 9,631 Elased 1:01:52. Training loss: 3.2341758270816103. Learning Rate: 1.7200160950160952e-05\n",
      "Batch 8,250 of 9,631 Elased 1:02:15. Training loss: 3.234782686883753. Learning Rate: 1.7182817182817185e-05\n",
      "Batch 8,300 of 9,631 Elased 1:02:37. Training loss: 3.235056508808251. Learning Rate: 1.7165473415473414e-05\n",
      "Batch 8,350 of 9,631 Elased 1:03:00. Training loss: 3.234904854882977. Learning Rate: 1.714812964812965e-05\n",
      "Batch 8,400 of 9,631 Elased 1:03:22. Training loss: 3.2340220138856344. Learning Rate: 1.7130785880785883e-05\n",
      "Batch 8,450 of 9,631 Elased 1:03:45. Training loss: 3.2332939134405914. Learning Rate: 1.7113442113442112e-05\n",
      "Batch 8,500 of 9,631 Elased 1:04:07. Training loss: 3.2329324895073386. Learning Rate: 1.7096098346098345e-05\n",
      "Batch 8,550 of 9,631 Elased 1:04:30. Training loss: 3.2325857479251616. Learning Rate: 1.707875457875458e-05\n",
      "Batch 8,600 of 9,631 Elased 1:04:52. Training loss: 3.2331809649356575. Learning Rate: 1.7061410811410814e-05\n",
      "Batch 8,650 of 9,631 Elased 1:05:15. Training loss: 3.2323188339079048. Learning Rate: 1.7044067044067043e-05\n",
      "Batch 8,700 of 9,631 Elased 1:05:38. Training loss: 3.2320522155432867. Learning Rate: 1.702672327672328e-05\n",
      "Batch 8,750 of 9,631 Elased 1:06:01. Training loss: 3.2317116280964444. Learning Rate: 1.700937950937951e-05\n",
      "Batch 8,800 of 9,631 Elased 1:06:23. Training loss: 3.231781136366454. Learning Rate: 1.699203574203574e-05\n",
      "Batch 8,850 of 9,631 Elased 1:06:45. Training loss: 3.2313577286402384. Learning Rate: 1.6974691974691974e-05\n",
      "Batch 8,900 of 9,631 Elased 1:07:08. Training loss: 3.2314026199030073. Learning Rate: 1.695734820734821e-05\n",
      "Batch 8,950 of 9,631 Elased 1:07:31. Training loss: 3.23098905347579. Learning Rate: 1.6940004440004442e-05\n",
      "Batch 9,000 of 9,631 Elased 1:07:53. Training loss: 3.229992335014873. Learning Rate: 1.692266067266067e-05\n",
      "Batch 9,050 of 9,631 Elased 1:08:16. Training loss: 3.230838757759958. Learning Rate: 1.6905316905316908e-05\n",
      "Batch 9,100 of 9,631 Elased 1:08:39. Training loss: 3.2307898123316714. Learning Rate: 1.688797313797314e-05\n",
      "Batch 9,150 of 9,631 Elased 1:09:01. Training loss: 3.230945880621509. Learning Rate: 1.687062937062937e-05\n",
      "Batch 9,200 of 9,631 Elased 1:09:24. Training loss: 3.231768998309322. Learning Rate: 1.6853285603285606e-05\n",
      "Batch 9,250 of 9,631 Elased 1:09:47. Training loss: 3.2313744618570484. Learning Rate: 1.6835941835941838e-05\n",
      "Batch 9,300 of 9,631 Elased 1:10:09. Training loss: 3.2320122118790944. Learning Rate: 1.6818598068598067e-05\n",
      "Batch 9,350 of 9,631 Elased 1:10:32. Training loss: 3.2319935384153684. Learning Rate: 1.68012543012543e-05\n",
      "Batch 9,400 of 9,631 Elased 1:10:54. Training loss: 3.231975196993097. Learning Rate: 1.6783910533910536e-05\n",
      "Batch 9,450 of 9,631 Elased 1:11:17. Training loss: 3.231892138347424. Learning Rate: 1.676656676656677e-05\n",
      "Batch 9,500 of 9,631 Elased 1:11:40. Training loss: 3.2315426237081226. Learning Rate: 1.6749222999222998e-05\n",
      "Batch 9,550 of 9,631 Elased 1:12:02. Training loss: 3.2310524465775616. Learning Rate: 1.6731879231879234e-05\n",
      "Batch 9,600 of 9,631 Elased 1:12:25. Training loss: 3.231569341160357. Learning Rate: 1.6714535464535467e-05\n",
      "\n",
      "\n",
      "  Average training loss: 3.23\n",
      "  Training epcoh took: 1:12:39\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66593eeffb9f4c628eefb4813e5cba93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=67.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Validation Loss: 3.41\n",
      "  Validation took: 0:00:45\n",
      "\n",
      "======== Epoch 26 / 30 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204d8b55b4e64f299ac98733f7d36983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch    50 of 9,631 Elased 0:00:23. Training loss: 3.3227658557891844. Learning Rate: 1.668643856143856e-05\n",
      "Batch   100 of 9,631 Elased 0:00:45. Training loss: 3.264635558128357. Learning Rate: 1.6669094794094794e-05\n",
      "Batch   150 of 9,631 Elased 0:01:08. Training loss: 3.267960205078125. Learning Rate: 1.6651751026751026e-05\n",
      "Batch   200 of 9,631 Elased 0:01:30. Training loss: 3.290643937587738. Learning Rate: 1.6634407259407262e-05\n",
      "Batch   250 of 9,631 Elased 0:01:53. Training loss: 3.2464710121154785. Learning Rate: 1.6617063492063492e-05\n",
      "Batch   300 of 9,631 Elased 0:02:16. Training loss: 3.235567187468211. Learning Rate: 1.6599719724719724e-05\n",
      "Batch   350 of 9,631 Elased 0:02:38. Training loss: 3.2309122889382498. Learning Rate: 1.658237595737596e-05\n",
      "Batch   400 of 9,631 Elased 0:03:01. Training loss: 3.235703544020653. Learning Rate: 1.656503219003219e-05\n",
      "Batch   450 of 9,631 Elased 0:03:23. Training loss: 3.2390798346201577. Learning Rate: 1.6547688422688422e-05\n",
      "Batch   500 of 9,631 Elased 0:03:46. Training loss: 3.2445018162727357. Learning Rate: 1.653034465534466e-05\n",
      "Batch   550 of 9,631 Elased 0:04:08. Training loss: 3.2310226345062256. Learning Rate: 1.651300088800089e-05\n",
      "Batch   600 of 9,631 Elased 0:04:31. Training loss: 3.2468045222759248. Learning Rate: 1.649565712065712e-05\n",
      "Batch   650 of 9,631 Elased 0:04:53. Training loss: 3.247160488275381. Learning Rate: 1.6478313353313353e-05\n",
      "Batch   700 of 9,631 Elased 0:05:16. Training loss: 3.242487076691219. Learning Rate: 1.646096958596959e-05\n",
      "Batch   750 of 9,631 Elased 0:05:39. Training loss: 3.2296643619537355. Learning Rate: 1.6443625818625818e-05\n",
      "Batch   800 of 9,631 Elased 0:06:01. Training loss: 3.2261785359680655. Learning Rate: 1.642628205128205e-05\n",
      "Batch   850 of 9,631 Elased 0:06:24. Training loss: 3.2192351412773133. Learning Rate: 1.6408938283938287e-05\n",
      "Batch   900 of 9,631 Elased 0:06:47. Training loss: 3.217898406320148. Learning Rate: 1.6391594516594516e-05\n",
      "Batch   950 of 9,631 Elased 0:07:09. Training loss: 3.2136217108525726. Learning Rate: 1.637425074925075e-05\n",
      "Batch 1,000 of 9,631 Elased 0:07:32. Training loss: 3.2155816727876663. Learning Rate: 1.635690698190698e-05\n",
      "Batch 1,050 of 9,631 Elased 0:07:54. Training loss: 3.218348442826952. Learning Rate: 1.6339563214563218e-05\n",
      "Batch 1,100 of 9,631 Elased 0:08:17. Training loss: 3.220524831793525. Learning Rate: 1.6322219447219447e-05\n",
      "Batch 1,150 of 9,631 Elased 0:08:39. Training loss: 3.2204628184567325. Learning Rate: 1.630487567987568e-05\n",
      "Batch 1,200 of 9,631 Elased 0:09:02. Training loss: 3.222863299846649. Learning Rate: 1.6287531912531916e-05\n",
      "Batch 1,250 of 9,631 Elased 0:09:24. Training loss: 3.222712156009674. Learning Rate: 1.6270188145188145e-05\n",
      "Batch 1,300 of 9,631 Elased 0:09:47. Training loss: 3.2176316676690027. Learning Rate: 1.6252844377844377e-05\n",
      "Batch 1,350 of 9,631 Elased 0:10:09. Training loss: 3.2170509946787798. Learning Rate: 1.6235500610500614e-05\n",
      "Batch 1,400 of 9,631 Elased 0:10:32. Training loss: 3.216535030858857. Learning Rate: 1.6218156843156843e-05\n",
      "Batch 1,450 of 9,631 Elased 0:10:55. Training loss: 3.2149067705253076. Learning Rate: 1.6200813075813075e-05\n",
      "Batch 1,500 of 9,631 Elased 0:11:18. Training loss: 3.2124736028512317. Learning Rate: 1.6183469308469308e-05\n",
      "Batch 1,550 of 9,631 Elased 0:11:40. Training loss: 3.2119242783515682. Learning Rate: 1.6166125541125544e-05\n",
      "Batch 1,600 of 9,631 Elased 0:12:03. Training loss: 3.2173131787776947. Learning Rate: 1.6148781773781773e-05\n",
      "Batch 1,650 of 9,631 Elased 0:12:25. Training loss: 3.2199720041679614. Learning Rate: 1.6131438006438006e-05\n",
      "Batch 1,700 of 9,631 Elased 0:12:48. Training loss: 3.2168547722171335. Learning Rate: 1.6114094239094242e-05\n",
      "Batch 1,750 of 9,631 Elased 0:13:11. Training loss: 3.2169243394987923. Learning Rate: 1.609675047175047e-05\n",
      "Batch 1,800 of 9,631 Elased 0:13:33. Training loss: 3.216250761018859. Learning Rate: 1.6079406704406704e-05\n",
      "Batch 1,850 of 9,631 Elased 0:13:56. Training loss: 3.218382432718535. Learning Rate: 1.6062062937062937e-05\n",
      "Batch 1,900 of 9,631 Elased 0:14:18. Training loss: 3.2177572529566913. Learning Rate: 1.6044719169719173e-05\n",
      "Batch 1,950 of 9,631 Elased 0:14:41. Training loss: 3.214707812223679. Learning Rate: 1.6027375402375402e-05\n",
      "Batch 2,000 of 9,631 Elased 0:15:04. Training loss: 3.2113694812059403. Learning Rate: 1.6010031635031635e-05\n",
      "Batch 2,050 of 9,631 Elased 0:15:26. Training loss: 3.2121843610158782. Learning Rate: 1.599268786768787e-05\n",
      "Batch 2,100 of 9,631 Elased 0:15:49. Training loss: 3.215887856369927. Learning Rate: 1.59753441003441e-05\n",
      "Batch 2,150 of 9,631 Elased 0:16:12. Training loss: 3.217096071797748. Learning Rate: 1.5958000333000333e-05\n",
      "Batch 2,200 of 9,631 Elased 0:16:35. Training loss: 3.216407198038968. Learning Rate: 1.594065656565657e-05\n",
      "Batch 2,250 of 9,631 Elased 0:16:57. Training loss: 3.213956967830658. Learning Rate: 1.5923312798312798e-05\n",
      "Batch 2,300 of 9,631 Elased 0:17:20. Training loss: 3.215531070802523. Learning Rate: 1.590596903096903e-05\n",
      "Batch 2,350 of 9,631 Elased 0:17:43. Training loss: 3.213549428696328. Learning Rate: 1.5888625263625263e-05\n",
      "Batch 2,400 of 9,631 Elased 0:18:05. Training loss: 3.2136282804608345. Learning Rate: 1.58712814962815e-05\n",
      "Batch 2,450 of 9,631 Elased 0:18:28. Training loss: 3.210330509847524. Learning Rate: 1.585393772893773e-05\n",
      "Batch 2,500 of 9,631 Elased 0:18:50. Training loss: 3.2089081615448. Learning Rate: 1.583659396159396e-05\n",
      "Batch 2,550 of 9,631 Elased 0:19:13. Training loss: 3.214013207286012. Learning Rate: 1.5819250194250197e-05\n",
      "Batch 2,600 of 9,631 Elased 0:19:35. Training loss: 3.2169090894552377. Learning Rate: 1.5801906426906427e-05\n",
      "Batch 2,650 of 9,631 Elased 0:19:58. Training loss: 3.219449084569823. Learning Rate: 1.578456265956266e-05\n",
      "Batch 2,700 of 9,631 Elased 0:20:21. Training loss: 3.2214746543213173. Learning Rate: 1.5767218892218892e-05\n",
      "Batch 2,750 of 9,631 Elased 0:20:44. Training loss: 3.2193264382102273. Learning Rate: 1.5749875124875128e-05\n",
      "Batch 2,800 of 9,631 Elased 0:21:06. Training loss: 3.2203014392512186. Learning Rate: 1.5732531357531357e-05\n",
      "Batch 2,850 of 9,631 Elased 0:21:29. Training loss: 3.2234995619455975. Learning Rate: 1.571518759018759e-05\n",
      "Batch 2,900 of 9,631 Elased 0:21:51. Training loss: 3.2232798790109567. Learning Rate: 1.5697843822843826e-05\n",
      "Batch 2,950 of 9,631 Elased 0:22:14. Training loss: 3.2237815515065598. Learning Rate: 1.5680500055500055e-05\n",
      "Batch 3,000 of 9,631 Elased 0:22:36. Training loss: 3.2268769951661427. Learning Rate: 1.5663156288156288e-05\n",
      "Batch 3,050 of 9,631 Elased 0:22:59. Training loss: 3.226175681919348. Learning Rate: 1.5645812520812524e-05\n",
      "Batch 3,100 of 9,631 Elased 0:23:22. Training loss: 3.226652038366564. Learning Rate: 1.5628468753468753e-05\n",
      "Batch 3,150 of 9,631 Elased 0:23:45. Training loss: 3.2231333563441322. Learning Rate: 1.5611124986124986e-05\n",
      "Batch 3,200 of 9,631 Elased 0:24:07. Training loss: 3.2252516208216546. Learning Rate: 1.559378121878122e-05\n",
      "Batch 3,250 of 9,631 Elased 0:24:30. Training loss: 3.2243647623428933. Learning Rate: 1.5576437451437454e-05\n",
      "Batch 3,300 of 9,631 Elased 0:24:52. Training loss: 3.227330079909527. Learning Rate: 1.5559093684093684e-05\n",
      "Batch 3,350 of 9,631 Elased 0:25:15. Training loss: 3.2283264247339165. Learning Rate: 1.5541749916749916e-05\n",
      "Batch 3,400 of 9,631 Elased 0:25:38. Training loss: 3.2281939355415457. Learning Rate: 1.5524406149406152e-05\n",
      "Batch 3,450 of 9,631 Elased 0:26:01. Training loss: 3.228606396654378. Learning Rate: 1.5507062382062382e-05\n",
      "Batch 3,500 of 9,631 Elased 0:26:23. Training loss: 3.227945270708629. Learning Rate: 1.5489718614718614e-05\n",
      "Batch 3,550 of 9,631 Elased 0:26:46. Training loss: 3.22921326613762. Learning Rate: 1.5472374847374847e-05\n",
      "Batch 3,600 of 9,631 Elased 0:27:08. Training loss: 3.231005846162637. Learning Rate: 1.5455031080031083e-05\n",
      "Batch 3,650 of 9,631 Elased 0:27:31. Training loss: 3.2343646643586355. Learning Rate: 1.5437687312687312e-05\n",
      "Batch 3,700 of 9,631 Elased 0:27:54. Training loss: 3.233774424662461. Learning Rate: 1.5420343545343545e-05\n",
      "Batch 3,750 of 9,631 Elased 0:28:16. Training loss: 3.2323341351509094. Learning Rate: 1.540299977799978e-05\n",
      "Batch 3,800 of 9,631 Elased 0:28:39. Training loss: 3.232049611022598. Learning Rate: 1.538565601065601e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3,850 of 9,631 Elased 0:29:02. Training loss: 3.2298808383012747. Learning Rate: 1.5368312243312243e-05\n",
      "Batch 3,900 of 9,631 Elased 0:29:24. Training loss: 3.2287920861060804. Learning Rate: 1.535096847596848e-05\n",
      "Batch 3,950 of 9,631 Elased 0:29:47. Training loss: 3.2298232040224195. Learning Rate: 1.5333624708624708e-05\n",
      "Batch 4,000 of 9,631 Elased 0:30:10. Training loss: 3.2304218938052656. Learning Rate: 1.531628094128094e-05\n",
      "Batch 4,050 of 9,631 Elased 0:30:32. Training loss: 3.229617580337289. Learning Rate: 1.5298937173937174e-05\n",
      "Batch 4,100 of 9,631 Elased 0:30:55. Training loss: 3.2289349782466887. Learning Rate: 1.528159340659341e-05\n",
      "Batch 4,150 of 9,631 Elased 0:31:17. Training loss: 3.228477508389806. Learning Rate: 1.526424963924964e-05\n",
      "Batch 4,200 of 9,631 Elased 0:31:40. Training loss: 3.228416783156849. Learning Rate: 1.5246905871905873e-05\n",
      "Batch 4,250 of 9,631 Elased 0:32:03. Training loss: 3.228745787901037. Learning Rate: 1.5229562104562106e-05\n",
      "Batch 4,300 of 9,631 Elased 0:32:27. Training loss: 3.2283243243638857. Learning Rate: 1.5212218337218337e-05\n",
      "Batch 4,350 of 9,631 Elased 0:32:50. Training loss: 3.228351427659221. Learning Rate: 1.519487456987457e-05\n",
      "Batch 4,400 of 9,631 Elased 0:33:14. Training loss: 3.2295346908135847. Learning Rate: 1.5177530802530804e-05\n",
      "Batch 4,450 of 9,631 Elased 0:33:37. Training loss: 3.2299631183067063. Learning Rate: 1.5160187035187035e-05\n",
      "Batch 4,500 of 9,631 Elased 0:34:00. Training loss: 3.230931717740165. Learning Rate: 1.5142843267843267e-05\n",
      "Batch 4,550 of 9,631 Elased 0:34:24. Training loss: 3.2294430676397385. Learning Rate: 1.5125499500499502e-05\n",
      "Batch 4,600 of 9,631 Elased 0:34:48. Training loss: 3.230737553394359. Learning Rate: 1.5108155733155734e-05\n",
      "Batch 4,650 of 9,631 Elased 0:35:10. Training loss: 3.2301904776532164. Learning Rate: 1.5090811965811965e-05\n",
      "Batch 4,700 of 9,631 Elased 0:35:33. Training loss: 3.23023316715626. Learning Rate: 1.50734681984682e-05\n",
      "Batch 4,750 of 9,631 Elased 0:35:56. Training loss: 3.2306679405664145. Learning Rate: 1.5056124431124432e-05\n",
      "Batch 4,800 of 9,631 Elased 0:36:19. Training loss: 3.2292081329474844. Learning Rate: 1.5038780663780663e-05\n",
      "Batch 4,850 of 9,631 Elased 0:36:41. Training loss: 3.2285871585128234. Learning Rate: 1.5021436896436896e-05\n",
      "Batch 4,900 of 9,631 Elased 0:37:04. Training loss: 3.2293971845811726. Learning Rate: 1.500409312909313e-05\n",
      "Batch 4,950 of 9,631 Elased 0:37:27. Training loss: 3.2297924555913364. Learning Rate: 1.4986749361749363e-05\n",
      "Batch 5,000 of 9,631 Elased 0:37:49. Training loss: 3.2306270553350447. Learning Rate: 1.4969405594405594e-05\n",
      "Batch 5,050 of 9,631 Elased 0:38:12. Training loss: 3.2301945376396177. Learning Rate: 1.4952061827061828e-05\n",
      "Batch 5,100 of 9,631 Elased 0:38:34. Training loss: 3.2285986442659413. Learning Rate: 1.4934718059718061e-05\n",
      "Batch 5,150 of 9,631 Elased 0:38:57. Training loss: 3.2288425888598544. Learning Rate: 1.4917374292374292e-05\n",
      "Batch 5,200 of 9,631 Elased 0:39:19. Training loss: 3.2284368277054565. Learning Rate: 1.4900030525030526e-05\n",
      "Batch 5,250 of 9,631 Elased 0:39:42. Training loss: 3.2284088285536994. Learning Rate: 1.4882686757686759e-05\n",
      "Batch 5,300 of 9,631 Elased 0:40:05. Training loss: 3.2281010484245587. Learning Rate: 1.486534299034299e-05\n",
      "Batch 5,350 of 9,631 Elased 0:40:27. Training loss: 3.2302571965823663. Learning Rate: 1.4847999222999223e-05\n",
      "Batch 5,400 of 9,631 Elased 0:40:50. Training loss: 3.230428559316529. Learning Rate: 1.4830655455655457e-05\n",
      "Batch 5,450 of 9,631 Elased 0:41:13. Training loss: 3.23056949464553. Learning Rate: 1.481331168831169e-05\n",
      "Batch 5,500 of 9,631 Elased 0:41:35. Training loss: 3.230725308700041. Learning Rate: 1.479596792096792e-05\n",
      "Batch 5,550 of 9,631 Elased 0:41:58. Training loss: 3.231243821801366. Learning Rate: 1.4778624153624155e-05\n",
      "Batch 5,600 of 9,631 Elased 0:42:20. Training loss: 3.230739715737956. Learning Rate: 1.4761280386280388e-05\n",
      "Batch 5,650 of 9,631 Elased 0:42:43. Training loss: 3.23273440234429. Learning Rate: 1.4743936618936619e-05\n",
      "Batch 5,700 of 9,631 Elased 0:43:06. Training loss: 3.2324396755402547. Learning Rate: 1.4726592851592851e-05\n",
      "Batch 5,750 of 9,631 Elased 0:43:28. Training loss: 3.23198304271698. Learning Rate: 1.4709249084249086e-05\n",
      "Batch 5,800 of 9,631 Elased 0:43:51. Training loss: 3.232342885198264. Learning Rate: 1.4691905316905318e-05\n",
      "Batch 5,850 of 9,631 Elased 0:44:14. Training loss: 3.232852315780444. Learning Rate: 1.467456154956155e-05\n",
      "Batch 5,900 of 9,631 Elased 0:44:36. Training loss: 3.23269591315318. Learning Rate: 1.4657217782217784e-05\n",
      "Batch 5,950 of 9,631 Elased 0:44:58. Training loss: 3.2332820726442737. Learning Rate: 1.4639874014874016e-05\n",
      "Batch 6,000 of 9,631 Elased 0:45:21. Training loss: 3.233061747908592. Learning Rate: 1.4622530247530247e-05\n",
      "Batch 6,050 of 9,631 Elased 0:45:44. Training loss: 3.233304726978964. Learning Rate: 1.4605186480186482e-05\n",
      "Batch 6,100 of 9,631 Elased 0:46:06. Training loss: 3.2338267770751576. Learning Rate: 1.4587842712842714e-05\n",
      "Batch 6,150 of 9,631 Elased 0:46:29. Training loss: 3.233780148320082. Learning Rate: 1.4570498945498945e-05\n",
      "Batch 6,200 of 9,631 Elased 0:46:52. Training loss: 3.2318405686270806. Learning Rate: 1.4553155178155178e-05\n",
      "Batch 6,250 of 9,631 Elased 0:47:14. Training loss: 3.231501412086487. Learning Rate: 1.4535811410811412e-05\n",
      "Batch 6,300 of 9,631 Elased 0:47:37. Training loss: 3.2314675101401313. Learning Rate: 1.4518467643467645e-05\n",
      "Batch 6,350 of 9,631 Elased 0:47:59. Training loss: 3.2312476490050788. Learning Rate: 1.4501123876123876e-05\n",
      "Batch 6,400 of 9,631 Elased 0:48:22. Training loss: 3.230710014626384. Learning Rate: 1.448378010878011e-05\n",
      "Batch 6,450 of 9,631 Elased 0:48:44. Training loss: 3.2319269294886626. Learning Rate: 1.4466436341436343e-05\n",
      "Batch 6,500 of 9,631 Elased 0:49:07. Training loss: 3.230366499387301. Learning Rate: 1.4449092574092574e-05\n",
      "Batch 6,550 of 9,631 Elased 0:49:30. Training loss: 3.2308914699627245. Learning Rate: 1.4431748806748806e-05\n",
      "Batch 6,600 of 9,631 Elased 0:49:52. Training loss: 3.231422462553689. Learning Rate: 1.441440503940504e-05\n",
      "Batch 6,650 of 9,631 Elased 0:50:15. Training loss: 3.231861787254649. Learning Rate: 1.4397061272061273e-05\n",
      "Batch 6,700 of 9,631 Elased 0:50:38. Training loss: 3.2327863490225663. Learning Rate: 1.4379717504717504e-05\n",
      "Batch 6,750 of 9,631 Elased 0:51:00. Training loss: 3.2328337819664568. Learning Rate: 1.4362373737373739e-05\n",
      "Batch 6,800 of 9,631 Elased 0:51:23. Training loss: 3.2326817515141824. Learning Rate: 1.4345029970029971e-05\n",
      "Batch 6,850 of 9,631 Elased 0:51:45. Training loss: 3.232247443738645. Learning Rate: 1.4327686202686202e-05\n",
      "Batch 6,900 of 9,631 Elased 0:52:08. Training loss: 3.2317390495625093. Learning Rate: 1.4310342435342437e-05\n",
      "Batch 6,950 of 9,631 Elased 0:52:31. Training loss: 3.231544284872014. Learning Rate: 1.429299866799867e-05\n",
      "Batch 7,000 of 9,631 Elased 0:52:53. Training loss: 3.2309337360007424. Learning Rate: 1.42756549006549e-05\n",
      "Batch 7,050 of 9,631 Elased 0:53:16. Training loss: 3.2302969292376904. Learning Rate: 1.4258311133311133e-05\n",
      "Batch 7,100 of 9,631 Elased 0:53:38. Training loss: 3.2305937238478326. Learning Rate: 1.4240967365967367e-05\n",
      "Batch 7,150 of 9,631 Elased 0:54:01. Training loss: 3.2306233131301987. Learning Rate: 1.42236235986236e-05\n",
      "Batch 7,200 of 9,631 Elased 0:54:23. Training loss: 3.2305680201285414. Learning Rate: 1.4206279831279831e-05\n",
      "Batch 7,250 of 9,631 Elased 0:54:46. Training loss: 3.2296893481386117. Learning Rate: 1.4188936063936065e-05\n",
      "Batch 7,300 of 9,631 Elased 0:55:09. Training loss: 3.229998151815101. Learning Rate: 1.4171592296592298e-05\n",
      "Batch 7,350 of 9,631 Elased 0:55:31. Training loss: 3.229777793576117. Learning Rate: 1.4154248529248529e-05\n",
      "Batch 7,400 of 9,631 Elased 0:55:54. Training loss: 3.228496703859922. Learning Rate: 1.4136904761904762e-05\n",
      "Batch 7,450 of 9,631 Elased 0:56:17. Training loss: 3.2283912258820244. Learning Rate: 1.4119560994560996e-05\n",
      "Batch 7,500 of 9,631 Elased 0:56:39. Training loss: 3.227402382262548. Learning Rate: 1.4102217227217229e-05\n",
      "Batch 7,550 of 9,631 Elased 0:57:02. Training loss: 3.2271767164539815. Learning Rate: 1.408487345987346e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7,600 of 9,631 Elased 0:57:24. Training loss: 3.2274406900060804. Learning Rate: 1.4067529692529694e-05\n",
      "Batch 7,650 of 9,631 Elased 0:57:47. Training loss: 3.227556150863373. Learning Rate: 1.4050185925185927e-05\n",
      "Batch 7,700 of 9,631 Elased 0:58:10. Training loss: 3.226655886173248. Learning Rate: 1.4032842157842157e-05\n",
      "Batch 7,750 of 9,631 Elased 0:58:32. Training loss: 3.2251717576672956. Learning Rate: 1.4015498390498392e-05\n",
      "Batch 7,800 of 9,631 Elased 0:58:55. Training loss: 3.2257150631073195. Learning Rate: 1.3998154623154624e-05\n",
      "Batch 7,850 of 9,631 Elased 0:59:17. Training loss: 3.2262195129758995. Learning Rate: 1.3980810855810855e-05\n",
      "Batch 7,900 of 9,631 Elased 0:59:40. Training loss: 3.225071376547029. Learning Rate: 1.3963467088467088e-05\n",
      "Batch 7,950 of 9,631 Elased 1:00:03. Training loss: 3.226265084008751. Learning Rate: 1.3946123321123322e-05\n",
      "Batch 8,000 of 9,631 Elased 1:00:25. Training loss: 3.2259872035384176. Learning Rate: 1.3928779553779555e-05\n",
      "Batch 8,050 of 9,631 Elased 1:00:48. Training loss: 3.2269980908624873. Learning Rate: 1.3911435786435786e-05\n",
      "Batch 8,100 of 9,631 Elased 1:01:10. Training loss: 3.2269985516571706. Learning Rate: 1.389409201909202e-05\n",
      "Batch 8,150 of 9,631 Elased 1:01:33. Training loss: 3.2278027394651634. Learning Rate: 1.3876748251748253e-05\n",
      "Batch 8,200 of 9,631 Elased 1:01:55. Training loss: 3.227525392480013. Learning Rate: 1.3859404484404484e-05\n",
      "Batch 8,250 of 9,631 Elased 1:02:18. Training loss: 3.227983068263892. Learning Rate: 1.3842060717060717e-05\n",
      "Batch 8,300 of 9,631 Elased 1:02:41. Training loss: 3.2281463863763467. Learning Rate: 1.3824716949716951e-05\n",
      "Batch 8,350 of 9,631 Elased 1:03:02. Training loss: 3.227944036843534. Learning Rate: 1.3807373182373182e-05\n",
      "Batch 8,400 of 9,631 Elased 1:03:24. Training loss: 3.2270055104153497. Learning Rate: 1.3790029415029415e-05\n",
      "Batch 8,450 of 9,631 Elased 1:03:45. Training loss: 3.2263126875239716. Learning Rate: 1.3772685647685649e-05\n",
      "Batch 8,500 of 9,631 Elased 1:04:06. Training loss: 3.2259903805396135. Learning Rate: 1.3755341880341882e-05\n",
      "Batch 8,550 of 9,631 Elased 1:04:28. Training loss: 3.2257400051473875. Learning Rate: 1.3737998112998113e-05\n",
      "Batch 8,600 of 9,631 Elased 1:04:49. Training loss: 3.226431676953338. Learning Rate: 1.3720654345654347e-05\n",
      "Batch 8,650 of 9,631 Elased 1:05:10. Training loss: 3.2254962130640283. Learning Rate: 1.370331057831058e-05\n",
      "Batch 8,700 of 9,631 Elased 1:05:32. Training loss: 3.2253243483620126. Learning Rate: 1.368596681096681e-05\n",
      "Batch 8,750 of 9,631 Elased 1:05:53. Training loss: 3.225090776225499. Learning Rate: 1.3668623043623043e-05\n",
      "Batch 8,800 of 9,631 Elased 1:06:14. Training loss: 3.2248472978852014. Learning Rate: 1.3651279276279278e-05\n",
      "Batch 8,850 of 9,631 Elased 1:06:36. Training loss: 3.2245822734078446. Learning Rate: 1.363393550893551e-05\n",
      "Batch 8,900 of 9,631 Elased 1:06:57. Training loss: 3.2246993507963886. Learning Rate: 1.3616591741591741e-05\n",
      "Batch 8,950 of 9,631 Elased 1:07:19. Training loss: 3.2242529071775894. Learning Rate: 1.3599247974247976e-05\n",
      "Batch 9,000 of 9,631 Elased 1:07:40. Training loss: 3.2234381060335373. Learning Rate: 1.3581904206904208e-05\n",
      "Batch 9,050 of 9,631 Elased 1:08:02. Training loss: 3.224330348178168. Learning Rate: 1.356456043956044e-05\n",
      "Batch 9,100 of 9,631 Elased 1:08:23. Training loss: 3.224157323313283. Learning Rate: 1.3547216672216672e-05\n",
      "Batch 9,150 of 9,631 Elased 1:08:44. Training loss: 3.2247264263538713. Learning Rate: 1.3529872904872906e-05\n",
      "Batch 9,200 of 9,631 Elased 1:09:05. Training loss: 3.2252806088717088. Learning Rate: 1.3512529137529137e-05\n",
      "Batch 9,250 of 9,631 Elased 1:09:26. Training loss: 3.2248995357204127. Learning Rate: 1.349518537018537e-05\n",
      "Batch 9,300 of 9,631 Elased 1:09:47. Training loss: 3.2252533794987586. Learning Rate: 1.3477841602841604e-05\n",
      "Batch 9,350 of 9,631 Elased 1:10:08. Training loss: 3.2253238760596292. Learning Rate: 1.3460497835497837e-05\n",
      "Batch 9,400 of 9,631 Elased 1:10:29. Training loss: 3.225190363412208. Learning Rate: 1.3443154068154068e-05\n",
      "Batch 9,450 of 9,631 Elased 1:10:50. Training loss: 3.225147130602882. Learning Rate: 1.3425810300810302e-05\n",
      "Batch 9,500 of 9,631 Elased 1:11:12. Training loss: 3.2247803628319187. Learning Rate: 1.3408466533466535e-05\n",
      "Batch 9,550 of 9,631 Elased 1:11:33. Training loss: 3.2241477939346073. Learning Rate: 1.3391122766122766e-05\n",
      "Batch 9,600 of 9,631 Elased 1:11:54. Training loss: 3.2247081625213228. Learning Rate: 1.3373778998778998e-05\n",
      "\n",
      "\n",
      "  Average training loss: 3.22\n",
      "  Training epcoh took: 1:12:07\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5542145eec2842e3b52b17198cf9284b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=67.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Validation Loss: 3.41\n",
      "  Validation took: 0:00:43\n",
      "\n",
      "======== Epoch 27 / 30 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c42b77c136ab4b00b45dad138cc91bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch    50 of 9,631 Elased 0:00:21. Training loss: 3.3002851486206053. Learning Rate: 1.3345682095682096e-05\n",
      "Batch   100 of 9,631 Elased 0:00:43. Training loss: 3.247886388301849. Learning Rate: 1.332833832833833e-05\n",
      "Batch   150 of 9,631 Elased 0:01:04. Training loss: 3.2518694670995076. Learning Rate: 1.3310994560994561e-05\n",
      "Batch   200 of 9,631 Elased 0:01:25. Training loss: 3.27808055639267. Learning Rate: 1.3293650793650794e-05\n",
      "Batch   250 of 9,631 Elased 0:01:47. Training loss: 3.23276779794693. Learning Rate: 1.3276307026307028e-05\n",
      "Batch   300 of 9,631 Elased 0:02:08. Training loss: 3.220919586420059. Learning Rate: 1.3258963258963258e-05\n",
      "Batch   350 of 9,631 Elased 0:02:29. Training loss: 3.2166263604164125. Learning Rate: 1.3241619491619492e-05\n",
      "Batch   400 of 9,631 Elased 0:02:50. Training loss: 3.2276259103417395. Learning Rate: 1.3224275724275725e-05\n",
      "Batch   450 of 9,631 Elased 0:03:11. Training loss: 3.232703510655297. Learning Rate: 1.3206931956931959e-05\n",
      "Batch   500 of 9,631 Elased 0:03:32. Training loss: 3.2393115680217743. Learning Rate: 1.318958818958819e-05\n",
      "Batch   550 of 9,631 Elased 0:03:54. Training loss: 3.2259140103513544. Learning Rate: 1.3172244422244423e-05\n",
      "Batch   600 of 9,631 Elased 0:04:15. Training loss: 3.2390203525622687. Learning Rate: 1.3154900654900657e-05\n",
      "Batch   650 of 9,631 Elased 0:04:36. Training loss: 3.240127882040464. Learning Rate: 1.3137556887556888e-05\n",
      "Batch   700 of 9,631 Elased 0:04:58. Training loss: 3.2351519078867774. Learning Rate: 1.312021312021312e-05\n",
      "Batch   750 of 9,631 Elased 0:05:19. Training loss: 3.2228793161710105. Learning Rate: 1.3102869352869355e-05\n",
      "Batch   800 of 9,631 Elased 0:05:40. Training loss: 3.2223432518541815. Learning Rate: 1.3085525585525584e-05\n",
      "Batch   850 of 9,631 Elased 0:06:01. Training loss: 3.2164232833245223. Learning Rate: 1.3068181818181819e-05\n",
      "Batch   900 of 9,631 Elased 0:06:22. Training loss: 3.2145340457227496. Learning Rate: 1.3050838050838051e-05\n",
      "Batch   950 of 9,631 Elased 0:06:42. Training loss: 3.21114809801704. Learning Rate: 1.3033494283494286e-05\n",
      "Batch 1,000 of 9,631 Elased 0:07:03. Training loss: 3.2125662239789965. Learning Rate: 1.3016150516150517e-05\n",
      "Batch 1,050 of 9,631 Elased 0:07:25. Training loss: 3.2138486945061455. Learning Rate: 1.299880674880675e-05\n",
      "Batch 1,100 of 9,631 Elased 0:07:46. Training loss: 3.215385327881033. Learning Rate: 1.2981462981462984e-05\n",
      "Batch 1,150 of 9,631 Elased 0:08:07. Training loss: 3.214928985574971. Learning Rate: 1.2964119214119213e-05\n",
      "Batch 1,200 of 9,631 Elased 0:08:29. Training loss: 3.214861392279466. Learning Rate: 1.2946775446775447e-05\n",
      "Batch 1,250 of 9,631 Elased 0:08:50. Training loss: 3.216917647075653. Learning Rate: 1.292943167943168e-05\n",
      "Batch 1,300 of 9,631 Elased 0:09:11. Training loss: 3.2125981625226827. Learning Rate: 1.2912087912087914e-05\n",
      "Batch 1,350 of 9,631 Elased 0:09:33. Training loss: 3.2122428250312804. Learning Rate: 1.2894744144744145e-05\n",
      "Batch 1,400 of 9,631 Elased 0:09:54. Training loss: 3.2112128907442092. Learning Rate: 1.2877400377400378e-05\n",
      "Batch 1,450 of 9,631 Elased 0:10:16. Training loss: 3.2092173761334912. Learning Rate: 1.2860056610056612e-05\n",
      "Batch 1,500 of 9,631 Elased 0:10:37. Training loss: 3.2083651961485544. Learning Rate: 1.2842712842712843e-05\n",
      "Batch 1,550 of 9,631 Elased 0:10:58. Training loss: 3.2075170648482536. Learning Rate: 1.2825369075369076e-05\n",
      "Batch 1,600 of 9,631 Elased 0:11:20. Training loss: 3.2131365352123975. Learning Rate: 1.280802530802531e-05\n",
      "Batch 1,650 of 9,631 Elased 0:11:41. Training loss: 3.2164123544548495. Learning Rate: 1.279068154068154e-05\n",
      "Batch 1,700 of 9,631 Elased 0:12:03. Training loss: 3.2136298362647784. Learning Rate: 1.2773337773337774e-05\n",
      "Batch 1,750 of 9,631 Elased 0:12:24. Training loss: 3.214144958087376. Learning Rate: 1.2755994005994006e-05\n",
      "Batch 1,800 of 9,631 Elased 0:12:46. Training loss: 3.2133164425690968. Learning Rate: 1.273865023865024e-05\n",
      "Batch 1,850 of 9,631 Elased 0:13:07. Training loss: 3.2158965577305976. Learning Rate: 1.2721306471306472e-05\n",
      "Batch 1,900 of 9,631 Elased 0:13:28. Training loss: 3.2158810201444123. Learning Rate: 1.2703962703962704e-05\n",
      "Batch 1,950 of 9,631 Elased 0:13:49. Training loss: 3.21237855727856. Learning Rate: 1.2686618936618939e-05\n",
      "Batch 2,000 of 9,631 Elased 0:14:11. Training loss: 3.2093767945766447. Learning Rate: 1.2669275169275168e-05\n",
      "Batch 2,050 of 9,631 Elased 0:14:32. Training loss: 3.2102929511884364. Learning Rate: 1.2651931401931402e-05\n",
      "Batch 2,100 of 9,631 Elased 0:14:53. Training loss: 3.214188590844472. Learning Rate: 1.2634587634587635e-05\n",
      "Batch 2,150 of 9,631 Elased 0:15:14. Training loss: 3.2151960152249. Learning Rate: 1.261724386724387e-05\n",
      "Batch 2,200 of 9,631 Elased 0:15:36. Training loss: 3.214869302402843. Learning Rate: 1.25999000999001e-05\n",
      "Batch 2,250 of 9,631 Elased 0:15:57. Training loss: 3.2127022142410278. Learning Rate: 1.2582556332556333e-05\n",
      "Batch 2,300 of 9,631 Elased 0:16:18. Training loss: 3.2139947847698047. Learning Rate: 1.2565212565212567e-05\n",
      "Batch 2,350 of 9,631 Elased 0:16:40. Training loss: 3.211954409619595. Learning Rate: 1.2547868797868798e-05\n",
      "Batch 2,400 of 9,631 Elased 0:17:00. Training loss: 3.211191739141941. Learning Rate: 1.2530525030525031e-05\n",
      "Batch 2,450 of 9,631 Elased 0:17:22. Training loss: 3.2080029384457336. Learning Rate: 1.2513181263181265e-05\n",
      "Batch 2,500 of 9,631 Elased 0:17:43. Training loss: 3.2063597643852235. Learning Rate: 1.2495837495837496e-05\n",
      "Batch 2,550 of 9,631 Elased 0:18:04. Training loss: 3.210652342964621. Learning Rate: 1.2478493728493729e-05\n",
      "Batch 2,600 of 9,631 Elased 0:18:25. Training loss: 3.2137766329141764. Learning Rate: 1.2461149961149962e-05\n",
      "Batch 2,650 of 9,631 Elased 0:18:47. Training loss: 3.2166023105945225. Learning Rate: 1.2443806193806194e-05\n",
      "Batch 2,700 of 9,631 Elased 0:19:08. Training loss: 3.219087077555833. Learning Rate: 1.2426462426462427e-05\n",
      "Batch 2,750 of 9,631 Elased 0:19:30. Training loss: 3.2170108429735356. Learning Rate: 1.240911865911866e-05\n",
      "Batch 2,800 of 9,631 Elased 0:19:50. Training loss: 3.2186022873009956. Learning Rate: 1.2391774891774892e-05\n",
      "Batch 2,850 of 9,631 Elased 0:20:11. Training loss: 3.2214427726310597. Learning Rate: 1.2374431124431125e-05\n",
      "Batch 2,900 of 9,631 Elased 0:20:33. Training loss: 3.221612247845222. Learning Rate: 1.2357087357087357e-05\n",
      "Batch 2,950 of 9,631 Elased 0:20:54. Training loss: 3.221420946686955. Learning Rate: 1.233974358974359e-05\n",
      "Batch 3,000 of 9,631 Elased 0:21:15. Training loss: 3.2238435684045155. Learning Rate: 1.2322399822399823e-05\n",
      "Batch 3,050 of 9,631 Elased 0:21:36. Training loss: 3.2234520019468715. Learning Rate: 1.2305056055056055e-05\n",
      "Batch 3,100 of 9,631 Elased 0:21:58. Training loss: 3.224091875168585. Learning Rate: 1.2287712287712288e-05\n",
      "Batch 3,150 of 9,631 Elased 0:22:19. Training loss: 3.2207699285613165. Learning Rate: 1.227036852036852e-05\n",
      "Batch 3,200 of 9,631 Elased 0:22:40. Training loss: 3.222966453246772. Learning Rate: 1.2253024753024753e-05\n",
      "Batch 3,250 of 9,631 Elased 0:23:01. Training loss: 3.2229462829369764. Learning Rate: 1.2235680985680986e-05\n",
      "Batch 3,300 of 9,631 Elased 0:23:22. Training loss: 3.225370145819404. Learning Rate: 1.2218337218337219e-05\n",
      "Batch 3,350 of 9,631 Elased 0:23:43. Training loss: 3.2260813557567882. Learning Rate: 1.2200993450993451e-05\n",
      "Batch 3,400 of 9,631 Elased 0:24:04. Training loss: 3.2264761748384028. Learning Rate: 1.2183649683649684e-05\n",
      "Batch 3,450 of 9,631 Elased 0:24:26. Training loss: 3.2269378383263296. Learning Rate: 1.2166305916305917e-05\n",
      "Batch 3,500 of 9,631 Elased 0:24:47. Training loss: 3.226335244349071. Learning Rate: 1.214896214896215e-05\n",
      "Batch 3,550 of 9,631 Elased 0:25:08. Training loss: 3.2273665540654894. Learning Rate: 1.2131618381618384e-05\n",
      "Batch 3,600 of 9,631 Elased 0:25:29. Training loss: 3.2288966724938817. Learning Rate: 1.2114274614274615e-05\n",
      "Batch 3,650 of 9,631 Elased 0:25:51. Training loss: 3.2323268124502. Learning Rate: 1.2096930846930847e-05\n",
      "Batch 3,700 of 9,631 Elased 0:26:12. Training loss: 3.2314145381708403. Learning Rate: 1.207958707958708e-05\n",
      "Batch 3,750 of 9,631 Elased 0:26:33. Training loss: 3.230226944510142. Learning Rate: 1.2062243312243313e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3,800 of 9,631 Elased 0:26:55. Training loss: 3.229081423627703. Learning Rate: 1.2044899544899545e-05\n",
      "Batch 3,850 of 9,631 Elased 0:27:16. Training loss: 3.226312584722197. Learning Rate: 1.2027555777555778e-05\n",
      "Batch 3,900 of 9,631 Elased 0:27:37. Training loss: 3.22523159241065. Learning Rate: 1.201021201021201e-05\n",
      "Batch 3,950 of 9,631 Elased 0:27:58. Training loss: 3.226259295970579. Learning Rate: 1.1992868242868243e-05\n",
      "Batch 4,000 of 9,631 Elased 0:28:20. Training loss: 3.226782262444496. Learning Rate: 1.1975524475524476e-05\n",
      "Batch 4,050 of 9,631 Elased 0:28:41. Training loss: 3.226020867265301. Learning Rate: 1.1958180708180709e-05\n",
      "Batch 4,100 of 9,631 Elased 0:29:02. Training loss: 3.2254218079985644. Learning Rate: 1.1940836940836941e-05\n",
      "Batch 4,150 of 9,631 Elased 0:29:23. Training loss: 3.224946981510484. Learning Rate: 1.1923493173493174e-05\n",
      "Batch 4,200 of 9,631 Elased 0:29:44. Training loss: 3.224806984208879. Learning Rate: 1.1906149406149407e-05\n",
      "Batch 4,250 of 9,631 Elased 0:30:06. Training loss: 3.2249173966015086. Learning Rate: 1.188880563880564e-05\n",
      "Batch 4,300 of 9,631 Elased 0:30:27. Training loss: 3.2244181903018507. Learning Rate: 1.1871461871461872e-05\n",
      "Batch 4,350 of 9,631 Elased 0:30:48. Training loss: 3.2240775261802237. Learning Rate: 1.1854118104118105e-05\n",
      "Batch 4,400 of 9,631 Elased 0:31:10. Training loss: 3.22475003128702. Learning Rate: 1.1836774336774337e-05\n",
      "Batch 4,450 of 9,631 Elased 0:31:31. Training loss: 3.2253427128845384. Learning Rate: 1.181943056943057e-05\n",
      "Batch 4,500 of 9,631 Elased 0:31:52. Training loss: 3.226435045666165. Learning Rate: 1.1802086802086802e-05\n",
      "Batch 4,550 of 9,631 Elased 0:32:14. Training loss: 3.2249190932053784. Learning Rate: 1.1784743034743035e-05\n",
      "Batch 4,600 of 9,631 Elased 0:32:35. Training loss: 3.22576310004877. Learning Rate: 1.1767399267399268e-05\n",
      "Batch 4,650 of 9,631 Elased 0:32:56. Training loss: 3.225300085595859. Learning Rate: 1.17500555000555e-05\n",
      "Batch 4,700 of 9,631 Elased 0:33:18. Training loss: 3.2256921146524715. Learning Rate: 1.1732711732711733e-05\n",
      "Batch 4,750 of 9,631 Elased 0:33:39. Training loss: 3.2258274177501076. Learning Rate: 1.1715367965367966e-05\n",
      "Batch 4,800 of 9,631 Elased 0:34:00. Training loss: 3.224246677532792. Learning Rate: 1.1698024198024198e-05\n",
      "Batch 4,850 of 9,631 Elased 0:34:21. Training loss: 3.223559149245626. Learning Rate: 1.1680680430680431e-05\n",
      "Batch 4,900 of 9,631 Elased 0:34:43. Training loss: 3.2246016748340764. Learning Rate: 1.1663336663336664e-05\n",
      "Batch 4,950 of 9,631 Elased 0:35:04. Training loss: 3.224867989583449. Learning Rate: 1.1645992895992896e-05\n",
      "Batch 5,000 of 9,631 Elased 0:35:25. Training loss: 3.2260560242414473. Learning Rate: 1.1628649128649129e-05\n",
      "Batch 5,050 of 9,631 Elased 0:35:46. Training loss: 3.225659832553108. Learning Rate: 1.1611305361305362e-05\n",
      "Batch 5,100 of 9,631 Elased 0:36:08. Training loss: 3.2237400625032535. Learning Rate: 1.1593961593961594e-05\n",
      "Batch 5,150 of 9,631 Elased 0:36:29. Training loss: 3.2238673752951392. Learning Rate: 1.1576617826617827e-05\n",
      "Batch 5,200 of 9,631 Elased 0:36:50. Training loss: 3.2232561961733377. Learning Rate: 1.155927405927406e-05\n",
      "Batch 5,250 of 9,631 Elased 0:37:12. Training loss: 3.2232713353066216. Learning Rate: 1.1541930291930292e-05\n",
      "Batch 5,300 of 9,631 Elased 0:37:33. Training loss: 3.222771478756419. Learning Rate: 1.1524586524586525e-05\n",
      "Batch 5,350 of 9,631 Elased 0:37:54. Training loss: 3.224936115452062. Learning Rate: 1.1507242757242758e-05\n",
      "Batch 5,400 of 9,631 Elased 0:38:15. Training loss: 3.2247043657744374. Learning Rate: 1.148989898989899e-05\n",
      "Batch 5,450 of 9,631 Elased 0:38:37. Training loss: 3.224821217300695. Learning Rate: 1.1472555222555223e-05\n",
      "Batch 5,500 of 9,631 Elased 0:38:58. Training loss: 3.2250144225033845. Learning Rate: 1.1455211455211456e-05\n",
      "Batch 5,550 of 9,631 Elased 0:39:20. Training loss: 3.225174345282821. Learning Rate: 1.1437867687867688e-05\n",
      "Batch 5,600 of 9,631 Elased 0:39:41. Training loss: 3.2244077110716276. Learning Rate: 1.1420523920523921e-05\n",
      "Batch 5,650 of 9,631 Elased 0:40:03. Training loss: 3.225868913684271. Learning Rate: 1.1403180153180154e-05\n",
      "Batch 5,700 of 9,631 Elased 0:40:24. Training loss: 3.2257712119504025. Learning Rate: 1.1385836385836386e-05\n",
      "Batch 5,750 of 9,631 Elased 0:40:45. Training loss: 3.225579201034878. Learning Rate: 1.1368492618492619e-05\n",
      "Batch 5,800 of 9,631 Elased 0:41:07. Training loss: 3.2256065531640217. Learning Rate: 1.1351148851148852e-05\n",
      "Batch 5,850 of 9,631 Elased 0:41:28. Training loss: 3.2259135162117136. Learning Rate: 1.1333805083805084e-05\n",
      "Batch 5,900 of 9,631 Elased 0:41:49. Training loss: 3.225917380260209. Learning Rate: 1.1316461316461317e-05\n",
      "Batch 5,950 of 9,631 Elased 0:42:10. Training loss: 3.2263573956689915. Learning Rate: 1.129911754911755e-05\n",
      "Batch 6,000 of 9,631 Elased 0:42:32. Training loss: 3.226167737921079. Learning Rate: 1.1281773781773782e-05\n",
      "Batch 6,050 of 9,631 Elased 0:42:54. Training loss: 3.2267118729835698. Learning Rate: 1.1264430014430015e-05\n",
      "Batch 6,100 of 9,631 Elased 0:43:16. Training loss: 3.2270843689168087. Learning Rate: 1.1247086247086247e-05\n",
      "Batch 6,150 of 9,631 Elased 0:43:37. Training loss: 3.226791041537029. Learning Rate: 1.122974247974248e-05\n",
      "Batch 6,200 of 9,631 Elased 0:44:00. Training loss: 3.225158689887293. Learning Rate: 1.1212398712398713e-05\n",
      "Batch 6,250 of 9,631 Elased 0:44:22. Training loss: 3.225012970428467. Learning Rate: 1.1195054945054945e-05\n",
      "Batch 6,300 of 9,631 Elased 0:44:43. Training loss: 3.224874779629329. Learning Rate: 1.1177711177711178e-05\n",
      "Batch 6,350 of 9,631 Elased 0:45:06. Training loss: 3.2245361882870593. Learning Rate: 1.116036741036741e-05\n",
      "Batch 6,400 of 9,631 Elased 0:45:27. Training loss: 3.2241307641379535. Learning Rate: 1.1143023643023643e-05\n",
      "Batch 6,450 of 9,631 Elased 0:45:48. Training loss: 3.225150690577751. Learning Rate: 1.1125679875679876e-05\n",
      "Batch 6,500 of 9,631 Elased 0:46:09. Training loss: 3.223846264105577. Learning Rate: 1.1108336108336109e-05\n",
      "Batch 6,550 of 9,631 Elased 0:46:31. Training loss: 3.2242574055504254. Learning Rate: 1.1090992340992341e-05\n",
      "Batch 6,600 of 9,631 Elased 0:46:52. Training loss: 3.224862691525257. Learning Rate: 1.1073648573648574e-05\n",
      "Batch 6,650 of 9,631 Elased 0:47:13. Training loss: 3.2253846338100005. Learning Rate: 1.1056304806304807e-05\n",
      "Batch 6,700 of 9,631 Elased 0:47:34. Training loss: 3.2262669558311576. Learning Rate: 1.103896103896104e-05\n",
      "Batch 6,750 of 9,631 Elased 0:47:56. Training loss: 3.226337477913609. Learning Rate: 1.1021617271617272e-05\n",
      "Batch 6,800 of 9,631 Elased 0:48:17. Training loss: 3.2261851461319364. Learning Rate: 1.1004273504273505e-05\n",
      "Batch 6,850 of 9,631 Elased 0:48:38. Training loss: 3.225753332176348. Learning Rate: 1.0986929736929737e-05\n",
      "Batch 6,900 of 9,631 Elased 0:48:59. Training loss: 3.225529716014862. Learning Rate: 1.096958596958597e-05\n",
      "Batch 6,950 of 9,631 Elased 0:49:20. Training loss: 3.2255941088422597. Learning Rate: 1.0952242202242203e-05\n",
      "Batch 7,000 of 9,631 Elased 0:49:41. Training loss: 3.2249346130234855. Learning Rate: 1.0934898434898435e-05\n",
      "Batch 7,050 of 9,631 Elased 0:50:02. Training loss: 3.2243402081009345. Learning Rate: 1.0917554667554668e-05\n",
      "Batch 7,100 of 9,631 Elased 0:50:23. Training loss: 3.224437278844941. Learning Rate: 1.09002109002109e-05\n",
      "Batch 7,150 of 9,631 Elased 0:50:45. Training loss: 3.2243004234187254. Learning Rate: 1.0882867132867133e-05\n",
      "Batch 7,200 of 9,631 Elased 0:51:06. Training loss: 3.224247277693616. Learning Rate: 1.0865523365523366e-05\n",
      "Batch 7,250 of 9,631 Elased 0:51:27. Training loss: 3.223151807373968. Learning Rate: 1.0848179598179599e-05\n",
      "Batch 7,300 of 9,631 Elased 0:51:48. Training loss: 3.2234762366667185. Learning Rate: 1.0830835830835831e-05\n",
      "Batch 7,350 of 9,631 Elased 0:52:10. Training loss: 3.223069845786711. Learning Rate: 1.0813492063492064e-05\n",
      "Batch 7,400 of 9,631 Elased 0:52:31. Training loss: 3.2218478023200423. Learning Rate: 1.0796148296148297e-05\n",
      "Batch 7,450 of 9,631 Elased 0:52:52. Training loss: 3.2217213488425185. Learning Rate: 1.077880452880453e-05\n",
      "Batch 7,500 of 9,631 Elased 0:53:13. Training loss: 3.2208317673842113. Learning Rate: 1.0761460761460762e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7,550 of 9,631 Elased 0:53:34. Training loss: 3.220610690243197. Learning Rate: 1.0744116994116995e-05\n",
      "Batch 7,600 of 9,631 Elased 0:53:55. Training loss: 3.2210621013453133. Learning Rate: 1.0726773226773227e-05\n",
      "Batch 7,650 of 9,631 Elased 0:54:17. Training loss: 3.221106622468412. Learning Rate: 1.070942945942946e-05\n",
      "Batch 7,700 of 9,631 Elased 0:54:38. Training loss: 3.220015694708019. Learning Rate: 1.0692085692085692e-05\n",
      "Batch 7,750 of 9,631 Elased 0:54:59. Training loss: 3.2185321325640524. Learning Rate: 1.0674741924741925e-05\n",
      "Batch 7,800 of 9,631 Elased 0:55:21. Training loss: 3.2191316990821788. Learning Rate: 1.0657398157398158e-05\n",
      "Batch 7,850 of 9,631 Elased 0:55:42. Training loss: 3.219520916164301. Learning Rate: 1.064005439005439e-05\n",
      "Batch 7,900 of 9,631 Elased 0:56:03. Training loss: 3.218332022745398. Learning Rate: 1.0622710622710623e-05\n",
      "Batch 7,950 of 9,631 Elased 0:56:24. Training loss: 3.219684779808956. Learning Rate: 1.0605366855366856e-05\n",
      "Batch 8,000 of 9,631 Elased 0:56:46. Training loss: 3.2194825154989957. Learning Rate: 1.0588023088023088e-05\n",
      "Batch 8,050 of 9,631 Elased 0:57:07. Training loss: 3.2202807554547093. Learning Rate: 1.0570679320679321e-05\n",
      "Batch 8,100 of 9,631 Elased 0:57:28. Training loss: 3.2205077511734435. Learning Rate: 1.0553335553335554e-05\n",
      "Batch 8,150 of 9,631 Elased 0:57:49. Training loss: 3.221298029276491. Learning Rate: 1.0535991785991786e-05\n",
      "Batch 8,200 of 9,631 Elased 0:58:11. Training loss: 3.2212021579975034. Learning Rate: 1.0518648018648019e-05\n",
      "Batch 8,250 of 9,631 Elased 0:58:32. Training loss: 3.2219185559243866. Learning Rate: 1.0501304251304252e-05\n",
      "Batch 8,300 of 9,631 Elased 0:58:53. Training loss: 3.222115783863757. Learning Rate: 1.0483960483960484e-05\n",
      "Batch 8,350 of 9,631 Elased 0:59:14. Training loss: 3.2219703735991154. Learning Rate: 1.0466616716616717e-05\n",
      "Batch 8,400 of 9,631 Elased 0:59:36. Training loss: 3.221199694275856. Learning Rate: 1.044927294927295e-05\n",
      "Batch 8,450 of 9,631 Elased 0:59:57. Training loss: 3.220657974502744. Learning Rate: 1.0431929181929182e-05\n",
      "Batch 8,500 of 9,631 Elased 1:00:18. Training loss: 3.220332054530873. Learning Rate: 1.0414585414585415e-05\n",
      "Batch 8,550 of 9,631 Elased 1:00:40. Training loss: 3.219833828719736. Learning Rate: 1.0397241647241648e-05\n",
      "Batch 8,600 of 9,631 Elased 1:01:01. Training loss: 3.220364662131598. Learning Rate: 1.037989787989788e-05\n",
      "Batch 8,650 of 9,631 Elased 1:01:22. Training loss: 3.2195454696837187. Learning Rate: 1.0362554112554113e-05\n",
      "Batch 8,700 of 9,631 Elased 1:01:44. Training loss: 3.219361432078241. Learning Rate: 1.0345210345210346e-05\n",
      "Batch 8,750 of 9,631 Elased 1:02:05. Training loss: 3.2191199251311167. Learning Rate: 1.0327866577866578e-05\n",
      "Batch 8,800 of 9,631 Elased 1:02:26. Training loss: 3.218898603387854. Learning Rate: 1.0310522810522811e-05\n",
      "Batch 8,850 of 9,631 Elased 1:02:47. Training loss: 3.2184207629483974. Learning Rate: 1.0293179043179044e-05\n",
      "Batch 8,900 of 9,631 Elased 1:03:09. Training loss: 3.2185160402903397. Learning Rate: 1.0275835275835276e-05\n",
      "Batch 8,950 of 9,631 Elased 1:03:30. Training loss: 3.218369270529827. Learning Rate: 1.0258491508491509e-05\n",
      "Batch 9,000 of 9,631 Elased 1:03:52. Training loss: 3.217473795109325. Learning Rate: 1.0241147741147742e-05\n",
      "Batch 9,050 of 9,631 Elased 1:04:13. Training loss: 3.2183975980426727. Learning Rate: 1.0223803973803974e-05\n",
      "Batch 9,100 of 9,631 Elased 1:04:34. Training loss: 3.218282832784967. Learning Rate: 1.0206460206460207e-05\n",
      "Batch 9,150 of 9,631 Elased 1:04:55. Training loss: 3.21830307077189. Learning Rate: 1.018911643911644e-05\n",
      "Batch 9,200 of 9,631 Elased 1:05:17. Training loss: 3.219018452089766. Learning Rate: 1.0171772671772672e-05\n",
      "Batch 9,250 of 9,631 Elased 1:05:38. Training loss: 3.2189113931655884. Learning Rate: 1.0154428904428905e-05\n",
      "Batch 9,300 of 9,631 Elased 1:05:59. Training loss: 3.2193728076258012. Learning Rate: 1.0137085137085137e-05\n",
      "Batch 9,350 of 9,631 Elased 1:06:21. Training loss: 3.2191994476063366. Learning Rate: 1.011974136974137e-05\n",
      "Batch 9,400 of 9,631 Elased 1:06:42. Training loss: 3.219000756245978. Learning Rate: 1.0102397602397603e-05\n",
      "Batch 9,450 of 9,631 Elased 1:07:03. Training loss: 3.2188320045244128. Learning Rate: 1.0085053835053835e-05\n",
      "Batch 9,500 of 9,631 Elased 1:07:24. Training loss: 3.2184161112308503. Learning Rate: 1.0067710067710068e-05\n",
      "Batch 9,550 of 9,631 Elased 1:07:45. Training loss: 3.2178316187359277. Learning Rate: 1.00503663003663e-05\n",
      "Batch 9,600 of 9,631 Elased 1:08:06. Training loss: 3.2181684320792554. Learning Rate: 1.0033022533022533e-05\n",
      "\n",
      "\n",
      "  Average training loss: 3.22\n",
      "  Training epcoh took: 1:08:19\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c626d25b93a64eff89b0ff146e5973d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=67.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Validation Loss: 3.41\n",
      "  Validation took: 0:00:43\n",
      "\n",
      "======== Epoch 28 / 30 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c15c2d652ffe4e209faf1fed63da8edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch    50 of 9,631 Elased 0:00:21. Training loss: 3.2811962127685548. Learning Rate: 1.0004925629925631e-05\n",
      "Batch   100 of 9,631 Elased 0:00:42. Training loss: 3.233729830980301. Learning Rate: 9.987581862581864e-06\n",
      "Batch   150 of 9,631 Elased 0:01:04. Training loss: 3.251251674493154. Learning Rate: 9.970238095238096e-06\n",
      "Batch   200 of 9,631 Elased 0:01:24. Training loss: 3.2838360112905502. Learning Rate: 9.952894327894329e-06\n",
      "Batch   250 of 9,631 Elased 0:01:46. Training loss: 3.2400669007301333. Learning Rate: 9.93555056055056e-06\n",
      "Batch   300 of 9,631 Elased 0:02:07. Training loss: 3.2290413955847423. Learning Rate: 9.918206793206794e-06\n",
      "Batch   350 of 9,631 Elased 0:02:28. Training loss: 3.224339339051928. Learning Rate: 9.900863025863025e-06\n",
      "Batch   400 of 9,631 Elased 0:02:49. Training loss: 3.2302422019839288. Learning Rate: 9.88351925851926e-06\n",
      "Batch   450 of 9,631 Elased 0:03:10. Training loss: 3.2317072039180332. Learning Rate: 9.866175491175492e-06\n",
      "Batch   500 of 9,631 Elased 0:03:31. Training loss: 3.237342901468277. Learning Rate: 9.848831723831723e-06\n",
      "Batch   550 of 9,631 Elased 0:03:52. Training loss: 3.22202246167443. Learning Rate: 9.831487956487958e-06\n",
      "Batch   600 of 9,631 Elased 0:04:13. Training loss: 3.237327464222908. Learning Rate: 9.814144189144189e-06\n",
      "Batch   650 of 9,631 Elased 0:04:35. Training loss: 3.2371599433972285. Learning Rate: 9.796800421800423e-06\n",
      "Batch   700 of 9,631 Elased 0:04:56. Training loss: 3.2319267158848897. Learning Rate: 9.779456654456656e-06\n",
      "Batch   750 of 9,631 Elased 0:05:17. Training loss: 3.2203247901598613. Learning Rate: 9.762112887112887e-06\n",
      "Batch   800 of 9,631 Elased 0:05:39. Training loss: 3.2177428847551344. Learning Rate: 9.744769119769121e-06\n",
      "Batch   850 of 9,631 Elased 0:06:00. Training loss: 3.210547856723561. Learning Rate: 9.727425352425352e-06\n",
      "Batch   900 of 9,631 Elased 0:06:21. Training loss: 3.2078858281506433. Learning Rate: 9.710081585081586e-06\n",
      "Batch   950 of 9,631 Elased 0:06:42. Training loss: 3.2037386227908886. Learning Rate: 9.692737817737819e-06\n",
      "Batch 1,000 of 9,631 Elased 0:07:04. Training loss: 3.204740463614464. Learning Rate: 9.67539405039405e-06\n",
      "Batch 1,050 of 9,631 Elased 0:07:25. Training loss: 3.2051952174731664. Learning Rate: 9.658050283050284e-06\n",
      "Batch 1,100 of 9,631 Elased 0:07:46. Training loss: 3.207552073543722. Learning Rate: 9.640706515706515e-06\n",
      "Batch 1,150 of 9,631 Elased 0:08:08. Training loss: 3.207081474946893. Learning Rate: 9.62336274836275e-06\n",
      "Batch 1,200 of 9,631 Elased 0:08:29. Training loss: 3.208186694582303. Learning Rate: 9.60601898101898e-06\n",
      "Batch 1,250 of 9,631 Elased 0:08:50. Training loss: 3.2090210496902465. Learning Rate: 9.588675213675215e-06\n",
      "Batch 1,300 of 9,631 Elased 0:09:11. Training loss: 3.2046097779273985. Learning Rate: 9.571331446331447e-06\n",
      "Batch 1,350 of 9,631 Elased 0:09:33. Training loss: 3.2045188234470507. Learning Rate: 9.553987678987678e-06\n",
      "Batch 1,400 of 9,631 Elased 0:09:54. Training loss: 3.20291103039469. Learning Rate: 9.536643911643913e-06\n",
      "Batch 1,450 of 9,631 Elased 0:10:15. Training loss: 3.201302504950556. Learning Rate: 9.519300144300144e-06\n",
      "Batch 1,500 of 9,631 Elased 0:10:36. Training loss: 3.2000750086307526. Learning Rate: 9.501956376956378e-06\n",
      "Batch 1,550 of 9,631 Elased 0:10:58. Training loss: 3.200064574749239. Learning Rate: 9.48461260961261e-06\n",
      "Batch 1,600 of 9,631 Elased 0:11:19. Training loss: 3.205562522634864. Learning Rate: 9.467268842268842e-06\n",
      "Batch 1,650 of 9,631 Elased 0:11:40. Training loss: 3.208619203784249. Learning Rate: 9.449925074925076e-06\n",
      "Batch 1,700 of 9,631 Elased 0:12:01. Training loss: 3.205946296593722. Learning Rate: 9.432581307581307e-06\n",
      "Batch 1,750 of 9,631 Elased 0:12:23. Training loss: 3.205334644794464. Learning Rate: 9.415237540237541e-06\n",
      "Batch 1,800 of 9,631 Elased 0:12:44. Training loss: 3.204402835302883. Learning Rate: 9.397893772893774e-06\n",
      "Batch 1,850 of 9,631 Elased 0:13:06. Training loss: 3.2071853310997422. Learning Rate: 9.380550005550005e-06\n",
      "Batch 1,900 of 9,631 Elased 0:13:27. Training loss: 3.206404431681884. Learning Rate: 9.36320623820624e-06\n",
      "Batch 1,950 of 9,631 Elased 0:13:48. Training loss: 3.203392259096488. Learning Rate: 9.34586247086247e-06\n",
      "Batch 2,000 of 9,631 Elased 0:14:09. Training loss: 3.1991962856650353. Learning Rate: 9.328518703518705e-06\n",
      "Batch 2,050 of 9,631 Elased 0:14:30. Training loss: 3.2000223289466487. Learning Rate: 9.311174936174936e-06\n",
      "Batch 2,100 of 9,631 Elased 0:14:51. Training loss: 3.203383565573465. Learning Rate: 9.29383116883117e-06\n",
      "Batch 2,150 of 9,631 Elased 0:15:12. Training loss: 3.2037252193827963. Learning Rate: 9.276487401487403e-06\n",
      "Batch 2,200 of 9,631 Elased 0:15:33. Training loss: 3.2031996889006007. Learning Rate: 9.259143634143634e-06\n",
      "Batch 2,250 of 9,631 Elased 0:15:54. Training loss: 3.2009542519781324. Learning Rate: 9.241799866799868e-06\n",
      "Batch 2,300 of 9,631 Elased 0:16:16. Training loss: 3.202098790407181. Learning Rate: 9.224456099456099e-06\n",
      "Batch 2,350 of 9,631 Elased 0:16:37. Training loss: 3.2005338309673554. Learning Rate: 9.207112332112333e-06\n",
      "Batch 2,400 of 9,631 Elased 0:16:58. Training loss: 3.199667691687743. Learning Rate: 9.189768564768566e-06\n",
      "Batch 2,450 of 9,631 Elased 0:17:20. Training loss: 3.1965943460075223. Learning Rate: 9.172424797424797e-06\n",
      "Batch 2,500 of 9,631 Elased 0:17:41. Training loss: 3.1953761858940126. Learning Rate: 9.155081030081031e-06\n",
      "Batch 2,550 of 9,631 Elased 0:18:03. Training loss: 3.19944461775761. Learning Rate: 9.137737262737262e-06\n",
      "Batch 2,600 of 9,631 Elased 0:18:24. Training loss: 3.2027065992355346. Learning Rate: 9.120393495393497e-06\n",
      "Batch 2,650 of 9,631 Elased 0:18:45. Training loss: 3.205586735707409. Learning Rate: 9.10304972804973e-06\n",
      "Batch 2,700 of 9,631 Elased 0:19:07. Training loss: 3.2073274957692184. Learning Rate: 9.08570596070596e-06\n",
      "Batch 2,750 of 9,631 Elased 0:19:28. Training loss: 3.205126626101407. Learning Rate: 9.068362193362195e-06\n",
      "Batch 2,800 of 9,631 Elased 0:19:49. Training loss: 3.2057790207862853. Learning Rate: 9.051018426018425e-06\n",
      "Batch 2,850 of 9,631 Elased 0:20:11. Training loss: 3.208612967875966. Learning Rate: 9.03367465867466e-06\n",
      "Batch 2,900 of 9,631 Elased 0:20:32. Training loss: 3.208738363611287. Learning Rate: 9.016330891330892e-06\n",
      "Batch 2,950 of 9,631 Elased 0:20:53. Training loss: 3.208915167016498. Learning Rate: 8.998987123987123e-06\n",
      "Batch 3,000 of 9,631 Elased 0:21:15. Training loss: 3.2110548818906146. Learning Rate: 8.981643356643358e-06\n",
      "Batch 3,050 of 9,631 Elased 0:21:36. Training loss: 3.210778033694283. Learning Rate: 8.964299589299589e-06\n",
      "Batch 3,100 of 9,631 Elased 0:21:57. Training loss: 3.211006708222051. Learning Rate: 8.946955821955823e-06\n",
      "Batch 3,150 of 9,631 Elased 0:22:18. Training loss: 3.2075646126459514. Learning Rate: 8.929612054612054e-06\n",
      "Batch 3,200 of 9,631 Elased 0:22:40. Training loss: 3.2100121155381203. Learning Rate: 8.912268287268288e-06\n",
      "Batch 3,250 of 9,631 Elased 0:23:01. Training loss: 3.2100106152754564. Learning Rate: 8.894924519924521e-06\n",
      "Batch 3,300 of 9,631 Elased 0:23:22. Training loss: 3.2124547119574114. Learning Rate: 8.877580752580752e-06\n",
      "Batch 3,350 of 9,631 Elased 0:23:43. Training loss: 3.2129481754729996. Learning Rate: 8.860236985236986e-06\n",
      "Batch 3,400 of 9,631 Elased 0:24:05. Training loss: 3.213009182530291. Learning Rate: 8.842893217893217e-06\n",
      "Batch 3,450 of 9,631 Elased 0:24:26. Training loss: 3.2135578483429508. Learning Rate: 8.825549450549452e-06\n",
      "Batch 3,500 of 9,631 Elased 0:24:47. Training loss: 3.2131629151276178. Learning Rate: 8.808205683205684e-06\n",
      "Batch 3,550 of 9,631 Elased 0:25:08. Training loss: 3.2148569985174795. Learning Rate: 8.790861915861915e-06\n",
      "Batch 3,600 of 9,631 Elased 0:25:30. Training loss: 3.216006955338849. Learning Rate: 8.77351814851815e-06\n",
      "Batch 3,650 of 9,631 Elased 0:25:51. Training loss: 3.2191688074804334. Learning Rate: 8.75617438117438e-06\n",
      "Batch 3,700 of 9,631 Elased 0:26:13. Training loss: 3.2186710310948863. Learning Rate: 8.738830613830615e-06\n",
      "Batch 3,750 of 9,631 Elased 0:26:34. Training loss: 3.2175242588361104. Learning Rate: 8.721486846486848e-06\n",
      "Batch 3,800 of 9,631 Elased 0:26:55. Training loss: 3.216531342738553. Learning Rate: 8.704143079143079e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3,850 of 9,631 Elased 0:27:17. Training loss: 3.2147837826183863. Learning Rate: 8.686799311799313e-06\n",
      "Batch 3,900 of 9,631 Elased 0:27:38. Training loss: 3.2137169739833245. Learning Rate: 8.669455544455544e-06\n",
      "Batch 3,950 of 9,631 Elased 0:27:59. Training loss: 3.214515285159968. Learning Rate: 8.652111777111778e-06\n",
      "Batch 4,000 of 9,631 Elased 0:28:20. Training loss: 3.214906221240759. Learning Rate: 8.63476800976801e-06\n",
      "Batch 4,050 of 9,631 Elased 0:28:41. Training loss: 3.214719373120202. Learning Rate: 8.617424242424244e-06\n",
      "Batch 4,100 of 9,631 Elased 0:29:02. Training loss: 3.2140615133250634. Learning Rate: 8.600080475080476e-06\n",
      "Batch 4,150 of 9,631 Elased 0:29:24. Training loss: 3.2137996319977633. Learning Rate: 8.582736707736707e-06\n",
      "Batch 4,200 of 9,631 Elased 0:29:45. Training loss: 3.2139619262161707. Learning Rate: 8.565392940392942e-06\n",
      "Batch 4,250 of 9,631 Elased 0:30:06. Training loss: 3.2147975417024948. Learning Rate: 8.548049173049172e-06\n",
      "Batch 4,300 of 9,631 Elased 0:30:27. Training loss: 3.2142856342570725. Learning Rate: 8.530705405705407e-06\n",
      "Batch 4,350 of 9,631 Elased 0:30:49. Training loss: 3.2140330957270216. Learning Rate: 8.51336163836164e-06\n",
      "Batch 4,400 of 9,631 Elased 0:31:10. Training loss: 3.2156702739813112. Learning Rate: 8.49601787101787e-06\n",
      "Batch 4,450 of 9,631 Elased 0:31:31. Training loss: 3.2159578761893712. Learning Rate: 8.478674103674105e-06\n",
      "Batch 4,500 of 9,631 Elased 0:31:53. Training loss: 3.217275792174869. Learning Rate: 8.461330336330336e-06\n",
      "Batch 4,550 of 9,631 Elased 0:32:14. Training loss: 3.2158375794546945. Learning Rate: 8.44398656898657e-06\n",
      "Batch 4,600 of 9,631 Elased 0:32:35. Training loss: 3.216774735606235. Learning Rate: 8.426642801642803e-06\n",
      "Batch 4,650 of 9,631 Elased 0:32:56. Training loss: 3.216261326061782. Learning Rate: 8.409299034299034e-06\n",
      "Batch 4,700 of 9,631 Elased 0:33:18. Training loss: 3.216370076017177. Learning Rate: 8.391955266955268e-06\n",
      "Batch 4,750 of 9,631 Elased 0:33:39. Training loss: 3.2168634508785448. Learning Rate: 8.374611499611499e-06\n",
      "Batch 4,800 of 9,631 Elased 0:34:00. Training loss: 3.2153622102737427. Learning Rate: 8.357267732267733e-06\n",
      "Batch 4,850 of 9,631 Elased 0:34:21. Training loss: 3.2148032650505143. Learning Rate: 8.339923964923964e-06\n",
      "Batch 4,900 of 9,631 Elased 0:34:43. Training loss: 3.2157306301837063. Learning Rate: 8.322580197580197e-06\n",
      "Batch 4,950 of 9,631 Elased 0:35:04. Training loss: 3.2161640461045082. Learning Rate: 8.305236430236431e-06\n",
      "Batch 5,000 of 9,631 Elased 0:35:25. Training loss: 3.217484208154678. Learning Rate: 8.287892662892662e-06\n",
      "Batch 5,050 of 9,631 Elased 0:35:47. Training loss: 3.2173209444839177. Learning Rate: 8.270548895548897e-06\n",
      "Batch 5,100 of 9,631 Elased 0:36:08. Training loss: 3.2153983588779673. Learning Rate: 8.253205128205128e-06\n",
      "Batch 5,150 of 9,631 Elased 0:36:29. Training loss: 3.2154676840143295. Learning Rate: 8.235861360861362e-06\n",
      "Batch 5,200 of 9,631 Elased 0:36:50. Training loss: 3.215196239077128. Learning Rate: 8.218517593517595e-06\n",
      "Batch 5,250 of 9,631 Elased 0:37:12. Training loss: 3.215212641352699. Learning Rate: 8.201173826173826e-06\n",
      "Batch 5,300 of 9,631 Elased 0:37:33. Training loss: 3.215295530715079. Learning Rate: 8.18383005883006e-06\n",
      "Batch 5,350 of 9,631 Elased 0:37:54. Training loss: 3.2172133092122657. Learning Rate: 8.166486291486291e-06\n",
      "Batch 5,400 of 9,631 Elased 0:38:15. Training loss: 3.2172848061058255. Learning Rate: 8.149142524142525e-06\n",
      "Batch 5,450 of 9,631 Elased 0:38:37. Training loss: 3.2172800924799856. Learning Rate: 8.131798756798758e-06\n",
      "Batch 5,500 of 9,631 Elased 0:38:58. Training loss: 3.2175963374701415. Learning Rate: 8.114454989454989e-06\n",
      "Batch 5,550 of 9,631 Elased 0:39:19. Training loss: 3.218157551030855. Learning Rate: 8.097111222111223e-06\n",
      "Batch 5,600 of 9,631 Elased 0:39:40. Training loss: 3.2175027660599778. Learning Rate: 8.079767454767454e-06\n",
      "Batch 5,650 of 9,631 Elased 0:40:02. Training loss: 3.21941795811189. Learning Rate: 8.062423687423689e-06\n",
      "Batch 5,700 of 9,631 Elased 0:40:23. Training loss: 3.2191173871567376. Learning Rate: 8.045079920079921e-06\n",
      "Batch 5,750 of 9,631 Elased 0:40:44. Training loss: 3.2188060021815095. Learning Rate: 8.027736152736152e-06\n",
      "Batch 5,800 of 9,631 Elased 0:41:06. Training loss: 3.218890980132695. Learning Rate: 8.010392385392387e-06\n",
      "Batch 5,850 of 9,631 Elased 0:41:27. Training loss: 3.2190165978007848. Learning Rate: 7.993048618048617e-06\n",
      "Batch 5,900 of 9,631 Elased 0:41:48. Training loss: 3.219183357024597. Learning Rate: 7.975704850704852e-06\n",
      "Batch 5,950 of 9,631 Elased 0:42:09. Training loss: 3.219507247399883. Learning Rate: 7.958361083361083e-06\n",
      "Batch 6,000 of 9,631 Elased 0:42:31. Training loss: 3.219475131014983. Learning Rate: 7.941017316017317e-06\n",
      "Batch 6,050 of 9,631 Elased 0:42:52. Training loss: 3.2199649559958905. Learning Rate: 7.92367354867355e-06\n",
      "Batch 6,100 of 9,631 Elased 0:43:13. Training loss: 3.220523060110749. Learning Rate: 7.90632978132978e-06\n",
      "Batch 6,150 of 9,631 Elased 0:43:34. Training loss: 3.220494956427473. Learning Rate: 7.888986013986015e-06\n",
      "Batch 6,200 of 9,631 Elased 0:43:56. Training loss: 3.2187446965325264. Learning Rate: 7.871642246642246e-06\n",
      "Batch 6,250 of 9,631 Elased 0:44:17. Training loss: 3.218529436759949. Learning Rate: 7.85429847929848e-06\n",
      "Batch 6,300 of 9,631 Elased 0:44:39. Training loss: 3.218347503344218. Learning Rate: 7.836954711954713e-06\n",
      "Batch 6,350 of 9,631 Elased 0:45:00. Training loss: 3.217981156458066. Learning Rate: 7.819610944610944e-06\n",
      "Batch 6,400 of 9,631 Elased 0:45:21. Training loss: 3.217681076582521. Learning Rate: 7.802267177267178e-06\n",
      "Batch 6,450 of 9,631 Elased 0:45:42. Training loss: 3.2186353883632393. Learning Rate: 7.78492340992341e-06\n",
      "Batch 6,500 of 9,631 Elased 0:46:04. Training loss: 3.2171698244168208. Learning Rate: 7.767579642579644e-06\n",
      "Batch 6,550 of 9,631 Elased 0:46:25. Training loss: 3.217434748729677. Learning Rate: 7.750235875235876e-06\n",
      "Batch 6,600 of 9,631 Elased 0:46:46. Training loss: 3.218028948921146. Learning Rate: 7.732892107892107e-06\n",
      "Batch 6,650 of 9,631 Elased 0:47:07. Training loss: 3.21856703865797. Learning Rate: 7.715548340548342e-06\n",
      "Batch 6,700 of 9,631 Elased 0:47:29. Training loss: 3.219671805936899. Learning Rate: 7.698204573204573e-06\n",
      "Batch 6,750 of 9,631 Elased 0:47:50. Training loss: 3.2195631261401707. Learning Rate: 7.680860805860807e-06\n",
      "Batch 6,800 of 9,631 Elased 0:48:11. Training loss: 3.219194121045225. Learning Rate: 7.663517038517038e-06\n",
      "Batch 6,850 of 9,631 Elased 0:48:32. Training loss: 3.218930158267056. Learning Rate: 7.64617327117327e-06\n",
      "Batch 6,900 of 9,631 Elased 0:48:54. Training loss: 3.2185841247655342. Learning Rate: 7.628829503829504e-06\n",
      "Batch 6,950 of 9,631 Elased 0:49:15. Training loss: 3.2187822705550158. Learning Rate: 7.611485736485737e-06\n",
      "Batch 7,000 of 9,631 Elased 0:49:37. Training loss: 3.2181131048543112. Learning Rate: 7.59414196914197e-06\n",
      "Batch 7,050 of 9,631 Elased 0:49:58. Training loss: 3.2175245011951907. Learning Rate: 7.576798201798202e-06\n",
      "Batch 7,100 of 9,631 Elased 0:50:19. Training loss: 3.2175410162563054. Learning Rate: 7.559454434454436e-06\n",
      "Batch 7,150 of 9,631 Elased 0:50:40. Training loss: 3.217298175104848. Learning Rate: 7.542110667110667e-06\n",
      "Batch 7,200 of 9,631 Elased 0:51:02. Training loss: 3.217507345130046. Learning Rate: 7.524766899766899e-06\n",
      "Batch 7,250 of 9,631 Elased 0:51:23. Training loss: 3.2165639025753943. Learning Rate: 7.507423132423133e-06\n",
      "Batch 7,300 of 9,631 Elased 0:51:44. Training loss: 3.216853355757178. Learning Rate: 7.490079365079365e-06\n",
      "Batch 7,350 of 9,631 Elased 0:52:05. Training loss: 3.2164178453010766. Learning Rate: 7.472735597735599e-06\n",
      "Batch 7,400 of 9,631 Elased 0:52:26. Training loss: 3.2150090745655264. Learning Rate: 7.455391830391831e-06\n",
      "Batch 7,450 of 9,631 Elased 0:52:48. Training loss: 3.214833296993435. Learning Rate: 7.4380480630480625e-06\n",
      "Batch 7,500 of 9,631 Elased 0:53:09. Training loss: 3.2139627589861552. Learning Rate: 7.420704295704296e-06\n",
      "Batch 7,550 of 9,631 Elased 0:53:30. Training loss: 3.2136412076760603. Learning Rate: 7.403360528360529e-06\n",
      "Batch 7,600 of 9,631 Elased 0:53:51. Training loss: 3.2140788844070936. Learning Rate: 7.386016761016762e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7,650 of 9,631 Elased 0:54:13. Training loss: 3.214026411343244. Learning Rate: 7.368672993672994e-06\n",
      "Batch 7,700 of 9,631 Elased 0:54:34. Training loss: 3.213233944255036. Learning Rate: 7.351329226329226e-06\n",
      "Batch 7,750 of 9,631 Elased 0:54:55. Training loss: 3.21179002604946. Learning Rate: 7.333985458985459e-06\n",
      "Batch 7,800 of 9,631 Elased 0:55:17. Training loss: 3.2122257507153047. Learning Rate: 7.316641691641692e-06\n",
      "Batch 7,850 of 9,631 Elased 0:55:38. Training loss: 3.212663041041915. Learning Rate: 7.2992979242979254e-06\n",
      "Batch 7,900 of 9,631 Elased 0:56:00. Training loss: 3.2115894644471665. Learning Rate: 7.281954156954157e-06\n",
      "Batch 7,950 of 9,631 Elased 0:56:22. Training loss: 3.2129895850247556. Learning Rate: 7.264610389610391e-06\n",
      "Batch 8,000 of 9,631 Elased 0:56:44. Training loss: 3.212799388051033. Learning Rate: 7.2472666222666226e-06\n",
      "Batch 8,050 of 9,631 Elased 0:57:05. Training loss: 3.2139068326298497. Learning Rate: 7.229922854922855e-06\n",
      "Batch 8,100 of 9,631 Elased 0:57:27. Training loss: 3.2140086924293896. Learning Rate: 7.212579087579088e-06\n",
      "Batch 8,150 of 9,631 Elased 0:57:49. Training loss: 3.2150398933668076. Learning Rate: 7.1952353202353205e-06\n",
      "Batch 8,200 of 9,631 Elased 0:58:11. Training loss: 3.214916070205409. Learning Rate: 7.177891552891554e-06\n",
      "Batch 8,250 of 9,631 Elased 0:58:33. Training loss: 3.2153951622356067. Learning Rate: 7.160547785547786e-06\n",
      "Batch 8,300 of 9,631 Elased 0:58:54. Training loss: 3.2157316378800265. Learning Rate: 7.143204018204018e-06\n",
      "Batch 8,350 of 9,631 Elased 0:59:16. Training loss: 3.2154310369206045. Learning Rate: 7.125860250860251e-06\n",
      "Batch 8,400 of 9,631 Elased 0:59:37. Training loss: 3.214618696655546. Learning Rate: 7.108516483516484e-06\n",
      "Batch 8,450 of 9,631 Elased 0:59:58. Training loss: 3.2141343112245817. Learning Rate: 7.091172716172717e-06\n",
      "Batch 8,500 of 9,631 Elased 1:00:19. Training loss: 3.213799908553853. Learning Rate: 7.073828948828949e-06\n",
      "Batch 8,550 of 9,631 Elased 1:00:41. Training loss: 3.213244402366772. Learning Rate: 7.056485181485181e-06\n",
      "Batch 8,600 of 9,631 Elased 1:01:02. Training loss: 3.213863827184189. Learning Rate: 7.0391414141414144e-06\n",
      "Batch 8,650 of 9,631 Elased 1:01:23. Training loss: 3.2129094123564705. Learning Rate: 7.021797646797647e-06\n",
      "Batch 8,700 of 9,631 Elased 1:01:44. Training loss: 3.2127310929627253. Learning Rate: 7.004453879453881e-06\n",
      "Batch 8,750 of 9,631 Elased 1:02:06. Training loss: 3.212551269381387. Learning Rate: 6.987110112110112e-06\n",
      "Batch 8,800 of 9,631 Elased 1:02:27. Training loss: 3.212435763004151. Learning Rate: 6.969766344766344e-06\n",
      "Batch 8,850 of 9,631 Elased 1:02:48. Training loss: 3.2120223695140773. Learning Rate: 6.952422577422578e-06\n",
      "Batch 8,900 of 9,631 Elased 1:03:10. Training loss: 3.2120282655619503. Learning Rate: 6.93507881007881e-06\n",
      "Batch 8,950 of 9,631 Elased 1:03:31. Training loss: 3.2115597308015023. Learning Rate: 6.917735042735044e-06\n",
      "Batch 9,000 of 9,631 Elased 1:03:52. Training loss: 3.210645041571723. Learning Rate: 6.900391275391276e-06\n",
      "Batch 9,050 of 9,631 Elased 1:04:13. Training loss: 3.2116065681836883. Learning Rate: 6.883047508047509e-06\n",
      "Batch 9,100 of 9,631 Elased 1:04:35. Training loss: 3.211501605982309. Learning Rate: 6.865703740703741e-06\n",
      "Batch 9,150 of 9,631 Elased 1:04:56. Training loss: 3.211668867309237. Learning Rate: 6.848359973359973e-06\n",
      "Batch 9,200 of 9,631 Elased 1:05:17. Training loss: 3.2123823095663733. Learning Rate: 6.831016206016206e-06\n",
      "Batch 9,250 of 9,631 Elased 1:05:38. Training loss: 3.212094082342612. Learning Rate: 6.813672438672439e-06\n",
      "Batch 9,300 of 9,631 Elased 1:05:59. Training loss: 3.2125041004662873. Learning Rate: 6.7963286713286725e-06\n",
      "Batch 9,350 of 9,631 Elased 1:06:20. Training loss: 3.2125960182506135. Learning Rate: 6.778984903984904e-06\n",
      "Batch 9,400 of 9,631 Elased 1:06:41. Training loss: 3.212471803731107. Learning Rate: 6.761641136641136e-06\n",
      "Batch 9,450 of 9,631 Elased 1:07:02. Training loss: 3.212455299871939. Learning Rate: 6.74429736929737e-06\n",
      "Batch 9,500 of 9,631 Elased 1:07:23. Training loss: 3.211987625548714. Learning Rate: 6.726953601953602e-06\n",
      "Batch 9,550 of 9,631 Elased 1:07:44. Training loss: 3.2114080904166737. Learning Rate: 6.709609834609836e-06\n",
      "Batch 9,600 of 9,631 Elased 1:08:05. Training loss: 3.211834959263603. Learning Rate: 6.6922660672660676e-06\n",
      "\n",
      "\n",
      "  Average training loss: 3.21\n",
      "  Training epcoh took: 1:08:19\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb20b53b4b9490ead11bbbdf0676d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=67.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Validation Loss: 3.41\n",
      "  Validation took: 0:00:43\n",
      "\n",
      "======== Epoch 29 / 30 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a812f8e8244d019a613e859a5f92d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch    50 of 9,631 Elased 0:00:21. Training loss: 3.2871251773834227. Learning Rate: 6.664169164169165e-06\n",
      "Batch   100 of 9,631 Elased 0:00:42. Training loss: 3.220701284408569. Learning Rate: 6.646825396825397e-06\n",
      "Batch   150 of 9,631 Elased 0:01:03. Training loss: 3.2309389607111614. Learning Rate: 6.629481629481629e-06\n",
      "Batch   200 of 9,631 Elased 0:01:24. Training loss: 3.259866029024124. Learning Rate: 6.612137862137862e-06\n",
      "Batch   250 of 9,631 Elased 0:01:45. Training loss: 3.213871967315674. Learning Rate: 6.594794094794095e-06\n",
      "Batch   300 of 9,631 Elased 0:02:07. Training loss: 3.2040879050890605. Learning Rate: 6.5774503274503285e-06\n",
      "Batch   350 of 9,631 Elased 0:02:28. Training loss: 3.2028891740526473. Learning Rate: 6.56010656010656e-06\n",
      "Batch   400 of 9,631 Elased 0:02:49. Training loss: 3.215782959461212. Learning Rate: 6.542762792762792e-06\n",
      "Batch   450 of 9,631 Elased 0:03:11. Training loss: 3.2211314498053656. Learning Rate: 6.525419025419026e-06\n",
      "Batch   500 of 9,631 Elased 0:03:32. Training loss: 3.226895092964172. Learning Rate: 6.508075258075258e-06\n",
      "Batch   550 of 9,631 Elased 0:03:53. Training loss: 3.214920075156472. Learning Rate: 6.490731490731492e-06\n",
      "Batch   600 of 9,631 Elased 0:04:14. Training loss: 3.229132090806961. Learning Rate: 6.473387723387724e-06\n",
      "Batch   650 of 9,631 Elased 0:04:35. Training loss: 3.23092002428495. Learning Rate: 6.456043956043957e-06\n",
      "Batch   700 of 9,631 Elased 0:04:56. Training loss: 3.2279184876169476. Learning Rate: 6.438700188700189e-06\n",
      "Batch   750 of 9,631 Elased 0:05:18. Training loss: 3.2172475417455035. Learning Rate: 6.4213564213564216e-06\n",
      "Batch   800 of 9,631 Elased 0:05:39. Training loss: 3.2153253369033337. Learning Rate: 6.404012654012655e-06\n",
      "Batch   850 of 9,631 Elased 0:06:00. Training loss: 3.2082591181642868. Learning Rate: 6.386668886668887e-06\n",
      "Batch   900 of 9,631 Elased 0:06:22. Training loss: 3.2050379971663157. Learning Rate: 6.36932511932512e-06\n",
      "Batch   950 of 9,631 Elased 0:06:43. Training loss: 3.2002305852739434. Learning Rate: 6.351981351981352e-06\n",
      "Batch 1,000 of 9,631 Elased 0:07:04. Training loss: 3.2022823239564895. Learning Rate: 6.334637584637584e-06\n",
      "Batch 1,050 of 9,631 Elased 0:07:25. Training loss: 3.2032249585787453. Learning Rate: 6.3172938172938175e-06\n",
      "Batch 1,100 of 9,631 Elased 0:07:46. Training loss: 3.204924210093238. Learning Rate: 6.29995004995005e-06\n",
      "Batch 1,150 of 9,631 Elased 0:08:08. Training loss: 3.205101630273073. Learning Rate: 6.282606282606284e-06\n",
      "Batch 1,200 of 9,631 Elased 0:08:29. Training loss: 3.205698430140813. Learning Rate: 6.2652625152625155e-06\n",
      "Batch 1,250 of 9,631 Elased 0:08:51. Training loss: 3.206497569465637. Learning Rate: 6.247918747918748e-06\n",
      "Batch 1,300 of 9,631 Elased 0:09:12. Training loss: 3.20293955711218. Learning Rate: 6.230574980574981e-06\n",
      "Batch 1,350 of 9,631 Elased 0:09:33. Training loss: 3.20292069223192. Learning Rate: 6.2132312132312134e-06\n",
      "Batch 1,400 of 9,631 Elased 0:09:54. Training loss: 3.2013856196403503. Learning Rate: 6.195887445887446e-06\n",
      "Batch 1,450 of 9,631 Elased 0:10:16. Training loss: 3.2002160490792373. Learning Rate: 6.178543678543679e-06\n",
      "Batch 1,500 of 9,631 Elased 0:10:37. Training loss: 3.198743443409602. Learning Rate: 6.161199911199911e-06\n",
      "Batch 1,550 of 9,631 Elased 0:10:58. Training loss: 3.1977635358225913. Learning Rate: 6.143856143856144e-06\n",
      "Batch 1,600 of 9,631 Elased 0:11:19. Training loss: 3.2034379480034114. Learning Rate: 6.126512376512377e-06\n",
      "Batch 1,650 of 9,631 Elased 0:11:40. Training loss: 3.205769137396957. Learning Rate: 6.109168609168609e-06\n",
      "Batch 1,700 of 9,631 Elased 0:12:02. Training loss: 3.2030976825601916. Learning Rate: 6.091824841824842e-06\n",
      "Batch 1,750 of 9,631 Elased 0:12:23. Training loss: 3.2023496924809045. Learning Rate: 6.074481074481075e-06\n",
      "Batch 1,800 of 9,631 Elased 0:12:44. Training loss: 3.2023491461409463. Learning Rate: 6.057137307137307e-06\n",
      "Batch 1,850 of 9,631 Elased 0:13:05. Training loss: 3.204263011507086. Learning Rate: 6.03979353979354e-06\n",
      "Batch 1,900 of 9,631 Elased 0:13:26. Training loss: 3.202691664005581. Learning Rate: 6.022449772449773e-06\n",
      "Batch 1,950 of 9,631 Elased 0:13:47. Training loss: 3.1992486437161762. Learning Rate: 6.005106005106005e-06\n",
      "Batch 2,000 of 9,631 Elased 0:14:08. Training loss: 3.195803124845028. Learning Rate: 5.987762237762238e-06\n",
      "Batch 2,050 of 9,631 Elased 0:14:30. Training loss: 3.1966020154953. Learning Rate: 5.970418470418471e-06\n",
      "Batch 2,100 of 9,631 Elased 0:14:52. Training loss: 3.201027125971658. Learning Rate: 5.953074703074703e-06\n",
      "Batch 2,150 of 9,631 Elased 0:15:13. Training loss: 3.2010564695402635. Learning Rate: 5.935730935730936e-06\n",
      "Batch 2,200 of 9,631 Elased 0:15:34. Training loss: 3.200873375480825. Learning Rate: 5.918387168387169e-06\n",
      "Batch 2,250 of 9,631 Elased 0:15:56. Training loss: 3.1988445479075116. Learning Rate: 5.901043401043401e-06\n",
      "Batch 2,300 of 9,631 Elased 0:16:17. Training loss: 3.199860173308331. Learning Rate: 5.883699633699634e-06\n",
      "Batch 2,350 of 9,631 Elased 0:16:38. Training loss: 3.1978918172450777. Learning Rate: 5.8663558663558666e-06\n",
      "Batch 2,400 of 9,631 Elased 0:16:59. Training loss: 3.198119255055984. Learning Rate: 5.849012099012099e-06\n",
      "Batch 2,450 of 9,631 Elased 0:17:20. Training loss: 3.195511923322872. Learning Rate: 5.831668331668332e-06\n",
      "Batch 2,500 of 9,631 Elased 0:17:41. Training loss: 3.1946447615623472. Learning Rate: 5.8143245643245645e-06\n",
      "Batch 2,550 of 9,631 Elased 0:18:03. Training loss: 3.1993188941244988. Learning Rate: 5.796980796980797e-06\n",
      "Batch 2,600 of 9,631 Elased 0:18:24. Training loss: 3.202233993823712. Learning Rate: 5.77963702963703e-06\n",
      "Batch 2,650 of 9,631 Elased 0:18:45. Training loss: 3.2049215884478586. Learning Rate: 5.7622932622932625e-06\n",
      "Batch 2,700 of 9,631 Elased 0:19:06. Training loss: 3.206907257857146. Learning Rate: 5.744949494949495e-06\n",
      "Batch 2,750 of 9,631 Elased 0:19:28. Training loss: 3.2047347559495405. Learning Rate: 5.727605727605728e-06\n",
      "Batch 2,800 of 9,631 Elased 0:19:50. Training loss: 3.205611280628613. Learning Rate: 5.7102619602619605e-06\n",
      "Batch 2,850 of 9,631 Elased 0:20:11. Training loss: 3.208300951907509. Learning Rate: 5.692918192918193e-06\n",
      "Batch 2,900 of 9,631 Elased 0:20:31. Training loss: 3.2085775597342128. Learning Rate: 5.675574425574426e-06\n",
      "Batch 2,950 of 9,631 Elased 0:20:53. Training loss: 3.2086119840104703. Learning Rate: 5.6582306582306584e-06\n",
      "Batch 3,000 of 9,631 Elased 0:21:14. Training loss: 3.211661137898763. Learning Rate: 5.640886890886891e-06\n",
      "Batch 3,050 of 9,631 Elased 0:21:35. Training loss: 3.2115594244784997. Learning Rate: 5.623543123543124e-06\n",
      "Batch 3,100 of 9,631 Elased 0:21:56. Training loss: 3.2115034160306375. Learning Rate: 5.606199356199356e-06\n",
      "Batch 3,150 of 9,631 Elased 0:22:17. Training loss: 3.2079171290473334. Learning Rate: 5.588855588855589e-06\n",
      "Batch 3,200 of 9,631 Elased 0:22:38. Training loss: 3.2101155032962563. Learning Rate: 5.571511821511822e-06\n",
      "Batch 3,250 of 9,631 Elased 0:22:58. Training loss: 3.209870227446923. Learning Rate: 5.554168054168054e-06\n",
      "Batch 3,300 of 9,631 Elased 0:23:20. Training loss: 3.2122710008332223. Learning Rate: 5.536824286824287e-06\n",
      "Batch 3,350 of 9,631 Elased 0:23:41. Training loss: 3.212681871029868. Learning Rate: 5.51948051948052e-06\n",
      "Batch 3,400 of 9,631 Elased 0:24:03. Training loss: 3.2130786273058725. Learning Rate: 5.502136752136752e-06\n",
      "Batch 3,450 of 9,631 Elased 0:24:24. Training loss: 3.2133321406184763. Learning Rate: 5.484792984792985e-06\n",
      "Batch 3,500 of 9,631 Elased 0:24:45. Training loss: 3.212774319921221. Learning Rate: 5.467449217449218e-06\n",
      "Batch 3,550 of 9,631 Elased 0:25:07. Training loss: 3.2140110722394057. Learning Rate: 5.45010545010545e-06\n",
      "Batch 3,600 of 9,631 Elased 0:25:28. Training loss: 3.2158375549316407. Learning Rate: 5.432761682761683e-06\n",
      "Batch 3,650 of 9,631 Elased 0:25:49. Training loss: 3.2191981922437067. Learning Rate: 5.415417915417916e-06\n",
      "Batch 3,700 of 9,631 Elased 0:26:10. Training loss: 3.2179506938844114. Learning Rate: 5.398074148074148e-06\n",
      "Batch 3,750 of 9,631 Elased 0:26:32. Training loss: 3.2167236095746357. Learning Rate: 5.380730380730381e-06\n",
      "Batch 3,800 of 9,631 Elased 0:26:53. Training loss: 3.2154161947338205. Learning Rate: 5.363386613386614e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3,850 of 9,631 Elased 0:27:14. Training loss: 3.2131452333462702. Learning Rate: 5.346042846042846e-06\n",
      "Batch 3,900 of 9,631 Elased 0:27:35. Training loss: 3.2119053492179286. Learning Rate: 5.328699078699079e-06\n",
      "Batch 3,950 of 9,631 Elased 0:27:56. Training loss: 3.2132057108456578. Learning Rate: 5.3113553113553116e-06\n",
      "Batch 4,000 of 9,631 Elased 0:28:17. Training loss: 3.21343224221468. Learning Rate: 5.294011544011544e-06\n",
      "Batch 4,050 of 9,631 Elased 0:28:38. Training loss: 3.212686151104209. Learning Rate: 5.276667776667777e-06\n",
      "Batch 4,100 of 9,631 Elased 0:28:59. Training loss: 3.2119735997479135. Learning Rate: 5.2593240093240095e-06\n",
      "Batch 4,150 of 9,631 Elased 0:29:21. Training loss: 3.21171028797885. Learning Rate: 5.241980241980242e-06\n",
      "Batch 4,200 of 9,631 Elased 0:29:44. Training loss: 3.2116330343201045. Learning Rate: 5.224636474636475e-06\n",
      "Batch 4,250 of 9,631 Elased 0:30:07. Training loss: 3.212186593392316. Learning Rate: 5.2072927072927075e-06\n",
      "Batch 4,300 of 9,631 Elased 0:30:29. Training loss: 3.2118352861182635. Learning Rate: 5.18994893994894e-06\n",
      "Batch 4,350 of 9,631 Elased 0:30:52. Training loss: 3.211594086296257. Learning Rate: 5.172605172605173e-06\n",
      "Batch 4,400 of 9,631 Elased 0:31:15. Training loss: 3.2128260816227305. Learning Rate: 5.1552614052614055e-06\n",
      "Batch 4,450 of 9,631 Elased 0:31:37. Training loss: 3.213301115089588. Learning Rate: 5.137917637917638e-06\n",
      "Batch 4,500 of 9,631 Elased 0:32:00. Training loss: 3.2145911651452383. Learning Rate: 5.120573870573871e-06\n",
      "Batch 4,550 of 9,631 Elased 0:32:22. Training loss: 3.2133027927954116. Learning Rate: 5.1032301032301034e-06\n",
      "Batch 4,600 of 9,631 Elased 0:32:45. Training loss: 3.214434988524603. Learning Rate: 5.085886335886336e-06\n",
      "Batch 4,650 of 9,631 Elased 0:33:08. Training loss: 3.2138575091669637. Learning Rate: 5.068542568542569e-06\n",
      "Batch 4,700 of 9,631 Elased 0:33:30. Training loss: 3.2142491420532795. Learning Rate: 5.051198801198801e-06\n",
      "Batch 4,750 of 9,631 Elased 0:33:53. Training loss: 3.2144641641064693. Learning Rate: 5.033855033855034e-06\n",
      "Batch 4,800 of 9,631 Elased 0:34:16. Training loss: 3.212928618093332. Learning Rate: 5.016511266511267e-06\n",
      "Batch 4,850 of 9,631 Elased 0:34:39. Training loss: 3.212549274729699. Learning Rate: 4.999167499167499e-06\n",
      "Batch 4,900 of 9,631 Elased 0:35:01. Training loss: 3.213428894208402. Learning Rate: 4.981823731823732e-06\n",
      "Batch 4,950 of 9,631 Elased 0:35:24. Training loss: 3.213584751215848. Learning Rate: 4.964479964479965e-06\n",
      "Batch 5,000 of 9,631 Elased 0:35:46. Training loss: 3.2144031792879106. Learning Rate: 4.947136197136197e-06\n",
      "Batch 5,050 of 9,631 Elased 0:36:09. Training loss: 3.2140516666138526. Learning Rate: 4.92979242979243e-06\n",
      "Batch 5,100 of 9,631 Elased 0:36:32. Training loss: 3.212339863169427. Learning Rate: 4.912448662448663e-06\n",
      "Batch 5,150 of 9,631 Elased 0:36:54. Training loss: 3.212820511304059. Learning Rate: 4.895104895104895e-06\n",
      "Batch 5,200 of 9,631 Elased 0:37:17. Training loss: 3.2122674707953744. Learning Rate: 4.877761127761128e-06\n",
      "Batch 5,250 of 9,631 Elased 0:37:40. Training loss: 3.212412612574441. Learning Rate: 4.860417360417361e-06\n",
      "Batch 5,300 of 9,631 Elased 0:38:02. Training loss: 3.2121515641347416. Learning Rate: 4.843073593073593e-06\n",
      "Batch 5,350 of 9,631 Elased 0:38:25. Training loss: 3.2140682885134333. Learning Rate: 4.825729825729826e-06\n",
      "Batch 5,400 of 9,631 Elased 0:38:47. Training loss: 3.214233825162605. Learning Rate: 4.808386058386059e-06\n",
      "Batch 5,450 of 9,631 Elased 0:39:10. Training loss: 3.214238902888167. Learning Rate: 4.791042291042291e-06\n",
      "Batch 5,500 of 9,631 Elased 0:39:33. Training loss: 3.214543745431033. Learning Rate: 4.773698523698524e-06\n",
      "Batch 5,550 of 9,631 Elased 0:39:55. Training loss: 3.2148341629526636. Learning Rate: 4.7563547563547566e-06\n",
      "Batch 5,600 of 9,631 Elased 0:40:18. Training loss: 3.2141196258366107. Learning Rate: 4.739010989010989e-06\n",
      "Batch 5,650 of 9,631 Elased 0:40:41. Training loss: 3.21563726819722. Learning Rate: 4.721667221667222e-06\n",
      "Batch 5,700 of 9,631 Elased 0:41:03. Training loss: 3.215559460384804. Learning Rate: 4.7043234543234545e-06\n",
      "Batch 5,750 of 9,631 Elased 0:41:26. Training loss: 3.2153569215484286. Learning Rate: 4.686979686979687e-06\n",
      "Batch 5,800 of 9,631 Elased 0:41:48. Training loss: 3.2156653144236267. Learning Rate: 4.66963591963592e-06\n",
      "Batch 5,850 of 9,631 Elased 0:42:11. Training loss: 3.216021421118679. Learning Rate: 4.6522921522921525e-06\n",
      "Batch 5,900 of 9,631 Elased 0:42:34. Training loss: 3.215988728171688. Learning Rate: 4.634948384948385e-06\n",
      "Batch 5,950 of 9,631 Elased 0:42:56. Training loss: 3.216248299875179. Learning Rate: 4.617604617604618e-06\n",
      "Batch 6,000 of 9,631 Elased 0:43:19. Training loss: 3.2158424831430117. Learning Rate: 4.6002608502608505e-06\n",
      "Batch 6,050 of 9,631 Elased 0:43:42. Training loss: 3.216265024803887. Learning Rate: 4.582917082917083e-06\n",
      "Batch 6,100 of 9,631 Elased 0:44:04. Training loss: 3.216532223341895. Learning Rate: 4.565573315573316e-06\n",
      "Batch 6,150 of 9,631 Elased 0:44:27. Training loss: 3.2164702124711946. Learning Rate: 4.5482295482295484e-06\n",
      "Batch 6,200 of 9,631 Elased 0:44:50. Training loss: 3.2147347068978895. Learning Rate: 4.530885780885781e-06\n",
      "Batch 6,250 of 9,631 Elased 0:45:12. Training loss: 3.214524844264984. Learning Rate: 4.513542013542014e-06\n",
      "Batch 6,300 of 9,631 Elased 0:45:35. Training loss: 3.2144532358835614. Learning Rate: 4.496198246198246e-06\n",
      "Batch 6,350 of 9,631 Elased 0:45:58. Training loss: 3.213968419427947. Learning Rate: 4.478854478854479e-06\n",
      "Batch 6,400 of 9,631 Elased 0:46:20. Training loss: 3.2136253701150417. Learning Rate: 4.461510711510712e-06\n",
      "Batch 6,450 of 9,631 Elased 0:46:43. Training loss: 3.214595478375753. Learning Rate: 4.444166944166944e-06\n",
      "Batch 6,500 of 9,631 Elased 0:47:05. Training loss: 3.2131771663335655. Learning Rate: 4.426823176823177e-06\n",
      "Batch 6,550 of 9,631 Elased 0:47:28. Training loss: 3.2135852879058313. Learning Rate: 4.40947940947941e-06\n",
      "Batch 6,600 of 9,631 Elased 0:47:51. Training loss: 3.2145539193984236. Learning Rate: 4.392135642135642e-06\n",
      "Batch 6,650 of 9,631 Elased 0:48:13. Training loss: 3.214870764707264. Learning Rate: 4.374791874791875e-06\n",
      "Batch 6,700 of 9,631 Elased 0:48:36. Training loss: 3.215879605652681. Learning Rate: 4.357448107448108e-06\n",
      "Batch 6,750 of 9,631 Elased 0:48:58. Training loss: 3.215970682338432. Learning Rate: 4.34010434010434e-06\n",
      "Batch 6,800 of 9,631 Elased 0:49:21. Training loss: 3.2157869821611573. Learning Rate: 4.322760572760573e-06\n",
      "Batch 6,850 of 9,631 Elased 0:49:43. Training loss: 3.2153010422817982. Learning Rate: 4.305416805416806e-06\n",
      "Batch 6,900 of 9,631 Elased 0:50:06. Training loss: 3.2150811341016188. Learning Rate: 4.288073038073038e-06\n",
      "Batch 6,950 of 9,631 Elased 0:50:29. Training loss: 3.215185234032089. Learning Rate: 4.270729270729271e-06\n",
      "Batch 7,000 of 9,631 Elased 0:50:52. Training loss: 3.214442187190056. Learning Rate: 4.253385503385504e-06\n",
      "Batch 7,050 of 9,631 Elased 0:51:14. Training loss: 3.2136299622819777. Learning Rate: 4.236041736041736e-06\n",
      "Batch 7,100 of 9,631 Elased 0:51:37. Training loss: 3.2136945624586564. Learning Rate: 4.218697968697969e-06\n",
      "Batch 7,150 of 9,631 Elased 0:52:00. Training loss: 3.213550054860282. Learning Rate: 4.2013542013542016e-06\n",
      "Batch 7,200 of 9,631 Elased 0:52:23. Training loss: 3.2136676243113147. Learning Rate: 4.184010434010434e-06\n",
      "Batch 7,250 of 9,631 Elased 0:52:45. Training loss: 3.212674197312059. Learning Rate: 4.166666666666667e-06\n",
      "Batch 7,300 of 9,631 Elased 0:53:08. Training loss: 3.2132896770189885. Learning Rate: 4.1493228993228995e-06\n",
      "Batch 7,350 of 9,631 Elased 0:53:31. Training loss: 3.2126985027352157. Learning Rate: 4.131979131979132e-06\n",
      "Batch 7,400 of 9,631 Elased 0:53:53. Training loss: 3.211660702260765. Learning Rate: 4.114635364635365e-06\n",
      "Batch 7,450 of 9,631 Elased 0:54:16. Training loss: 3.2117133611320647. Learning Rate: 4.0972915972915975e-06\n",
      "Batch 7,500 of 9,631 Elased 0:54:38. Training loss: 3.210755192565918. Learning Rate: 4.07994782994783e-06\n",
      "Batch 7,550 of 9,631 Elased 0:55:01. Training loss: 3.2106743360197307. Learning Rate: 4.062604062604063e-06\n",
      "Batch 7,600 of 9,631 Elased 0:55:24. Training loss: 3.211049783621964. Learning Rate: 4.0452602952602955e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7,650 of 9,631 Elased 0:55:46. Training loss: 3.2112029646892175. Learning Rate: 4.027916527916528e-06\n",
      "Batch 7,700 of 9,631 Elased 0:56:09. Training loss: 3.21053115166627. Learning Rate: 4.010572760572761e-06\n",
      "Batch 7,750 of 9,631 Elased 0:56:32. Training loss: 3.2089820142253753. Learning Rate: 3.9932289932289934e-06\n",
      "Batch 7,800 of 9,631 Elased 0:56:54. Training loss: 3.209571653635074. Learning Rate: 3.975885225885226e-06\n",
      "Batch 7,850 of 9,631 Elased 0:57:17. Training loss: 3.2099339028680403. Learning Rate: 3.958541458541459e-06\n",
      "Batch 7,900 of 9,631 Elased 0:57:40. Training loss: 3.2088211474841155. Learning Rate: 3.941197691197691e-06\n",
      "Batch 7,950 of 9,631 Elased 0:58:02. Training loss: 3.210049768483864. Learning Rate: 3.923853923853924e-06\n",
      "Batch 8,000 of 9,631 Elased 0:58:25. Training loss: 3.209574041545391. Learning Rate: 3.906510156510157e-06\n",
      "Batch 8,050 of 9,631 Elased 0:58:47. Training loss: 3.210586868961405. Learning Rate: 3.889166389166389e-06\n",
      "Batch 8,100 of 9,631 Elased 0:59:10. Training loss: 3.2106175921287066. Learning Rate: 3.871822621822622e-06\n",
      "Batch 8,150 of 9,631 Elased 0:59:33. Training loss: 3.211487382438285. Learning Rate: 3.854478854478855e-06\n",
      "Batch 8,200 of 9,631 Elased 0:59:55. Training loss: 3.211475373855451. Learning Rate: 3.837135087135087e-06\n",
      "Batch 8,250 of 9,631 Elased 1:00:18. Training loss: 3.212008492787679. Learning Rate: 3.81979131979132e-06\n",
      "Batch 8,300 of 9,631 Elased 1:00:41. Training loss: 3.2123830432776943. Learning Rate: 3.802447552447553e-06\n",
      "Batch 8,350 of 9,631 Elased 1:01:03. Training loss: 3.212034870421815. Learning Rate: 3.785103785103785e-06\n",
      "Batch 8,400 of 9,631 Elased 1:01:26. Training loss: 3.211416788782392. Learning Rate: 3.767760017760018e-06\n",
      "Batch 8,450 of 9,631 Elased 1:01:48. Training loss: 3.210889392406983. Learning Rate: 3.7504162504162506e-06\n",
      "Batch 8,500 of 9,631 Elased 1:02:11. Training loss: 3.210746861513923. Learning Rate: 3.7330724830724833e-06\n",
      "Batch 8,550 of 9,631 Elased 1:02:33. Training loss: 3.210490774731887. Learning Rate: 3.7157287157287164e-06\n",
      "Batch 8,600 of 9,631 Elased 1:02:56. Training loss: 3.211270038457804. Learning Rate: 3.698384948384948e-06\n",
      "Batch 8,650 of 9,631 Elased 1:03:19. Training loss: 3.2103198588514603. Learning Rate: 3.681041181041181e-06\n",
      "Batch 8,700 of 9,631 Elased 1:03:42. Training loss: 3.2100987542771744. Learning Rate: 3.663697413697414e-06\n",
      "Batch 8,750 of 9,631 Elased 1:04:04. Training loss: 3.2099577632904053. Learning Rate: 3.6463536463536466e-06\n",
      "Batch 8,800 of 9,631 Elased 1:04:27. Training loss: 3.2102140835333954. Learning Rate: 3.629009879009879e-06\n",
      "Batch 8,850 of 9,631 Elased 1:04:50. Training loss: 3.209872396019219. Learning Rate: 3.6116661116661123e-06\n",
      "Batch 8,900 of 9,631 Elased 1:05:12. Training loss: 3.2102063971021204. Learning Rate: 3.594322344322344e-06\n",
      "Batch 8,950 of 9,631 Elased 1:05:35. Training loss: 3.209742593885134. Learning Rate: 3.576978576978577e-06\n",
      "Batch 9,000 of 9,631 Elased 1:05:58. Training loss: 3.208749914182557. Learning Rate: 3.55963480963481e-06\n",
      "Batch 9,050 of 9,631 Elased 1:06:20. Training loss: 3.2096105554090677. Learning Rate: 3.5422910422910425e-06\n",
      "Batch 9,100 of 9,631 Elased 1:06:43. Training loss: 3.2092864004989248. Learning Rate: 3.524947274947275e-06\n",
      "Batch 9,150 of 9,631 Elased 1:07:05. Training loss: 3.2095106879348965. Learning Rate: 3.5076035076035082e-06\n",
      "Batch 9,200 of 9,631 Elased 1:07:28. Training loss: 3.210051351111868. Learning Rate: 3.49025974025974e-06\n",
      "Batch 9,250 of 9,631 Elased 1:07:51. Training loss: 3.209779292776778. Learning Rate: 3.472915972915973e-06\n",
      "Batch 9,300 of 9,631 Elased 1:08:13. Training loss: 3.21017770267302. Learning Rate: 3.4555722055722058e-06\n",
      "Batch 9,350 of 9,631 Elased 1:08:36. Training loss: 3.2100510703306147. Learning Rate: 3.4382284382284384e-06\n",
      "Batch 9,400 of 9,631 Elased 1:08:59. Training loss: 3.2098739472602276. Learning Rate: 3.4208846708846715e-06\n",
      "Batch 9,450 of 9,631 Elased 1:09:22. Training loss: 3.2096788965205034. Learning Rate: 3.4035409035409033e-06\n",
      "Batch 9,500 of 9,631 Elased 1:09:44. Training loss: 3.209294637002443. Learning Rate: 3.3861971361971364e-06\n",
      "Batch 9,550 of 9,631 Elased 1:10:07. Training loss: 3.2087278485672637. Learning Rate: 3.368853368853369e-06\n",
      "Batch 9,600 of 9,631 Elased 1:10:31. Training loss: 3.2092117266108593. Learning Rate: 3.3515096015096017e-06\n",
      "\n",
      "\n",
      "  Average training loss: 3.21\n",
      "  Training epcoh took: 1:10:45\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3077e1e043124f9b8673687bf5be95ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=67.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Validation Loss: 3.40\n",
      "  Validation took: 0:00:46\n",
      "\n",
      "======== Epoch 30 / 30 ========\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2466ffe1aa24185828817ebe0e4a377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch    50 of 9,631 Elased 0:00:24. Training loss: 3.296131315231323. Learning Rate: 3.3234126984126985e-06\n",
      "Batch   100 of 9,631 Elased 0:00:47. Training loss: 3.2367349755764008. Learning Rate: 3.306068931068931e-06\n",
      "Batch   150 of 9,631 Elased 0:01:11. Training loss: 3.243235325018565. Learning Rate: 3.2887251637251642e-06\n",
      "Batch   200 of 9,631 Elased 0:01:34. Training loss: 3.265122215151787. Learning Rate: 3.271381396381396e-06\n",
      "Batch   250 of 9,631 Elased 0:01:57. Training loss: 3.219876080036163. Learning Rate: 3.254037629037629e-06\n",
      "Batch   300 of 9,631 Elased 0:02:19. Training loss: 3.205914080540339. Learning Rate: 3.236693861693862e-06\n",
      "Batch   350 of 9,631 Elased 0:02:42. Training loss: 3.207563124043601. Learning Rate: 3.2193500943500944e-06\n",
      "Batch   400 of 9,631 Elased 0:03:04. Training loss: 3.2191692239046095. Learning Rate: 3.2020063270063275e-06\n",
      "Batch   450 of 9,631 Elased 0:03:27. Training loss: 3.2228533368640475. Learning Rate: 3.18466255966256e-06\n",
      "Batch   500 of 9,631 Elased 0:03:50. Training loss: 3.228385564804077. Learning Rate: 3.167318792318792e-06\n",
      "Batch   550 of 9,631 Elased 0:04:12. Training loss: 3.216770078052174. Learning Rate: 3.149975024975025e-06\n",
      "Batch   600 of 9,631 Elased 0:04:35. Training loss: 3.23001417974631. Learning Rate: 3.1326312576312577e-06\n",
      "Batch   650 of 9,631 Elased 0:04:58. Training loss: 3.229693615069756. Learning Rate: 3.1152874902874904e-06\n",
      "Batch   700 of 9,631 Elased 0:05:20. Training loss: 3.2230341875553132. Learning Rate: 3.097943722943723e-06\n",
      "Batch   750 of 9,631 Elased 0:05:43. Training loss: 3.2122505332628886. Learning Rate: 3.0805999555999557e-06\n",
      "Batch   800 of 9,631 Elased 0:06:06. Training loss: 3.209524488300085. Learning Rate: 3.0632561882561884e-06\n",
      "Batch   850 of 9,631 Elased 0:06:28. Training loss: 3.201287111534792. Learning Rate: 3.045912420912421e-06\n",
      "Batch   900 of 9,631 Elased 0:06:50. Training loss: 3.198278163406584. Learning Rate: 3.0285686535686537e-06\n",
      "Batch   950 of 9,631 Elased 0:07:13. Training loss: 3.1941763715994984. Learning Rate: 3.0112248862248863e-06\n",
      "Batch 1,000 of 9,631 Elased 0:07:35. Training loss: 3.1954917734861374. Learning Rate: 2.993881118881119e-06\n",
      "Batch 1,050 of 9,631 Elased 0:07:58. Training loss: 3.1977555353300913. Learning Rate: 2.9765373515373516e-06\n",
      "Batch 1,100 of 9,631 Elased 0:08:21. Training loss: 3.199761345711621. Learning Rate: 2.9591935841935843e-06\n",
      "Batch 1,150 of 9,631 Elased 0:08:43. Training loss: 3.1998951535639555. Learning Rate: 2.941849816849817e-06\n",
      "Batch 1,200 of 9,631 Elased 0:09:07. Training loss: 3.200660430093606. Learning Rate: 2.9245060495060496e-06\n",
      "Batch 1,250 of 9,631 Elased 0:09:29. Training loss: 3.2023944657325742. Learning Rate: 2.9071622821622823e-06\n",
      "Batch 1,300 of 9,631 Elased 0:09:52. Training loss: 3.1972304950310635. Learning Rate: 2.889818514818515e-06\n",
      "Batch 1,350 of 9,631 Elased 0:10:14. Training loss: 3.1964524882811087. Learning Rate: 2.8724747474747476e-06\n",
      "Batch 1,400 of 9,631 Elased 0:10:37. Training loss: 3.1959638429539545. Learning Rate: 2.8551309801309802e-06\n",
      "Batch 1,450 of 9,631 Elased 0:11:00. Training loss: 3.1938415511723224. Learning Rate: 2.837787212787213e-06\n",
      "Batch 1,500 of 9,631 Elased 0:11:22. Training loss: 3.192064479033152. Learning Rate: 2.8204434454434455e-06\n",
      "Batch 1,550 of 9,631 Elased 0:11:45. Training loss: 3.191984612711014. Learning Rate: 2.803099678099678e-06\n",
      "Batch 1,600 of 9,631 Elased 0:12:08. Training loss: 3.197743290960789. Learning Rate: 2.785755910755911e-06\n",
      "Batch 1,650 of 9,631 Elased 0:12:30. Training loss: 3.200609372095628. Learning Rate: 2.7684121434121435e-06\n",
      "Batch 1,700 of 9,631 Elased 0:12:52. Training loss: 3.1981183495241052. Learning Rate: 2.751068376068376e-06\n",
      "Batch 1,750 of 9,631 Elased 0:13:15. Training loss: 3.1970517797470093. Learning Rate: 2.733724608724609e-06\n",
      "Batch 1,800 of 9,631 Elased 0:13:38. Training loss: 3.196165577901734. Learning Rate: 2.7163808413808415e-06\n",
      "Batch 1,850 of 9,631 Elased 0:14:00. Training loss: 3.1984732435200667. Learning Rate: 2.699037074037074e-06\n",
      "Batch 1,900 of 9,631 Elased 0:14:23. Training loss: 3.1981320931409534. Learning Rate: 2.681693306693307e-06\n",
      "Batch 1,950 of 9,631 Elased 0:14:45. Training loss: 3.1944052462088757. Learning Rate: 2.6643495393495394e-06\n",
      "Batch 2,000 of 9,631 Elased 0:15:08. Training loss: 3.1907785779833793. Learning Rate: 2.647005772005772e-06\n",
      "Batch 2,050 of 9,631 Elased 0:15:30. Training loss: 3.1913733168927636. Learning Rate: 2.6296620046620048e-06\n",
      "Batch 2,100 of 9,631 Elased 0:15:53. Training loss: 3.194908332427343. Learning Rate: 2.6123182373182374e-06\n",
      "Batch 2,150 of 9,631 Elased 0:16:16. Training loss: 3.1949179191922035. Learning Rate: 2.59497446997447e-06\n",
      "Batch 2,200 of 9,631 Elased 0:16:38. Training loss: 3.195016863291914. Learning Rate: 2.5776307026307027e-06\n",
      "Batch 2,250 of 9,631 Elased 0:17:01. Training loss: 3.1931325312190584. Learning Rate: 2.5602869352869354e-06\n",
      "Batch 2,300 of 9,631 Elased 0:17:24. Training loss: 3.1944148067287776. Learning Rate: 2.542943167943168e-06\n",
      "Batch 2,350 of 9,631 Elased 0:17:46. Training loss: 3.1936942581927523. Learning Rate: 2.5255994005994007e-06\n",
      "Batch 2,400 of 9,631 Elased 0:18:09. Training loss: 3.193375742584467. Learning Rate: 2.5082556332556334e-06\n",
      "Batch 2,450 of 9,631 Elased 0:18:32. Training loss: 3.190124608886485. Learning Rate: 2.490911865911866e-06\n",
      "Batch 2,500 of 9,631 Elased 0:18:54. Training loss: 3.1885996774196625. Learning Rate: 2.4735680985680987e-06\n",
      "Batch 2,550 of 9,631 Elased 0:19:17. Training loss: 3.1926984236754623. Learning Rate: 2.4562243312243313e-06\n",
      "Batch 2,600 of 9,631 Elased 0:19:39. Training loss: 3.1959680936428216. Learning Rate: 2.438880563880564e-06\n",
      "Batch 2,650 of 9,631 Elased 0:20:02. Training loss: 3.199188714522236. Learning Rate: 2.4215367965367966e-06\n",
      "Batch 2,700 of 9,631 Elased 0:20:24. Training loss: 3.2009930780640357. Learning Rate: 2.4041930291930293e-06\n",
      "Batch 2,750 of 9,631 Elased 0:20:47. Training loss: 3.1989073436476967. Learning Rate: 2.386849261849262e-06\n",
      "Batch 2,800 of 9,631 Elased 0:21:09. Training loss: 3.199909235196454. Learning Rate: 2.3695054945054946e-06\n",
      "Batch 2,850 of 9,631 Elased 0:21:32. Training loss: 3.2028312605724. Learning Rate: 2.3521617271617273e-06\n",
      "Batch 2,900 of 9,631 Elased 0:21:54. Training loss: 3.2030920955641515. Learning Rate: 2.33481795981796e-06\n",
      "Batch 2,950 of 9,631 Elased 0:22:17. Training loss: 3.203626137628394. Learning Rate: 2.3174741924741926e-06\n",
      "Batch 3,000 of 9,631 Elased 0:22:39. Training loss: 3.206672385017077. Learning Rate: 2.3001304251304252e-06\n",
      "Batch 3,050 of 9,631 Elased 0:23:02. Training loss: 3.206048031986737. Learning Rate: 2.282786657786658e-06\n",
      "Batch 3,100 of 9,631 Elased 0:23:24. Training loss: 3.2063549081356295. Learning Rate: 2.2654428904428905e-06\n",
      "Batch 3,150 of 9,631 Elased 0:23:47. Training loss: 3.203097440401713. Learning Rate: 2.248099123099123e-06\n",
      "Batch 3,200 of 9,631 Elased 0:24:09. Training loss: 3.20577911991626. Learning Rate: 2.230755355755356e-06\n",
      "Batch 3,250 of 9,631 Elased 0:24:32. Training loss: 3.2054212869864243. Learning Rate: 2.2134115884115885e-06\n",
      "Batch 3,300 of 9,631 Elased 0:24:55. Training loss: 3.207388170560201. Learning Rate: 2.196067821067821e-06\n",
      "Batch 3,350 of 9,631 Elased 0:25:17. Training loss: 3.20794440319289. Learning Rate: 2.178724053724054e-06\n",
      "Batch 3,400 of 9,631 Elased 0:25:40. Training loss: 3.2081358634023105. Learning Rate: 2.1613802863802865e-06\n",
      "Batch 3,450 of 9,631 Elased 0:26:02. Training loss: 3.208373226635698. Learning Rate: 2.144036519036519e-06\n",
      "Batch 3,500 of 9,631 Elased 0:26:25. Training loss: 3.207558004106794. Learning Rate: 2.126692751692752e-06\n",
      "Batch 3,550 of 9,631 Elased 0:26:48. Training loss: 3.208701364893309. Learning Rate: 2.1093489843489844e-06\n",
      "Batch 3,600 of 9,631 Elased 0:27:11. Training loss: 3.2103001654810375. Learning Rate: 2.092005217005217e-06\n",
      "Batch 3,650 of 9,631 Elased 0:27:33. Training loss: 3.2136782077240618. Learning Rate: 2.0746614496614498e-06\n",
      "Batch 3,700 of 9,631 Elased 0:27:56. Training loss: 3.212952918523067. Learning Rate: 2.0573176823176824e-06\n",
      "Batch 3,750 of 9,631 Elased 0:28:19. Training loss: 3.211612396589915. Learning Rate: 2.039973914973915e-06\n",
      "Batch 3,800 of 9,631 Elased 0:28:42. Training loss: 3.21030594151271. Learning Rate: 2.0226301476301477e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3,850 of 9,631 Elased 0:29:04. Training loss: 3.2082172893239305. Learning Rate: 2.0052863802863804e-06\n",
      "Batch 3,900 of 9,631 Elased 0:29:27. Training loss: 3.206952193822616. Learning Rate: 1.987942612942613e-06\n",
      "Batch 3,950 of 9,631 Elased 0:29:49. Training loss: 3.207677463338345. Learning Rate: 1.9705988455988457e-06\n",
      "Batch 4,000 of 9,631 Elased 0:30:12. Training loss: 3.2081917875409127. Learning Rate: 1.9532550782550784e-06\n",
      "Batch 4,050 of 9,631 Elased 0:30:34. Training loss: 3.207465637701529. Learning Rate: 1.935911310911311e-06\n",
      "Batch 4,100 of 9,631 Elased 0:30:57. Training loss: 3.2069586594511823. Learning Rate: 1.9185675435675437e-06\n",
      "Batch 4,150 of 9,631 Elased 0:31:20. Training loss: 3.206935509084219. Learning Rate: 1.9012237762237765e-06\n",
      "Batch 4,200 of 9,631 Elased 0:31:42. Training loss: 3.2066178610211327. Learning Rate: 1.883880008880009e-06\n",
      "Batch 4,250 of 9,631 Elased 0:32:05. Training loss: 3.206836882394903. Learning Rate: 1.8665362415362416e-06\n",
      "Batch 4,300 of 9,631 Elased 0:32:27. Training loss: 3.206780231913855. Learning Rate: 1.849192474192474e-06\n",
      "Batch 4,350 of 9,631 Elased 0:32:50. Training loss: 3.2058991402319106. Learning Rate: 1.831848706848707e-06\n",
      "Batch 4,400 of 9,631 Elased 0:33:13. Training loss: 3.2073040201718155. Learning Rate: 1.8145049395049396e-06\n",
      "Batch 4,450 of 9,631 Elased 0:33:35. Training loss: 3.208407460678829. Learning Rate: 1.797161172161172e-06\n",
      "Batch 4,500 of 9,631 Elased 0:33:58. Training loss: 3.209674371957779. Learning Rate: 1.779817404817405e-06\n",
      "Batch 4,550 of 9,631 Elased 0:34:21. Training loss: 3.207984811735677. Learning Rate: 1.7624736374736376e-06\n",
      "Batch 4,600 of 9,631 Elased 0:34:44. Training loss: 3.2092246325637985. Learning Rate: 1.74512987012987e-06\n",
      "Batch 4,650 of 9,631 Elased 0:35:06. Training loss: 3.2086412691300916. Learning Rate: 1.7277861027861029e-06\n",
      "Batch 4,700 of 9,631 Elased 0:35:29. Training loss: 3.2090731877976277. Learning Rate: 1.7104423354423358e-06\n",
      "Batch 4,750 of 9,631 Elased 0:35:52. Training loss: 3.2094440910941677. Learning Rate: 1.6930985680985682e-06\n",
      "Batch 4,800 of 9,631 Elased 0:36:14. Training loss: 3.2078131855775913. Learning Rate: 1.6757548007548009e-06\n",
      "Batch 4,850 of 9,631 Elased 0:36:37. Training loss: 3.2073978960636964. Learning Rate: 1.6584110334110337e-06\n",
      "Batch 4,900 of 9,631 Elased 0:37:00. Training loss: 3.208029882883539. Learning Rate: 1.6410672660672662e-06\n",
      "Batch 4,950 of 9,631 Elased 0:37:22. Training loss: 3.2084548471190715. Learning Rate: 1.6237234987234988e-06\n",
      "Batch 5,000 of 9,631 Elased 0:37:45. Training loss: 3.2095400683164597. Learning Rate: 1.6063797313797313e-06\n",
      "Batch 5,050 of 9,631 Elased 0:38:07. Training loss: 3.2089413021578648. Learning Rate: 1.5890359640359641e-06\n",
      "Batch 5,100 of 9,631 Elased 0:38:30. Training loss: 3.207341133192474. Learning Rate: 1.5716921966921968e-06\n",
      "Batch 5,150 of 9,631 Elased 0:38:52. Training loss: 3.208029637568205. Learning Rate: 1.5543484293484294e-06\n",
      "Batch 5,200 of 9,631 Elased 0:39:15. Training loss: 3.20731311752246. Learning Rate: 1.537004662004662e-06\n",
      "Batch 5,250 of 9,631 Elased 0:39:38. Training loss: 3.2075288604554677. Learning Rate: 1.5196608946608948e-06\n",
      "Batch 5,300 of 9,631 Elased 0:40:00. Training loss: 3.2071947072586924. Learning Rate: 1.5023171273171274e-06\n",
      "Batch 5,350 of 9,631 Elased 0:40:23. Training loss: 3.2089684545882395. Learning Rate: 1.48497335997336e-06\n",
      "Batch 5,400 of 9,631 Elased 0:40:46. Training loss: 3.2090109139018588. Learning Rate: 1.4676295926295927e-06\n",
      "Batch 5,450 of 9,631 Elased 0:41:09. Training loss: 3.209012621520856. Learning Rate: 1.4502858252858254e-06\n",
      "Batch 5,500 of 9,631 Elased 0:41:31. Training loss: 3.2095412541302766. Learning Rate: 1.432942057942058e-06\n",
      "Batch 5,550 of 9,631 Elased 0:41:53. Training loss: 3.210069306734446. Learning Rate: 1.4155982905982907e-06\n",
      "Batch 5,600 of 9,631 Elased 0:42:16. Training loss: 3.2091883492895534. Learning Rate: 1.3982545232545234e-06\n",
      "Batch 5,650 of 9,631 Elased 0:42:38. Training loss: 3.210931301749913. Learning Rate: 1.380910755910756e-06\n",
      "Batch 5,700 of 9,631 Elased 0:43:01. Training loss: 3.210796539281544. Learning Rate: 1.3635669885669887e-06\n",
      "Batch 5,750 of 9,631 Elased 0:43:23. Training loss: 3.2104948501586916. Learning Rate: 1.3462232212232213e-06\n",
      "Batch 5,800 of 9,631 Elased 0:43:47. Training loss: 3.2106378426634032. Learning Rate: 1.328879453879454e-06\n",
      "Batch 5,850 of 9,631 Elased 0:44:09. Training loss: 3.2112929735020694. Learning Rate: 1.3115356865356866e-06\n",
      "Batch 5,900 of 9,631 Elased 0:44:32. Training loss: 3.21127205759792. Learning Rate: 1.2941919191919193e-06\n",
      "Batch 5,950 of 9,631 Elased 0:44:54. Training loss: 3.2119054164004925. Learning Rate: 1.276848151848152e-06\n",
      "Batch 6,000 of 9,631 Elased 0:45:17. Training loss: 3.2115423808892567. Learning Rate: 1.2595043845043846e-06\n",
      "Batch 6,050 of 9,631 Elased 0:45:40. Training loss: 3.2120932017082025. Learning Rate: 1.2421606171606173e-06\n",
      "Batch 6,100 of 9,631 Elased 0:46:03. Training loss: 3.2124518667674455. Learning Rate: 1.22481684981685e-06\n",
      "Batch 6,150 of 9,631 Elased 0:46:25. Training loss: 3.2122599675015704. Learning Rate: 1.2074730824730824e-06\n",
      "Batch 6,200 of 9,631 Elased 0:46:48. Training loss: 3.2105552086522504. Learning Rate: 1.1901293151293152e-06\n",
      "Batch 6,250 of 9,631 Elased 0:47:11. Training loss: 3.210185920906067. Learning Rate: 1.1727855477855479e-06\n",
      "Batch 6,300 of 9,631 Elased 0:47:33. Training loss: 3.210077194599878. Learning Rate: 1.1554417804417805e-06\n",
      "Batch 6,350 of 9,631 Elased 0:47:56. Training loss: 3.2098375921925224. Learning Rate: 1.138098013098013e-06\n",
      "Batch 6,400 of 9,631 Elased 0:48:18. Training loss: 3.2095381040498614. Learning Rate: 1.1207542457542459e-06\n",
      "Batch 6,450 of 9,631 Elased 0:48:41. Training loss: 3.210989553909893. Learning Rate: 1.1034104784104785e-06\n",
      "Batch 6,500 of 9,631 Elased 0:49:03. Training loss: 3.209374773869148. Learning Rate: 1.086066711066711e-06\n",
      "Batch 6,550 of 9,631 Elased 0:49:26. Training loss: 3.2095415892127814. Learning Rate: 1.0687229437229438e-06\n",
      "Batch 6,600 of 9,631 Elased 0:49:49. Training loss: 3.2103803718812536. Learning Rate: 1.0513791763791765e-06\n",
      "Batch 6,650 of 9,631 Elased 0:50:11. Training loss: 3.210718785694667. Learning Rate: 1.0340354090354091e-06\n",
      "Batch 6,700 of 9,631 Elased 0:50:34. Training loss: 3.211721947157561. Learning Rate: 1.0166916416916416e-06\n",
      "Batch 6,750 of 9,631 Elased 0:50:57. Training loss: 3.2117197839595653. Learning Rate: 9.993478743478744e-07\n",
      "Batch 6,800 of 9,631 Elased 0:51:19. Training loss: 3.2116213849362203. Learning Rate: 9.82004107004107e-07\n",
      "Batch 6,850 of 9,631 Elased 0:51:42. Training loss: 3.2112378775464356. Learning Rate: 9.646603396603395e-07\n",
      "Batch 6,900 of 9,631 Elased 0:52:05. Training loss: 3.2110166250968324. Learning Rate: 9.473165723165724e-07\n",
      "Batch 6,950 of 9,631 Elased 0:52:27. Training loss: 3.2110249962738093. Learning Rate: 9.299728049728051e-07\n",
      "Batch 7,000 of 9,631 Elased 0:52:50. Training loss: 3.210507201552391. Learning Rate: 9.126290376290376e-07\n",
      "Batch 7,050 of 9,631 Elased 0:53:13. Training loss: 3.2099682873022473. Learning Rate: 8.952852702852703e-07\n",
      "Batch 7,100 of 9,631 Elased 0:53:35. Training loss: 3.2100064906939654. Learning Rate: 8.77941502941503e-07\n",
      "Batch 7,150 of 9,631 Elased 0:53:58. Training loss: 3.2098463890769264. Learning Rate: 8.605977355977356e-07\n",
      "Batch 7,200 of 9,631 Elased 0:54:20. Training loss: 3.210074522395929. Learning Rate: 8.432539682539682e-07\n",
      "Batch 7,250 of 9,631 Elased 0:54:43. Training loss: 3.2091873486288662. Learning Rate: 8.25910200910201e-07\n",
      "Batch 7,300 of 9,631 Elased 0:55:06. Training loss: 3.209729086666891. Learning Rate: 8.085664335664337e-07\n",
      "Batch 7,350 of 9,631 Elased 0:55:28. Training loss: 3.2092318058176104. Learning Rate: 7.912226662226662e-07\n",
      "Batch 7,400 of 9,631 Elased 0:55:51. Training loss: 3.2080845986024755. Learning Rate: 7.73878898878899e-07\n",
      "Batch 7,450 of 9,631 Elased 0:56:14. Training loss: 3.2081090914803063. Learning Rate: 7.565351315351315e-07\n",
      "Batch 7,500 of 9,631 Elased 0:56:36. Training loss: 3.2073372870604198. Learning Rate: 7.391913641913642e-07\n",
      "Batch 7,550 of 9,631 Elased 0:56:59. Training loss: 3.2074297475025353. Learning Rate: 7.218475968475968e-07\n",
      "Batch 7,600 of 9,631 Elased 0:57:21. Training loss: 3.2077093062432187. Learning Rate: 7.045038295038295e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7,650 of 9,631 Elased 0:57:44. Training loss: 3.2077602992026635. Learning Rate: 6.871600621600623e-07\n",
      "Batch 7,700 of 9,631 Elased 0:58:07. Training loss: 3.2067308513839525. Learning Rate: 6.698162948162948e-07\n",
      "Batch 7,750 of 9,631 Elased 0:58:29. Training loss: 3.20523954437625. Learning Rate: 6.524725274725276e-07\n",
      "Batch 7,800 of 9,631 Elased 0:58:52. Training loss: 3.2056952729897623. Learning Rate: 6.351287601287601e-07\n",
      "Batch 7,850 of 9,631 Elased 0:59:15. Training loss: 3.206221793199041. Learning Rate: 6.177849927849928e-07\n",
      "Batch 7,900 of 9,631 Elased 0:59:37. Training loss: 3.2051122402541243. Learning Rate: 6.004412254412254e-07\n",
      "Batch 7,950 of 9,631 Elased 1:00:00. Training loss: 3.206408513417034. Learning Rate: 5.830974580974581e-07\n",
      "Batch 8,000 of 9,631 Elased 1:00:23. Training loss: 3.206087317198515. Learning Rate: 5.657536907536909e-07\n",
      "Batch 8,050 of 9,631 Elased 1:00:45. Training loss: 3.2069820316978124. Learning Rate: 5.484099234099234e-07\n",
      "Batch 8,100 of 9,631 Elased 1:01:08. Training loss: 3.2071058669796697. Learning Rate: 5.310661560661562e-07\n",
      "Batch 8,150 of 9,631 Elased 1:01:30. Training loss: 3.2079212956925844. Learning Rate: 5.137223887223887e-07\n",
      "Batch 8,200 of 9,631 Elased 1:01:53. Training loss: 3.207760330642142. Learning Rate: 4.963786213786214e-07\n",
      "Batch 8,250 of 9,631 Elased 1:02:16. Training loss: 3.2084091455864185. Learning Rate: 4.79034854034854e-07\n",
      "Batch 8,300 of 9,631 Elased 1:02:38. Training loss: 3.2087951895415063. Learning Rate: 4.6169108669108674e-07\n",
      "Batch 8,350 of 9,631 Elased 1:03:01. Training loss: 3.2084290343987014. Learning Rate: 4.4434731934731934e-07\n",
      "Batch 8,400 of 9,631 Elased 1:03:24. Training loss: 3.2075460270472935. Learning Rate: 4.27003552003552e-07\n",
      "Batch 8,450 of 9,631 Elased 1:03:46. Training loss: 3.2069160867724897. Learning Rate: 4.096597846597847e-07\n",
      "Batch 8,500 of 9,631 Elased 1:04:09. Training loss: 3.206607792966506. Learning Rate: 3.923160173160173e-07\n",
      "Batch 8,550 of 9,631 Elased 1:04:32. Training loss: 3.2062755575793527. Learning Rate: 3.7497224997224997e-07\n",
      "Batch 8,600 of 9,631 Elased 1:04:54. Training loss: 3.2068915575604104. Learning Rate: 3.576284826284827e-07\n",
      "Batch 8,650 of 9,631 Elased 1:05:17. Training loss: 3.205891222940015. Learning Rate: 3.4028471528471533e-07\n",
      "Batch 8,700 of 9,631 Elased 1:05:40. Training loss: 3.2058640252447677. Learning Rate: 3.22940947940948e-07\n",
      "Batch 8,750 of 9,631 Elased 1:06:02. Training loss: 3.2056796794210163. Learning Rate: 3.055971805971806e-07\n",
      "Batch 8,800 of 9,631 Elased 1:06:25. Training loss: 3.205871443558823. Learning Rate: 2.8825341325341325e-07\n",
      "Batch 8,850 of 9,631 Elased 1:06:47. Training loss: 3.2055803048947435. Learning Rate: 2.709096459096459e-07\n",
      "Batch 8,900 of 9,631 Elased 1:07:10. Training loss: 3.2055186364088164. Learning Rate: 2.5356587856587856e-07\n",
      "Batch 8,950 of 9,631 Elased 1:07:33. Training loss: 3.2051900524666856. Learning Rate: 2.3622211122211122e-07\n",
      "Batch 9,000 of 9,631 Elased 1:07:55. Training loss: 3.2042633820507262. Learning Rate: 2.188783438783439e-07\n",
      "Batch 9,050 of 9,631 Elased 1:08:18. Training loss: 3.2051125268382923. Learning Rate: 2.0153457653457656e-07\n",
      "Batch 9,100 of 9,631 Elased 1:08:41. Training loss: 3.2049088221984907. Learning Rate: 1.841908091908092e-07\n",
      "Batch 9,150 of 9,631 Elased 1:09:03. Training loss: 3.205128439045995. Learning Rate: 1.6684704184704187e-07\n",
      "Batch 9,200 of 9,631 Elased 1:09:26. Training loss: 3.205727774228739. Learning Rate: 1.495032745032745e-07\n",
      "Batch 9,250 of 9,631 Elased 1:09:49. Training loss: 3.205568231621304. Learning Rate: 1.3215950715950716e-07\n",
      "Batch 9,300 of 9,631 Elased 1:10:11. Training loss: 3.206051304199362. Learning Rate: 1.1481573981573982e-07\n",
      "Batch 9,350 of 9,631 Elased 1:10:34. Training loss: 3.2060446584543443. Learning Rate: 9.747197247197248e-08\n",
      "Batch 9,400 of 9,631 Elased 1:10:56. Training loss: 3.2059923614973718. Learning Rate: 8.012820512820512e-08\n",
      "Batch 9,450 of 9,631 Elased 1:11:19. Training loss: 3.205881576197488. Learning Rate: 6.27844377844378e-08\n",
      "Batch 9,500 of 9,631 Elased 1:11:42. Training loss: 3.205496536141948. Learning Rate: 4.544067044067044e-08\n",
      "Batch 9,550 of 9,631 Elased 1:12:04. Training loss: 3.2049654658677067. Learning Rate: 2.8096903096903103e-08\n",
      "Batch 9,600 of 9,631 Elased 1:12:26. Training loss: 3.205260869897902. Learning Rate: 1.0753135753135754e-08\n",
      "\n",
      "\n",
      "  Average training loss: 3.21\n",
      "  Training epcoh took: 1:12:41\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04100262619f470da470d7cd6543f2a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=67.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Validation Loss: 3.40\n",
      "  Validation took: 0:00:45\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Total training took 1 day, 10:14:12 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "seed_val = 0\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "training_stats = []\n",
    "in_training_stats = []\n",
    "total_t0 = time.time()\n",
    "\n",
    "writer = SummaryWriter('../../../repository/mju/SUM_KoBART_base_4(2)_1e-4_30_fix-loss')\n",
    "\n",
    "for epoch_i in tqdm(range(epochs)):\n",
    "    #               Training\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in tqdm(enumerate(train_dataloader)):\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            elased = format_time(time.time() - t0)\n",
    "            avg_loss = total_train_loss / step\n",
    "            lr = get_lr(optimizer)\n",
    "            print('Batch {:>5,} of {:5,} Elased {:}. Training loss: {}. Learning Rate: {}'.format(\n",
    "                step, len(train_dataloader), elased, avg_loss, lr))\n",
    "            in_training_stats.append(\n",
    "                {\n",
    "                'step': step,\n",
    "                'elased': elased,\n",
    "                'Training Loss': avg_loss,\n",
    "                'Learning Rate': lr\n",
    "            })\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_attention_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        output = model(input_ids=b_input_ids, attention_mask = b_attention_mask, labels=b_labels, return_dict=True)\n",
    "        loss = output.loss\n",
    "        \n",
    "        lr = get_lr(optimizer)\n",
    "        total_train_loss += sum(loss).float().item()\n",
    "        sum(loss).float().backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        writer.add_scalar('Loss/train',\n",
    "                         total_train_loss/(step+1),\n",
    "                         epoch_i * len(train_dataloader) + step + 1)\n",
    "        \n",
    "        writer.add_scalar('lr',\n",
    "                         lr,\n",
    "                         epoch_i * len(train_dataloader) + step + 1)\n",
    "        \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    training_time = format_time(time.time() - t0)\n",
    "    lr = get_lr(optimizer)\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "\n",
    "    #               Validation\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_loss = 0\n",
    "    total_eval_accuracy = 0\n",
    "\n",
    "    for batch in tqdm(val_dataloader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_attention_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            output = model(input_ids=b_input_ids, attention_mask = b_attention_mask, labels=b_labels, return_dict=True)\n",
    "            loss = output.loss\n",
    "            logits = output.logits\n",
    "            \n",
    "        total_eval_loss += sum(loss).float().item()\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(val_dataloader)\n",
    "    avg_val_accuracy = total_eval_accuracy / len(val_dataloader)\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    lr = get_lr(optimizer)\n",
    "    \n",
    "\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid Loss': avg_val_loss,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time,\n",
    "            'Learning Rate': lr,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid Loss</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "      <th>Learning Rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.123</td>\n",
       "      <td>9.242</td>\n",
       "      <td>1:05:16</td>\n",
       "      <td>0:00:42</td>\n",
       "      <td>9.688e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.545</td>\n",
       "      <td>5.145</td>\n",
       "      <td>1:06:19</td>\n",
       "      <td>0:00:42</td>\n",
       "      <td>9.354e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.892</td>\n",
       "      <td>4.588</td>\n",
       "      <td>1:06:22</td>\n",
       "      <td>0:00:42</td>\n",
       "      <td>9.020e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.497</td>\n",
       "      <td>4.325</td>\n",
       "      <td>1:06:20</td>\n",
       "      <td>0:00:40</td>\n",
       "      <td>8.686e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.229</td>\n",
       "      <td>4.099</td>\n",
       "      <td>1:01:51</td>\n",
       "      <td>0:00:40</td>\n",
       "      <td>8.352e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.053</td>\n",
       "      <td>3.963</td>\n",
       "      <td>1:01:57</td>\n",
       "      <td>0:00:39</td>\n",
       "      <td>8.018e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.906</td>\n",
       "      <td>3.846</td>\n",
       "      <td>1:01:57</td>\n",
       "      <td>0:00:39</td>\n",
       "      <td>7.684e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.794</td>\n",
       "      <td>3.753</td>\n",
       "      <td>1:02:37</td>\n",
       "      <td>0:00:43</td>\n",
       "      <td>7.350e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.710</td>\n",
       "      <td>3.688</td>\n",
       "      <td>1:08:16</td>\n",
       "      <td>0:00:43</td>\n",
       "      <td>7.016e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.639</td>\n",
       "      <td>3.640</td>\n",
       "      <td>1:08:14</td>\n",
       "      <td>0:00:44</td>\n",
       "      <td>6.682e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.581</td>\n",
       "      <td>3.604</td>\n",
       "      <td>1:08:10</td>\n",
       "      <td>0:00:43</td>\n",
       "      <td>6.347e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3.533</td>\n",
       "      <td>3.572</td>\n",
       "      <td>1:08:15</td>\n",
       "      <td>0:00:42</td>\n",
       "      <td>6.013e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.494</td>\n",
       "      <td>3.548</td>\n",
       "      <td>1:08:20</td>\n",
       "      <td>0:00:43</td>\n",
       "      <td>5.679e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.456</td>\n",
       "      <td>3.525</td>\n",
       "      <td>1:08:19</td>\n",
       "      <td>0:00:42</td>\n",
       "      <td>5.345e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3.421</td>\n",
       "      <td>3.507</td>\n",
       "      <td>1:08:16</td>\n",
       "      <td>0:00:42</td>\n",
       "      <td>5.011e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3.395</td>\n",
       "      <td>3.490</td>\n",
       "      <td>1:08:21</td>\n",
       "      <td>0:00:43</td>\n",
       "      <td>4.677e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.365</td>\n",
       "      <td>3.477</td>\n",
       "      <td>1:08:12</td>\n",
       "      <td>0:00:43</td>\n",
       "      <td>4.343e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3.341</td>\n",
       "      <td>3.466</td>\n",
       "      <td>1:08:20</td>\n",
       "      <td>0:00:42</td>\n",
       "      <td>4.009e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3.322</td>\n",
       "      <td>3.456</td>\n",
       "      <td>1:08:19</td>\n",
       "      <td>0:00:43</td>\n",
       "      <td>3.675e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.301</td>\n",
       "      <td>3.447</td>\n",
       "      <td>1:08:16</td>\n",
       "      <td>0:00:43</td>\n",
       "      <td>3.341e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3.284</td>\n",
       "      <td>3.437</td>\n",
       "      <td>1:08:20</td>\n",
       "      <td>0:00:43</td>\n",
       "      <td>3.007e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3.265</td>\n",
       "      <td>3.429</td>\n",
       "      <td>1:08:19</td>\n",
       "      <td>0:00:43</td>\n",
       "      <td>2.673e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3.254</td>\n",
       "      <td>3.423</td>\n",
       "      <td>1:08:14</td>\n",
       "      <td>0:00:42</td>\n",
       "      <td>2.339e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3.244</td>\n",
       "      <td>3.417</td>\n",
       "      <td>1:11:17</td>\n",
       "      <td>0:00:46</td>\n",
       "      <td>2.004e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3.231</td>\n",
       "      <td>3.413</td>\n",
       "      <td>1:12:39</td>\n",
       "      <td>0:00:45</td>\n",
       "      <td>1.670e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3.225</td>\n",
       "      <td>3.410</td>\n",
       "      <td>1:12:07</td>\n",
       "      <td>0:00:43</td>\n",
       "      <td>1.336e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3.218</td>\n",
       "      <td>3.408</td>\n",
       "      <td>1:08:19</td>\n",
       "      <td>0:00:43</td>\n",
       "      <td>1.002e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3.212</td>\n",
       "      <td>3.405</td>\n",
       "      <td>1:08:19</td>\n",
       "      <td>0:00:43</td>\n",
       "      <td>6.682e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3.209</td>\n",
       "      <td>3.404</td>\n",
       "      <td>1:10:45</td>\n",
       "      <td>0:00:46</td>\n",
       "      <td>3.341e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3.205</td>\n",
       "      <td>3.404</td>\n",
       "      <td>1:12:41</td>\n",
       "      <td>0:00:45</td>\n",
       "      <td>0.000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid Loss Training Time Validation Time  Learning Rate\n",
       "epoch                                                                        \n",
       "1             13.123       9.242       1:05:16         0:00:42      9.688e-05\n",
       "2              6.545       5.145       1:06:19         0:00:42      9.354e-05\n",
       "3              4.892       4.588       1:06:22         0:00:42      9.020e-05\n",
       "4              4.497       4.325       1:06:20         0:00:40      8.686e-05\n",
       "5              4.229       4.099       1:01:51         0:00:40      8.352e-05\n",
       "6              4.053       3.963       1:01:57         0:00:39      8.018e-05\n",
       "7              3.906       3.846       1:01:57         0:00:39      7.684e-05\n",
       "8              3.794       3.753       1:02:37         0:00:43      7.350e-05\n",
       "9              3.710       3.688       1:08:16         0:00:43      7.016e-05\n",
       "10             3.639       3.640       1:08:14         0:00:44      6.682e-05\n",
       "11             3.581       3.604       1:08:10         0:00:43      6.347e-05\n",
       "12             3.533       3.572       1:08:15         0:00:42      6.013e-05\n",
       "13             3.494       3.548       1:08:20         0:00:43      5.679e-05\n",
       "14             3.456       3.525       1:08:19         0:00:42      5.345e-05\n",
       "15             3.421       3.507       1:08:16         0:00:42      5.011e-05\n",
       "16             3.395       3.490       1:08:21         0:00:43      4.677e-05\n",
       "17             3.365       3.477       1:08:12         0:00:43      4.343e-05\n",
       "18             3.341       3.466       1:08:20         0:00:42      4.009e-05\n",
       "19             3.322       3.456       1:08:19         0:00:43      3.675e-05\n",
       "20             3.301       3.447       1:08:16         0:00:43      3.341e-05\n",
       "21             3.284       3.437       1:08:20         0:00:43      3.007e-05\n",
       "22             3.265       3.429       1:08:19         0:00:43      2.673e-05\n",
       "23             3.254       3.423       1:08:14         0:00:42      2.339e-05\n",
       "24             3.244       3.417       1:11:17         0:00:46      2.004e-05\n",
       "25             3.231       3.413       1:12:39         0:00:45      1.670e-05\n",
       "26             3.225       3.410       1:12:07         0:00:43      1.336e-05\n",
       "27             3.218       3.408       1:08:19         0:00:43      1.002e-05\n",
       "28             3.212       3.405       1:08:19         0:00:43      6.682e-06\n",
       "29             3.209       3.404       1:10:45         0:00:46      3.341e-06\n",
       "30             3.205       3.404       1:12:41         0:00:45      0.000e+00"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('precision', 3)\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(style = 'darkgrid')\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams['figure.figsize'] = (12,6)\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label = 'Training')\n",
    "plt.plot(df_stats['Valid Loss'], 'g-o', label = 'Validation')\n",
    "\n",
    "plt.title('Training & Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.xticks(range(1,epochs+1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f873f8d50347139a92a2c20efbd0cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done 0:21:37\n"
     ]
    }
   ],
   "source": [
    "GPU_NUM = 4 # 원하는 GPU 번호 입력\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device) # change allocation of current GPU\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, sampler = SequentialSampler(test_dataset), batch_size = 16)\n",
    "\n",
    "t0=time.time()\n",
    "predictions = []\n",
    "\n",
    "model.eval()\n",
    "for batch in tqdm(test_dataloader):\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_attention_mask = batch[1].to(device)\n",
    "\n",
    "    with torch.no_grad():        \n",
    "        output = model.module.generate(input_ids=b_input_ids, attention_mask = b_attention_mask, num_beams=5, max_length=50, early_stopping=True)\n",
    "    predictions.append(output)\n",
    "    \n",
    "validation_time = format_time(time.time() - t0)\n",
    "print(\"Done\", validation_time)\n",
    "\n",
    "output = []\n",
    "for i in range(len(predictions)):\n",
    "    for j in range(len(predictions[i])):\n",
    "        output.append(tokenizer.decode(predictions[i][j], skip_special_tokens = True))\n",
    "\n",
    "submission = pd.read_csv('../dataset/abstractive_sample_submission_v2.csv')\n",
    "submission.columns = ['id', 'summary']\n",
    "submission['summary'] = output\n",
    "\n",
    "submission.to_csv('../result/SUM_KoBART_base_4(2)_1e-4_30_fix-loss.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '../finetuned_model/SUM_KoBART_base_4(2)_1e-4_30_fix-loss.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate ROUGE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../finetuned_model/SUM_KoBART_base_4(2)_1e-4_30_fix-loss.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from rouge_metric import Rouge\n",
    "\n",
    "class RougeScorer:\n",
    "    def __init__(self):\n",
    "        self.rouge_evaluator = Rouge(\n",
    "            metrics=[\"rouge-n\", \"rouge-l\"],\n",
    "            max_n=2,\n",
    "            limit_length=True,\n",
    "            length_limit=1000,\n",
    "            length_limit_type=\"words\",\n",
    "            use_tokenizer=True,\n",
    "            apply_avg=True,\n",
    "            apply_best=False,\n",
    "            alpha=0.5,  # Default F1_score\n",
    "            weight_factor=1.2,\n",
    "        )\n",
    "\n",
    "    def compute_rouge(self, ref_df, hyp_df):\n",
    "        #ref_df = pd.read_csv(ref_path)\n",
    "        #hyp_df = pd.read_csv(hyp_path)\n",
    "        hyp_df.iloc[:,1] = hyp_df.iloc[:,1].fillna(' ')\n",
    "        ids = ref_df['id']\n",
    "        hyp_df = hyp_df[hyp_df['id'].isin(ids)]\n",
    "        hyp_df.index = ref_df.index\n",
    "\n",
    "        ref_df = ref_df.sort_values(by=[\"id\"])\n",
    "        hyp_df = hyp_df.sort_values(by=[\"id\"])\n",
    "        ref_df[\"id\"] = ref_df[\"id\"].astype(int)\n",
    "        hyp_df[\"id\"] = hyp_df[\"id\"].astype(int)\n",
    "\n",
    "        hyps = [tuple(row) for row in hyp_df.values]\n",
    "        refs = [tuple(row) for row in ref_df.values]\n",
    "\n",
    "        reference_summaries = []\n",
    "        generated_summaries = []\n",
    "\n",
    "        for ref_tp, hyp_tp in zip(refs, hyps):\n",
    "            ref_id, ref = ref_tp\n",
    "            hyp_id, hyp = hyp_tp\n",
    "\n",
    "            assert ref_id == hyp_id\n",
    "\n",
    "            reference_summaries.append(ref)\n",
    "            generated_summaries.append(hyp)\n",
    "\n",
    "        scores = self.rouge_evaluator.get_scores(generated_summaries, reference_summaries)\n",
    "        str_scores = self.format_rouge_scores(scores)\n",
    "        #self.save_rouge_scores(str_scores)\n",
    "        return str_scores\n",
    "\n",
    "    def save_rouge_scores(self, str_scores):\n",
    "        with open(\"rouge_scores.txt\", \"w\") as output:\n",
    "            output.write(str_scores)\n",
    "\n",
    "    def format_rouge_scores(self, scores):\n",
    "    \treturn \"{:.3f},{:.3f},{:.3f}\".format(\n",
    "            scores[\"rouge-1\"][\"f\"],\n",
    "            scores[\"rouge-2\"][\"f\"],\n",
    "            scores[\"rouge-l\"][\"f\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38,523 training samples\n",
      "4,280 validation samples\n",
      "9,987 test samples\n"
     ]
    }
   ],
   "source": [
    "labels = torch.cat(labels_list, axis=0)\n",
    "torch.manual_seed(0)\n",
    "dataset = TensorDataset(new_input_ids, new_attention_mask, labels)\n",
    "train_size, val_size = round(len(train_data) * 0.9), round(len(train_data) * 0.1)\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_mask)\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))\n",
    "print('{:>5,} test samples'.format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "GPU_NUM = 4 # 원하는 GPU 번호 입력\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device) # change allocation of current GPU\n",
    "\n",
    "for i in range(5): \n",
    "    with torch.no_grad():        \n",
    "        output = model.module.generate(input_ids=val_dataset[i][0].cuda().unsqueeze(0),\n",
    "                                       attention_mask = val_dataset[i][1].cuda().unsqueeze(0),\n",
    "                                       num_beams=5, num_beam_groups=1, max_length=50, diversity_penalty =0.0, early_stopping=True)\n",
    "    predictions.append(output)\n",
    "    \n",
    "output = []\n",
    "for i in range(len(predictions)):\n",
    "    for j in range(len(predictions[i])):\n",
    "        output.append(tokenizer.decode(predictions[i][j], skip_special_tokens = True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['부산 해운대 해운대·수영·동래구가 연내 조정대상지역에서 해제될 가능성이 커지고 있다. ',\n",
       " '부산지방기상청(청장 유희동)과 부산중구(구청장 윤종서)는 「대청큰마루터 기상사업」의 일환으로 양 기관이 협력하여 부산기상관측소 내에 조성한',\n",
       " '코미디언 조혜련이 공연 중 다리가 부러진 사연을 밝혀 화제가 되고 되고 있다.',\n",
       " '5월 행사 줄이어 이달만 국내외 행사 10건 열려 이달만 국내외 행사 10건 열려 3천600여 명 여수 방문 국내 최초 MICE 인증도시 여수가 MICE 행사를 잇달아 개최한다.',\n",
       " '올해 제3회째를 맞는 청산생선국수와 함께하는 민속씨름 대회가 오는 13-14일까지 청산면 교평리 체육공원일원에서 열린다.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 268/268 [13:16<00:00,  2.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 0:13:16\n",
      "total inference time: 0:13:17\n",
      "average inference time: 0.18614409487938213\n"
     ]
    }
   ],
   "source": [
    "GPU_NUM = 4 # 원하는 GPU 번호 입력\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device) # change allocation of current GPU\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, sampler = SequentialSampler(val_dataset), batch_size = 16)\n",
    "t0=time.time()\n",
    "predictions = []\n",
    "\n",
    "model.eval()\n",
    "for batch in tqdm(val_dataloader):\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_attention_mask = batch[1].to(device)\n",
    "\n",
    "    with torch.no_grad():        \n",
    "        output = model.module.generate(input_ids=b_input_ids, attention_mask = b_attention_mask, num_beams=8, max_length=100, early_stopping=True, )\n",
    "    predictions.append(output)\n",
    "\n",
    "validation_time = format_time(time.time() - t0)\n",
    "print(\"Done\", validation_time)\n",
    "\n",
    "output = []\n",
    "for i in range(len(predictions)):\n",
    "    for j in range(len(predictions[i])):\n",
    "        output.append(tokenizer.decode(predictions[i][j], skip_special_tokens = True))\n",
    "        \n",
    "t1=time.time()\n",
    "print('total inference time: {}'.format(format_time(t1-t0)))\n",
    "print('average inference time: {}'.format((t1-t0)/len(output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../dataset/abstractive_sample_submission_v2.csv')\n",
    "submission = submission[:4280]\n",
    "submission.columns = ['id', 'summary']\n",
    "submission['summary'] = output\n",
    "submission['id'] = range(4280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "for i in range(len(val_dataset)):\n",
    "    label.append(tokenizer.decode(val_dataset[i][2], skip_special_tokens = True))\n",
    "\n",
    "label_df = pd.read_csv('../dataset/abstractive_sample_submission_v2.csv')\n",
    "label_df = label_df[:4280]\n",
    "label_df.columns = ['id', 'summary']\n",
    "label_df['summary'] = label\n",
    "label_df['id'] = range(4280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.404,0.257,0.343'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_length=50, seed=0\n",
    "rouge = RougeScorer()\n",
    "rouge.compute_rouge(label_df, submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.416,0.267,0.352'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_length=100, seed=0\n",
    "rouge = RougeScorer()\n",
    "rouge.compute_rouge(label_df, submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.409,0.261,0.345'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 실험시 사용한 데이터셋\n",
    "rouge.compute_rouge(label_df, submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 문장:  주민체감형 디지털 사회혁신 활성화 사업 선정 국비 1억5천 확보“ICT기반 돌봄 시스템 구축” 신안군 주민체감형 디지털 사화혁신 활성화 사업 개요도\n",
      "정답 문장:  전남 신안군이 행정안전부에서 공모하는 ‘주민체감형 디지털 사회혁신 활성화 사업’에 선정돼 국비 1억 5천만원을 지원받아 ‘ICT기반 1004섬 생활밀착 돌봄 시스템’을 구축하고 군민의 복지체감도 향상을 위한 사업을 추진한다. \n",
      "\n",
      "예측 문장:  행정안전위원회는 4일 상임위원회를 열고 부처 업무보고를 받거나 법안 심사 등 각종 안건을 처리했다. 행정안전위원회는 이날 전체회의를 열고 진영 행정안전부 장관 후보자에 대한 인사청문 경과보고서를 채택했다.\n",
      "정답 문장:  국회는 4일 상임위원회를 열고 부처 업무보고를 받거나 법안 심사 등의 여러가지 안건을 처리한 가운데, 행정안전위원회는 진영 행정안전부 장관 후보자에 대한 인사청문 경과보고서를 채택했으며, 법제사법위원회는 9일과 10일에 문형배·이미선 헌법재판소 재판관 후보자에 대한 인사청문회를  각각 진행하기로 하였다고 전해졌다.\n",
      " \n",
      "\n",
      "예측 문장:  양승조 충남도지사는 24일(현지시각) 폴란드 비엘코폴스카주 코닌시를 찾아 시 관계자 등과 에너지 전환 정책 등을 주제로 간담회를 갖고 충남도내 석탄화력 현황과 대책에 대해 발표하고 있다.\n",
      "정답 문장:  양승조 충남도지사는 앞으로 200년 간 사용할 수 있는 석탄을 보유한 '석탄 강국' 폴란드가 탈석탄을 위해 다방면으로 힘쓰고 있음을 살피고 6박8일간의 유럽 출장 일정을 마무리하고 귀국했다.  \n",
      "\n",
      "예측 문장:  광주 소속 선수단은 나폴리 하계U대회 포상금 수여식에 참석한 광주 소속 선수단이 파이팅을 외치고 있다.\n",
      "정답 문장:  지난 24일 체육회관 중회의실에서 광주시체육회는 2019 제 30회 나폴리 하계유니버시아드 대회에서 금2·은1·동5개의 메달을 획득한 광주시체육회 소속 임원과 선수에게 입상 포상금을 전달했다.   \n",
      "\n",
      "예측 문장:  인천시, 미추홀구 등 3곳에 오픈 이웃들 모여 정보 공유·체험학습 2022년까지 총 100곳 달성 목표 인천시가 인천형 보육정책의 핵심 공약인 (가칭)혁신육아카페 사업을 본격화한다.\n",
      "정답 문장:  24일 인천시에 따르면 인천형 육아정책의 패러다임을 분석해 새롭게 마련한 민선7기 시의 육아정책 로드맵 핵심 사업인 혁신육아카페 위탁사업을 공모했고, 시범사업으로 연내 미추홀구와 남동구, 서구 등 3곳에 문을 연다. \n",
      "\n",
      "예측 문장:  경북교육청은 오는 7일 구미 경북교육청연수원에서 초등교원 800여 명이 참석한 가운데 수업 나눔 축제를 연다.\n",
      "정답 문장:  경북교육청은 오는 7일 구미 경북교육청연수원에서 초등교원 800여 명을 대상으로 수업 나눔 축제를 열고 수업·행복 나눔, 소통·공유, 전시·체험 부스 등 다양한 프로그램을 운영하며 수업 나눔 문화 확산을 위해 한 해 동안 연구하고 실천한 결과를 공유할 예정이다. \n",
      "\n",
      "예측 문장:  부천시는 지난 10일 G2B(Government to Business) 42개 회원사와 홍보네트워크 회의를 개최했다. 홍보 협력 강화와 정보 공유를 위해 마련된 회의에서 이들은 정부와 기업 간의 온·오프라인 비영리적\n",
      "정답 문장:  부천시에서 G2B 42개 회원사들과 홍보네트워크 회의를 개최하였으며 회의를 주관한 총장은 담당자들의 노고에 감사드린다고 말하며 정부와 기업간의 온.오프라인 비영리적 상호작용을 위해 노력해 나갈 것을 다짐하였다 \n",
      "\n",
      "예측 문장:  한용덕 한화이글스 감독.\n",
      "정답 문장:  프로야구 한화이글스 한용덕(54) 감독은 13일 오전 두산베어스와의 2차 시범경기를 앞두고 가진 언론 인터뷰에서 지난 12일 첫 시범경기에 등판한 신인투수 김이환, 박윤철의 투구에 대해 \"제구도 좋았고 주무기를 잘 살려 앞으로 활용 가치가 높을 것 같다\"고 기대감을 드러냈고, 신인타자인 노시환과 유장혁에 대해서도 안타를 치고 선구안이 좋았다고 호평했다. \n",
      "\n",
      "예측 문장:  정부는 지난 31일 케이블과 위성방송 등 유료방송 의무송출 대상에서 종합편성채널(종편)만 제외하는 내용의 방송법 시행령 일부개정안을 입법 예고했다.\n",
      "정답 문장:  정부가 31일에 케이블과 위성방송 등 유료방송 의무송출 대상에서 종합편성채널만 제외하는 방송법 시행령 일부개정안을 입법 예고하자, 자유한국당 강효상 의원은 종편탄압 꼼수 개정이라며,  방송의 공정성과 다양성을 보장하는 제도적 방지를 추진하기 위한 방송법 시행령 일부개정안을 입법 예고했다.\n",
      " \n",
      "\n",
      "예측 문장:  2019 나폴리 하계유니버시아드 대회에서 동메달을 획득한 호남대 김정미(왼쪽)와 전수인 광주 선수들이 나폴리 하계유니버시아드 대회에서 선전하고 있다.\n",
      "정답 문장:  9일 이탈리아 살레르노에서 열린 2019 나폴리 하계유니버시아드 대회에서 호남대 전수인·김정미가 펜싱 사브르 단체전에서 동메달, 수영 혼성종목에서 이재경(광주시체육회)이 동메달을 각각 획득하며 선전을 펼쳤다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('예측 문장: ', output[i])\n",
    "    print('정답 문장: ', tokenizer.decode(val_dataset[i][2], skip_special_tokens=True), '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
